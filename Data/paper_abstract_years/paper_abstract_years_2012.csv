title,Abstract,year
"Lighting Optimization for Sustainable Building Design Considering User Productivity","User productivity is a key component of an integrated sustainable building design framework, as a consideration in addition to operating and construction costs associated with sustainable building practices. Research has shown that employee productivity increases as visible light levels rise in the workplace. Incorporating efficient lighting systems into sustainable building techniques can potentially increase user productivity while reducing electricity costs. This paper presents a single criterion approach that captures the trade-offs between costs, users, and current building standards. A model has been created to explore the feasible design space of a commercial workspace by populating a repository of both active and passive lighting components that can be accessed to generate various lighting designs. A genetic algorithm is used to optimize potential lighting choices in a given workspace, and allows the designer to explore Pareto optimal productivity solutions that best fit the desired application. The result is an optimal solution for a given workspace lighting configuration that captures user productivity while minimizing operating costs.",2012
"Finite Element Synthesis for Design Optimization","The present work introduces a new Finite Element methodology that can be used directly in calculating and optimizing the thickness of a mechanical structure. This method depends on the constant strain triangle model after assuming different thicknesses at the elements nodes. A linear relation between the nodes thicknesses is assumed and a new stiffness matrix is created. Nodes thicknesses are optimized using the developed HGP method to reach uniform stress among the structure to satisfy the constrained allowable stress designated by the designer. Examples and sample applications are employed for comparisons and their results culminate in removing unnecessary elements and increasing the thickness, which is subjected to high stresses. Results indicate marked improvements and potential for topology optimization.",2012
"A Local-Diffusion Genetic Algorithm for Disjoint Pareto-Optimal Problems With Application to Vehicle Suspension","This paper presents a multi-objective design optimization study of a vehicle suspension system with passive variable stiffness and active damping. Design of suspension systems is particularly challenging when the effective mass of the vehicle is subject to considerable variation during service. Perfectly maintaining the suspension performance under the variable load typically requires a controlled actuator in order to emulate variable stiffness. This is typically done through a hydraulic or pneumatic system, which can be too costly for small/medium pickup trucks. The system in this paper employs two springs with an offset to the second spring so that it engages during large deformation only, thereby providing passive variable stiffness without expensive hydraulics. The system damping is assumed to be controlled via variable viscosity magnetizable fluid, which can be implemented in a compact, low-power setup. Simulation of the suspension system is performed by numerically solving the nonlinear equations of motion for a quarter-vehicle mode subject to excitation from a road profile over a set period of time. A performance index from the literature is evaluated for the suspension system for the cases of minimum and maximum weight, and the two indices values are regarded as objectives in a multi-objective problem. As the individual objectives are prone to having local optima, the multi-objective problem is prone to having a disjointed Pareto-space. To deal with this issue, a modification is proposed to a multi-objective genetic algorithm. The algorithm performance is investigated via analytical test functions as well as a design case of the suspension system. Results show a reduction in the system’s spring size, with the Pareto point obtained from the proposed diffusion model, without compromising the system’s performance.",2012
"Optimum Design of Columns Under Elastic Buckling","In this paper, a generalized approach is developed to optimize column configuration subjected to buckling load. The configuration utilizes B-spline contour to provide more freedom to model the column shape. Previous columns in literature use tapered or parabolic tapered for configuration. This work considers hinged-hinged columns of circular solid cross-sectional area. Two sample applications are optimized using Genetic Algorithm with the finite difference method to satisfy the buckling constraints. The length and load are fixed. The objective is to minimize the volume considering the cross-sectional diameters as the design variables. B-Spline quadratic with three and five control points and cubic with five control points are applied. The proposed configuration is compared with tapered and parabolic tapered columns. Results show that continuity provides a better optimum against column buckling than other tapered columns. Even though volume is more than some configurations by about 1.67%, but those configurations would not satisfy buckling constraints over the entire length of the column.",2012
"Interactive UAS Path Planning Using Particle Swarm Optimization and Flight Mechanics","Military engagements are continuing the movement toward automated and unmanned vehicles for a variety of military tasks. One important piece of automating is the ability to perform path planning quickly, safely, and reliably for unmanned aerial systems (UASs). Path planners often ignore the UAS’s ability to perform the maneuvers necessary to fly through the specified waypoints, instead relying on them to fly as close as possible. To date, path planning algorithms have been limited to providing only a single solution without input from an operator. This project attempts to incorporate operator experience in the path planning process. Particle Swarm Optimization (PSO) allows the generation of multiple optimized three-dimensional flight paths for the operator to choose from. This paper improves upon a previously developed method by incorporating flight mechanics equations into the optimization problem formulation. The new problem formulation ensures that the returned flight paths do not violate maximum load factor (G-force), minimum velocity (stall velocity), or the minimum turning radius.",2012
"A Framework for Parallel Sampling of Design Space With Application to Vehicle Crashworthiness Optimization","This paper presents a framework for simulation-based design optimization of computationally-expensive problems, where economizing the generation of sample designs is highly desirable. Various meta-modeling schemes are used in practice in order to approximate the input-output relationships in the designed system and suggest candidate locations in the design space where high quality designs are likely to be found. One such popular approach is known as ",2012
"Non-Probabilistic Based Structural Design Optimization Under External Load Uncertainty With Eigenvalue-Superposition of Convex Models","The non-probabilistic-based structural design optimization problems with external load uncertainties are often solved through a two-level approach. However there are several challenges in this method. Firstly, to assure the reliability of the design, the lower level problem must be solved to its global optimality. Secondly, the sensitivity of the upper level problem cannot be analytically derived. To overcome these challenges, a new method based on the Eigenvalue-Superposition of Convex Models (ESCM) is proposed in this paper. The ESCM method replaces the global optimum of the lower level problem by a confidence bound, namely the ESCM bound, and with which the two-level problem can be formulated into a single level problem. The advantages of the ESCM method in efficiency and stability are demonstrated through numerical examples.",2012
"A Graph Grammar Based Approach to Automated Manufacturing Planning","In this paper, a new graph grammar representation is proposed to reason about the manufacturability of solid models. The knowledge captured in the graph grammar rules serves as a virtual machinist in its ability to recognize arbitrary geometries and match them to various machine operations. Firstly, a given part is decomposed into multiple sub-volumes, where each sub-volume is assumed to be machined in one operation or to be non-machinable. The decomposed part is converted into a graph so that graph grammar rules can determine the machining details. For each operation, rules determine the face on the part that the tool enters, the type of tools used, the type of machine used, and how the part is fixed within the machine. A candidate plan is a feasible sequence of all of the necessary machining operations needed to manufacture this part. If a given geometry is not machinable, the rules will fail to find operations for all of the partitions.",2012
"On the Use of Active Learning in Engineering Design","Active learning refers to the mechanism of querying users to accomplish a classification task in machine learning or a conjoint analysis in econometrics with minimum cost. Classification and conjoint analysis have been introduced to design research to automate design feasibility checking and to construct marketing demand models, respectively. In this paper, we review active learning algorithms from computer and marketing science, and establish the mathematical commonality between the two approaches. We compare empirically the performance of active learning and static D-optimal design on simulated classification and conjoint analysis test problems with labelling noise. Results show that active learning outperforms D-optimal design when query size is large or noise is small.",2012
"Representation: Structural Complexity of Assemblies to Create Neural Network Based Assembly Time Estimation Models","Assembly time estimation is traditionally a time intensive manual process requiring detailed geometric and process information to be available to a human designer. As a result of these factors, assembly time estimation is rarely applied during early design iterations. This paper explores the possibility that the assembly time estimation process can be automated while reducing the level of design detail required. The approach presented here trains artificial neural networks (ANNs) to estimate the assembly times of vehicle sub-assemblies at various stages using properties of the connectivity graph at that point as input data. Effectiveness of estimation is evaluated based on the distribution of estimates provided by a population of ANNs trained on the same input data using varying initial conditions. Results suggest that the method presented here can complete the time estimation of an assembly process with +/− 15% error given an initial sample of manually estimated times for the given sub-assembly.",2012
"Representation: Formal Development and Computational Recognition of Localized Requirement Change Types","Requirement change propagation, the process in which a change to one requirement results in additional requirement changes when otherwise this change would not have been needed, occurs frequently and must be managed. Multiple approaches exist, and have been readily published, for predicting requirement change propagation, analyzing change how a change to one requirement may propagate forward to other, related requirements (global level). However, the type of change encountered within a single requirement (localized level) has not been thoroughly studied and could be used to assist in the global analysis of requirement change propagation. This paper seeks to begin to fill this gap by identifying types of change requirements may encounter. By surveying research performed in the realm of requirement change, a taxonomy of change types is developed. To computationally analyze the changes, the localized requirement changes are represented through syntactical elements to identify which requirements’ parts of speech is affected. Using part of speech language rules, the identification of requirement change type is automatically identified. Further, the automatic identification of requirement change type is used to assist in predicting change propagation, a process currently automated. This bridges the gap between localized and global requirement change in an automated, systematic manner.",2012
"Decision-Based Design Using Time-Varying Preferences Represented by Stochastic Processes","Soliciting and expressing the preferences of a decision maker in engineering design is critical. In general, the preferences vary through time, complicating the design of engineering systems. In this article, we propose that if parameterized utility functions are used to model the preferences, the time-varying characteristics of the parameters can provide valuable information on the likely decisions the decision maker can make at a future time. To model the time-dependent uncertainty in preferences, we use parameterized utility functions with the parameters characterized by stochastic processes and demonstrate how the design process is affected by stationarity properties of the random parameters. We work in the normative utility theoretic domain and show a property of the multiplicative utility function that allows us to use the common Black-Scholes-Merton options pricing model from finance, to account for variability in preferences with time. Finally, we discuss how to modify the design process so that optimal products are ready when there is a future need for them. The applicability of our approach is demonstrated using a cell phone example.",2012
"Enabling Decision-Based Design Using Solution Trajectory Correction and Backtracking Rules","Optimization is needed for effective decision based design (DBD). However, a utility function assessed a priori in DBD does not usually capture the preferences of the decision maker over the entire design space. As a result, when the optimizer searches for the optimal design, it traverses (or ends up) in regions where the preference order among different solutions is different from the actual order. For a highly non-convex design space, this can lead to convergence to a grossly suboptimal design depending on the initial design. In this article, we propose two approaches to alleviate this issue. First, we map the trajectory of the solution as generated by the optimizer and generate ranking questions that are presented to the designer to verify the correctness of the utility function. We then propose backtracking rules if a local utility function is very different from the initially assessed function. We demonstrate our methodology using a mathematical example and a welded beam design problem.",2012
"Coordination of Product Design Process in View of Product and Organizational Structures","Management of product design projects becomes increasingly difficult as the complexity of products increases. For better management of such projects, well-considered preliminary coordination of design processes is essential. This paper proposes a method for coordination in the design process, which comprises two phases: 1) division of the design work into smaller tasks and sequencing them and 2) establishment of management activities. To facilitate this coordination, an integrated model of a product, process, and organization is proposed.",2012
"Safety Design Evaluation of Motorcycle Helmet for Oblique Impact","In this work, safety of motorcycle helmet design is investigated by using standard oblique impact test method. First, testing procedure is explained and test rig mechanism is introduced. Next, standard impact tests are performed on helmets. Data are collected using a tri-axial linear accelerometer embedded inside the headform and a high speed camera for measuring rotational acceleration. Then, results are studied and compared to injurious limit for human head injury. It is shown that during an oblique impact rotational acceleration can easily surpass the safe limit while the linear acceleration is well below the safe limit.",2012
"Optimization Method of Location for Cooperative Lifting of Two Mobile Cranes","Dual-crane lifting has been generally used with the need of erection and installation of large equipment. Choosing proper locations for two mobile cranes is an important work as well as a difficulty in design of heavy lifting plan. So, this paper proposes an optimization method of location for cooperative lifting of dual-crane. This approach starts with determining search field and maximum step length, and then finds out load’s possible initial locations by bisection method; secondly calculates work envelope of dual-crane which will be dispersed to find all dual-crane’s possible locations; after this, it puts cranes in their respective possible locations corresponding to each load’s location and simulates the whole lifting course while carries out collision detection to exclude crash happened locations; finally, creates weighted optimization function, uses enumeration method to traverse all collision-free locations, and gains optimal location of dual-crane and load. At last, this approach has been used in an actual engineering case and optimal location is got, from which we can see its feasibility and validity.",2012
"Application of Dynamic State Variable Models for Multiple-Generation Product Lines With Cannibalization Across Generations","Developing multiple-generation of products has become a mainstream tactic in today’s markets. The most notable case is Apple Inc.’s huge success with its iPod, iPhone and iPad product lines. Multiple-generation product lines require carefully planned strategies. Under a multiple-generation product development strategy, companies introduce a line of products to the market instead of introducing a single product to better utilize technology assets and resources in an elongated time span. For such product development and launch scenarios, cannibalization can occur, however. That is, multiple product generations compete in the same market and partition the company’s market shares. In the paper, we propose a new framework to predict the sales and introduction timing for every product generation in a multiple-generation product line while considering cannibalization. We demonstrate a case study implementing the proposed framework on Apple Inc.’s iPhone product line. The results show that the forecast performance of the model matches the realized real data.",2012
"Model Development Under Uncertainty via Conjoint Analysis","Model development decisions are critical in the early phases of engineering design. Engineering models serve as representations of reality that help designers understand input/output relationships, answer ‘what-if’ questions, and find optimal design solutions. Upon making model development decisions, the designer commits a large percentage of the costs associated with reaching design goals/objectives. The decisions dictate cost-drivers such as experimental setups and computation time. Unfortunately, the desire to develop the most accurate model competes with the desire to reduce costs. The designer is ultimately required to make trade-offs between attributes when choosing the best model development decision. Hence it is critical to develop tools for selecting the model development decision that appropriately balances trade-offs. A framework is proposed for model development decision-making. Conjoint Analysis (CA) is implemented in order to handle trade-offs among attributes. Thus, the framework can be used to make optimal decisions based on the assessment of multiple attributes. Moreover, the framework addresses the uncertainty that exists early in model design. Imprecision in model parameters are estimated and propagated through the model. In particular, the proposed decision framework is employed to select the optimal model development decision with respect to the final phase of experimentation. Preference intervals are evaluated in order to choose which final experimentation to perform. The decision framework proves to be useful for making model development decisions under uncertainty by considering the preference of multiple attributes and the imprecision of said attributes that is prevalent in early model development phases.",2012
"Hybrid Manufacturing System Modeling and Development","Reliable and economical fabrication of metallic parts with complicated geometries is of considerable interest for the aerospace, medical, automotive, tooling and consumer products industries. In an effort to shorten the time-to-market, decrease the manufacturing process chain, and cut production costs of products produced by these industries, research has focused on the integration of multiple unit manufacturing processes into one machine. The end goal is to reduce production space, time, and manpower requirements.",2012
"Stochastic Reassembly for Managing the Information Complexity in Multilevel Analysis of Heterogeneous Materials","Efficient and accurate analysis of materials behavior across multiple scales is critically important in designing complex materials systems with exceptional performance. For heterogeneous materials, apparent properties are typically computed by averaging stress-strain behavior in a statistically representative cell. To be statistically representative, such cells must be larger and are often computationally intractable, especially with standard computing resources. In this research, a stochastic reassembly approach is proposed for managing the information complexity and reducing the computational burden, while maintaining accuracy, of apparent property prediction of heterogeneous materials. The approach relies on a hierarchical decomposition strategy that carries the materials analyses at two levels, the RVE (representative volume element) level and the SVE (statistical volume element) level. The hierarchical decomposition process uses clustering methods to group SVEs with similar microstructure features. The stochastic reassembly process then uses t-testing to minimize the number of SVEs to garner their own apparent properties and fits a random field model to high-dimensional properties to be put back into the RVE. The RVE thus becomes a coarse representation, or “mosaic,” of itself. Such a mosaic approach maintains sufficient microstructure detail to accurately predict the macro-property but becomes far cheaper from a computational standpoint. A nice feature of the approach is that the stochastic reassembly process naturally creates an apparent-SVE property database. Thus, material design studies may be undertaken with SVE-apparent properties as the building blocks of a new material’s mosaic. Some simple examples of possible designs are shown. The approach is demonstrated on polymer nanocomposites.",2012
"Concept Analysis for Reconfigurable Products","There has been a significant amount of research investigating the design task of concept analysis, and much research on reconfigurable system design. Despite previous efforts, further research is still needed that explores how concept analysis should best be conducted for reconfigurable systems. Because reconfigurable systems have multiple configurations and different performance levels, additional information is required to understand each concept, making the design selection process more demanding. Desirable functions, concepts for each phase, and concepts for transition methods between configurations all need to be evaluated. In this paper, the fundamental challenges of reconfigurable system analysis are identified. A framework is then developed to evaluate the many options a designer may face when performing the concept analysis phase of reconfigurable system design, identifying the influence of reconfigurable systems on the process. A mini unmanned aerial vehicle case study is used to demonstrate how the framework is applied, and how existing concept analysis tools can be adapted to account for additional criteria. The paper concludes with a review of the contributions of this work and identifies areas of future work.",2012
"Design of Origami Sheets for Foldable Object Fabrication","Reconfigurable structures that are enabled through the integration of multiple materials are important for future design and manufacturing practice. We investigate one of such reconfigurable structures — an origami sheet, which can be designed based on a 3D object and unfolded into a 2D sheet with complex creases. A fabrication approach based on a hybrid manufacturing process by integrating layer-based additive manufacturing and silicon molding techniques is developed. Related challenges on designing creases for given folding requirements and the related material properties are discussed. A novel structure design is presented to ensure the fabricated creases that are in soft materials can be folded and unfolded without failures. The design method can be applied to different scale levels. The origami sheets for test cases in different complexity have been tested. The experimental results illustrate that the designed and fabricated origami sheets can be folded and used for product components with reconfigurable shapes.",2012
"Optimal Design of a Simplified Morphing Blade for Fixed-Speed Horizontal Axis Wind Turbines","The aim of designing the wind turbine blades is to improve the power capture ability. Since the rotor control technology is currently limited to controlling the rotor rotational speed and the pitch of the blades, an increasing concern has been given to the morphing blades. In this paper, a simplified morphing blade is introduced, which has a linear twisted distribution along the span and its shape can be controlled by adjusting the root twisted angle and the tip twisted angle of the blade. Moreover, to evaluate the performances of the wind turbine blades, a numerical code based on the blade element momentum theory is developed and validated. The blade of the NREL Phase VI wind turbine is taken as a reference blade, and the optimization problems associated with the morphing blade and pitch control blade are both formulated. The optimal results show that the morphing blade gives better results than the pitch control blade in terms of produced power. Under the assumption that in a given site, the annual average wind speed is known and the wind speed follows the Rayleigh distribution, we can evaluate the annual energy produced by these three blade types. While the annual average wind speed varies from 5 m/s to 15 m/s, the results show that the optimal morphing blade can increase 23.9 percent to 71.4 percent in annual energy production while the optimal pitch control blade can increase 22.5 percent to 67.4 percent in annual energy production, over the existing twisted pitch fixed blade.",2012
"Optimization of Wind Farm Layout and Wind Turbine Geometry Using a Multi-Level Extended Pattern Search Algorithm That Accounts for Variation in Wind Shear Profile Shape","This paper presents a multi-level Extended Pattern Search algorithm (EPS) to optimize both the local positioning and geometry of wind turbines on a wind farm. Additionally, this work begins to draw attention to the effects of atmospheric stability on wind farm power development. The wind farm layout optimization problem involves optimizing the local position and size of wind turbines such that the aerodynamic effects of upstream turbines are reduced, thereby increasing the effective wind speed at each turbine, allowing it to develop more power. The extended pattern search, employed within a multi-agent system architecture, uses a deterministic approach with stochastic extensions to avoid local minima and converge on superior solutions compared to other algorithms. The EPS presented herein is used in an iterative, hierarchical scheme — an overarching pattern search determines individual turbine positioning, then a sub-level EPS determines the optimal hub height and rotor for each turbine, and the entire search is iterated. This work also explores the wind shear profile shape to better estimate the effects of changes in the atmosphere, specifically the changes in wind speed with respect to height on the total power development of the farm. This consideration shows how even slight changes in time of day, hub height, and farm location can impact the resulting power. The objective function used in this work is the maximization of profit. The farm installation cost is estimated using a data surface derived from the National Renewable Energy Laboratory (NREL) JEDI wind model. Two wind cases are considered: a test case utilizing constant wind speed and unidirectional wind, and a more realistic wind case that considers three discrete wind speeds and varying wind directions, each of which is represented by a fraction of occurrence. Resulting layouts indicate the effects of more accurate cost and power modeling, partial wake interaction, as well as the differences attributed to including and neglecting the effects of atmospheric stability on the wind shear profile shape.",2012
"Policy Design for Sustainable Energy Systems Considering Multiple Objectives and Incomplete Preferences","The focus of this paper is on policy design problems related to large scale complex systems such as the decentralized energy infrastructure. In such systems, the policy affects the technical decisions made by stakeholders (e.g., energy producers), and the stakeholders are coordinated by market mechanisms. The decentralized decisions of the stakeholders affect the sustainability of the overall system. Hence, appropriate design of policies is an important aspect of achieving sustainability. The state-of-the-art computational approach to policy design problem is to model them as bilevel programs, specifically mathematical programs with equilibrium constraints. However, this approach is limited to single-objective policy design problems and is based on the assumption that the policy designer has complete information of the stakeholders’ preferences. In this paper, we take a step towards addressing these two limitations. We present a formulation based on the integration of multi-objective mathematical programs with equilibrium constraints with games with vector payoffs, and Nash equilibra of games with incomplete preferences. The formulation, along with a simple solution approach, is presented using an illustrative example from the design of feed-in-tariff (FIT) policy with two stakeholders. The contributions of this paper include a mathematical formulation of the FIT policy, the extension of computational policy design problems to multiple objectives, and the consideration of incomplete preferences of stakeholders.",2012
"Toward an Interactive Visualization Environment for Architecting Microgrids in Ultra Low Energy Communities","In this paper we describe the development of an interactive visualization tool to support the design and evaluation of microgrid architectures in ultra low energy communities. The work is motivated by recent Department of Defense regulations to reduce energy costs at and increase energy conservation at military installations. Using two sets of energy analysis models derived from existing energy modeling software packages, we illustrate how such a design environment can be used to (1) run a fast, low fidelity model to support an initial trade space exploration, (2) understand key trends and relationships, (3) filter microgrid architectures based on desired constraints, (4) identify architectures of interest, (5) run high fidelity analyses for architectures of interest, and (6) select an architecture and use a map view to change device type locations. The process is demonstrated through a web-based design environment that we prototyped and applied to two design examples. In both cases, promising microgrid architectures are identified from an initial set of 500 randomly generated designs. Manual adjustments of the position and location of the device types were used to further improve system performance. The end result in each case was a microgrid architecture that offered low fixed and operating costs based on the assumed electrical and thermal loads. The prototype effectively illustrates how Visual Analysis might be performed during Steps 4 & 5 of the Army’s Real Property Master Planning Process. Future enhancements to support the design decision-making process are also discussed.",2012
"Integrated Optimization of a Solar-Powered Humidification-Dehumidification Desalination System for Small Communities","Fresh water availability is essential for the economic growth and development, especially in small and semi-isolated communities. In some of these communities fresh water may be scarce, yet brackish water from wells or seawater is often abundantly available. This motivates a need for cost-effective desalination at small scale capacity driven by renewable energy sources. This paper presents an integrated optimization model of a solar powered humidification-dehumidification (HDH) desalination system. The system under investigation is a water-heated system. The design variables include the sizing of solar collectors, storage tank, humidifier and dehumidifier, as well as air circulation flow rate and operating temperature. The objective of the optimization is to minimize the unit cost of the produced fresh water. Thermodynamic performance prediction is done by solving energy and mass balance equations for each of the system components, with consideration to hourly-varying solar irradiance that corresponds to a typical one year cycle. System cost is predicted via first-order estimators. A genetic algorithm is used to obtain the designs optimized for local climate and market. A case study discusses a desalination plant on the Red Sea near the city of Hurgada.",2012
"Prognosis Informed Design Framework for Operation and Maintenance of Wind Turbines","Advances in high performance sensing and signal processing technology enable the development of failure prognosis tools for wind turbines to detect, diagnose, and predict the system-wide effects of failure events. Although prognostics can provide valuable information for proactive actions in preventing system failures, the benefits have not been fully utilized for the operation and maintenance decision making of wind turbines. This paper presents a generic failure prognosis informed decision making tool for wind farm operation and maintenance while considering the predictive failure information of individual turbine and its uncertainty. In the presented approach, the probabilistic damage growth model is used to characterize individual wind turbine performance degradation and failure prognostics, whereas the economic loss measured by monetary values and environmental performance measured by unified carbon credits are considered in the decision making process. Based on the customized wind farm information inputs, the developed decision making methodology can be used to identify optimum and robust strategies for wind farm operation and maintenance in order to maximize the economic and environmental benefits concurrently. The efficacy of proposed prognosis informed maintenance strategy is compared with the condition based maintenance strategy and demonstrated with the case study.",2012
"An Experimental Approach to Assess the Disparities in the Usage Trends of Domestic Electric Lighting","In a country like France, electricity consumption devoted to domestic lighting represents nearly a fifth of the total energy consumption of a building. The use of electric lighting is influenced by several factors such as the building’s structural characteristics, the activities of its occupants, the lighting equipments, and the level of natural light. Designers do take into account, in their energy models, the influence of occupants on the building’s overall energy consumption. However, these models still have some drawbacks regarding the comprehension of real “occupants’ energy behaviors” which play an important role in the discrepancies between predicted and real energy consumptions. The behavioral factors behind occupants’ usage trends of energy are still not thoroughly explored. Therefore, it is assumed that a better comprehension of these behaviors and consumption mechanisms could lead to the identification of technical solutions and energy saving potentials, thus resulting in a more robust building design.",2012
"Designing Resource-Efficient Military Base Camps From a Holistic Perspective","The U.S. Department of Defense (DoD) has recently shown an interest in incorporating resource efficiency into decision-making processes, including decisions that pertain to Forward Operating Military Base Camp (FOB) equipment. Often deployed in environments without access to grid utilities, FOBs require costly deliveries by land or air of resources such as fuel and fresh water. Resource-efficient FOB designs have the potential to reduce supply costs, but competing objectives and uncertain operational conditions complicate the design process. For example, integration of solar photovoltaic panels into existing designs has the potential to reduce the need to burn fuel in generators, however solar panels have up-front logistical and monetary costs that limit widespread use. There are also uncertainties associated with available solar energy and camp electrical loads. The research described here uses computer modeling and simulation of a real FOB subsystem under different operational scenarios to develop configurations of solar panels and batteries that, when integrated with an existing FOB design, maximize resource savings but minimize logistical and monetary costs, showing the benefit of a holistic design strategy that accounts for scenario variation. This research will also show that while different hardware configurations prove most efficient under different scenarios and objectives, certain hardware configurations provide good performance under all scenarios and objectives.",2012
"Wind Farm Layout Optimization Considering Energy Generation and Noise Propagation","Wind farm design deals with the optimal placement of turbines in a wind farm. Past studies have focused on energy-maximization, cost-minimization or revenue-maximization objectives. As land is more extensively exploited for onshore wind farms, wind farms are more likely to be in close proximity with human dwellings. Therefore governments, developers, and landowners have to be aware of wind farms’ environmental impacts. After considering land constraints due to environmental features, noise generation remains the main environmental/health concern for wind farm design. Therefore, noise generation is sometimes included in optimization models as a constraint. Here we present continuous-location models for layout optimization that take noise and energy as objective functions, in order to fully characterize the design and performance spaces of the optimal wind farm layout problem. Based on Jensen’s wake model and ISO-9613-2 noise calculations, we used single- and multi-objective genetic algorithms (NSGA-II) to solve the optimization problem. Preliminary results from the bi-objective optimization model illustrate the trade-off between energy generation and noise production by identifying several key parts of Pareto frontiers. In addition, comparison of single-objective noise and energy optimization models show that the turbine layouts and the inter-turbine distance distributions are different when considering these objectives individually. The relevance of these results for wind farm layout designers is explored.",2012
"An Ensemble Approach for Robust Data-Driven Prognostics","Prognostics aims at determining whether a failure of an engineered system (e.g., a nuclear power plant) is impending and estimating the remaining useful life (RUL) before the failure occurs. The traditional data-driven prognostic approach involves the following three steps: (Step 1) construct multiple candidate algorithms using a training data set; (Step 2) evaluate their respective performance using a testing data set; and (Step 3) select the one with the best performance while discarding all the others. There are three main challenges in the traditional data-driven prognostic approach: (i) lack of robustness in the selected standalone algorithm; (ii) waste of the resources for constructing the algorithms that are discarded; and (iii) demand for the testing data in addition to the training data. To address these challenges, this paper proposes an ensemble approach for data-driven prognostics. This approach combines multiple member algorithms with a weighted-sum formulation where the weights are estimated by using one of the three weighting schemes, namely the accuracy-based weighting, diversity-based weighting and optimization-based weighting. In order to estimate the prediction error required by the accuracy- and optimization-based weighting schemes, we propose the use of the ",2012
"A Hybrid Inference Approach for Health Diagnostics With Unexampled Faulty States","System health diagnostics provides diversified benefits such as improved safety, improved reliability and reduced costs for the operation and maintenance of engineered systems. Successful health diagnostics requires the knowledge of system failures. However, with an increasing complexity it is extraordinarily difficult to have a well-tested system so that all potential faulty states can be realized and studied at product testing stage. Thus, real time health diagnostics requires automatic detection of unexampled faulty states through the sensory signals to avoid sudden catastrophic system failures. This paper presents a hybrid inference approach (HIA) for structural health diagnosis with unexampled faulty states, which employs a two-fold inference process comprising of preliminary statistical learning based anomaly detection and artificial intelligence based health state classification for real time condition monitoring. The HIA is able to identify and isolate the unexampled faulty states through interactively detecting the deviation of sensory data from the known health states and forming new health states autonomously. The proposed approach takes the advantages of both statistical approaches and artificial intelligence based techniques and integrates them together in a unified diagnosis framework. The performance of proposed HIA is demonstrated with a power transformer and roller bearing health diagnosis case studies, where Mahalanobis distance serves as a representative statistical inference approach.",2012
"A Copula Based Sampling Method for Residual Life Prediction of Engineering Systems Under Uncertainty","The success of health prognostics of engineering systems will allow engineers to shift the traditional breakdown and time based maintenance to the state-of-art predictive and condition-based maintenance. Performing the right type of maintenance activity at the right time will minimize maintenance costs and the downtime of engineering systems. However, techniques and methodologies for health prognostics are typically application-specific. This paper aims at developing a generic real time sensor-based prognostic methodology for predicting residual life of engineering systems by modeling explicit relationship between the failure time and the time realizations at different degradation levels. Specifically, a Copula based sampling method is proposed with four technical components for off-line training and on-line life prediction. First of all, degradation signals are pre-processed to have non-decreasing degradation data sets. Next, degradation data sets are dicretized into a certain number of degradation levels with associated time realizations. Then, explicit statistical dependence modeling between the failure time and the time realizations at different degradation levels is conducted using the Bayesian Copula approach and the semi-Copula model. Finally, probability density function of the failure time and the residual life are efficiently predicted using the sampling method provided that we know some true time realizations at a certain number of degradation levels. Residual life predictions of electric cooling fans are employed to demonstrate the proposed method.",2012
"Application of Bayesian Sensor Placement Optimization for Real–Time Health Monitoring","Sensors are being increasingly used for real–time health monitoring of complex systems. The measured quantities are expected to provide real–time information about the state of the system, its subsystems, components, and internal and external physical parameters. A complex system normally requires many sensors to extract required information from the sensed environment. The increasing costs of aging systems and infrastructures have become a major concern and real–time health monitoring systems could ensure increased safety and reliability of these systems. Real–time system health monitoring, assesses the state of systems’ health and, through appropriate data processing and interpretation, can predict the remaining life of the system. This paper introduces a method based on Bayesian networks and attempts to find optimum locations of sensors for the best estimate a system health. Information metrics are used for optimized sensor placement based on the value of information that each possible sensor placement scenario provides.",2012
"An Approach for Revealed Consumer Preferences for Technology Products: A Case Study of Residential Solar Panels","Consumer preferences can serve as an effective basis for determining key product attributes necessary for market success, allowing firms to optimally allocate time and resources toward the development of these critical attributes. However, identification of consumer preferences can be challenging, particularly for technology-push products that are still early on in the technology diffusion S-curve, which need an additional push to appeal to the early majority. This paper presents a method for revealing preferences from actual market data and technical specifications. The approach is explored using three machine learning methods: Artificial Neural Networks, Random Forest decision trees, and Gradient Boosted regression applied on the residential photovoltaic panel industry in California, USA. Residential solar photovoltaic installation data over a period of 5 years from 2007–2011 obtained from the California Solar Initiative is analyzed, and 3 critical attributes are extracted from a pool of 34 technical attributes obtained from panel specification sheets. The work shows that machine learning methods, when used carefully, can be an inexpensive and effective method of revealing consumer preferences and guiding design priorities.",2012
"Incorporating Social Impact on New Product Adoption in Choice Modeling: A Case Study in Green Vehicles","While discrete choice analysis is prevalent in capturing consumers’ preferences and describing their choice behaviors in product design, the traditional choice modeling approach assumes that each individual makes independent decisions, without considering the social impact. However, empirical studies show that choice is social — influenced by many factors beyond engineering performance of a product and consumer attributes. To alleviate this limitation, we propose a new choice modeling framework to capture the dynamic influence from social network on consumer adoption of new products. By introducing the social influence attributes into the choice utility function, the social network simulation is integrated with the traditional discrete choice analysis in a three-stage process. Our study shows the need for considering social impact in forecasting new product adoption. Using hybrid electric vehicle as an example, our work illustrates the procedure of social network construction, social influence evaluation, and choice model estimation based on data from National Household Travel Survey. Our study also demonstrates several interesting findings on the dynamic nature of new technology adoption and how social network may influence consumers’ “green attitude” in hybrid electric vehicle adoption.",2012
"Consider-Then-Choose Models in Decision-Based Design Optimization","This article describes an advance in design optimization that includes consumer purchasing decisions. Decision-Based Design optimization commonly relies on Discrete Choice Analysis (DCA) to forecast sales and revenues for different product variants. Conventional DCA, which represents consumer choice as a compensatory process through maximization of a smooth utility function, has proven to be reasonably accurate at predicting choice and interfaces easily with engineering models. However the marketing literature has documented significant improvement in modeling choice with the use of models that incorporate non-compensatory (descriptive) and compensatory (predictive) components. The non-compensatory component can, for example, model a “consider-then-choose” process in which potential customers first narrow their decisions to a small set of products using heuristic screening rules and then employ a compensatory evaluation to select from this set. This article demonstrates that ignoring consider-then-choose behavior can lead to sub-optimal designs, and that optimality cannot be “recovered” by changing marketing variables alone. A new computational approach is proposed for solving optimal design problems with consider-then-choose models whose screening rules are based on conjunctive (logical “and”) rules. Computational results are provided using three state-of-the-art commercial solvers (matlab, KNITRO, and SNOPT).",2012
"Numerically Stable Design Optimization With Price Competition","Researchers in Decision-Based Design have asserted that business objectives, e.g. profits, should replace engineering requirements or performance metrics as the objective for engineering design. Using profits as the objective for engineering design, however, requires modeling consumer preferences and competition between firms. Game theoretic “design-then-pricing” models—i.e. product design with price competition—provide an important framework for integrating consumer preferences and competition when design decisions must be made before prices are decided by a firm or by its competitors. This article proposes a method for solving design-then-pricing problems that exhibits improved efficiency and reliability, relative to existing methods. Numerical results for a vehicle design example using three solvers—matlab, KNITRO, and SNOPT—to validate this claim. We also highlight the importance of checking the Second-Order Sufficient Conditions in design-then-pricing problems that use Mixed Logit models of demand.",2012
"Design and Selection of Safe Water Supply Solutions for Emerging Regions: A Demography Based Demand Driven Approach","Due to global climate change, increase in pollution along with reduced quantity of drinking water compared to the total volume of water, the scarcity of potable water is declining gradually. Researchers have become increasingly interested in efficient design of treatment processes, but, there is a lack of research to investigate appropriate, applicable, low cost and simple water treatment processes for underprivileged communities. Providing safe drinking water in these communities is more challenging due to limitation of resources and infrastructure.",2012
"Modular Product Optimization to Alleviate Poverty: An Irrigation Pump Case Study","Modular products have the potential to significantly reduce the financial risks associated with purchasing an income generating product in the developing world. Their modular nature allows a product to adapt to the changing needs of the customer (changing views of affordability due to increase in income potential). In a previous work by the authors, an optimization-based modular product design method was developed and implemented in the design of a modular irrigation pump for poverty alleviation. This paper revisits this modular pump example with the purpose of physically validating the ability of the method to identify theoretical progressively affordable modular products. This paper gives a summary of the method, presents the theoretical pump design, and compares the performance of the theoretical design to a physical prototype of the pump. Based on observations from this comparison, the authors conclude that the method is a feasible approach to engineering-based poverty alleviation.",2012
"Designing Products for Optimal Collaborative Performance With Application to Engineering-Based Poverty Alleviation","Collaborative products are created by combining components from two or more products to result in an additional product that performs previously unattainable tasks. The resulting reduction in cost, weight, and size of a set of products needed to perform a set of functions makes collaborative products useful in the developing world. In this paper, a method for designing a set of products for optimal individual and collaborative performance is introduced. This is accomplished by: (i) characterizing the collaborative design space of the product set and collaborative product, (ii) defining areas of acceptable Pareto offset, (iii) identifying the combinations of designs that fall within the defined areas of acceptable Pareto offset for each product, and (iv) selecting the optimal set of product designs. An example is provided to illustrate this method and demonstrate its usefulness in designing collaborative products for both the developed and developing world. We conclude that the presented method is a novel, and useful, approach for balancing the inherent trade-offs between the performance of collaborative products and the product sets used to create them.",2012
"A Case Study of the Implementation and Maintenance of a Fee for Service Lighting System for a Rural Village in Sub Saharan Africa","Electricity is a critical need for the rural poor in developing countries. Often this need is met with disposable batteries. This results in high cost and problems with disposal. For example it was recently reported that an isolated rural village in West Africa with a population of 770 uses more than 21,000 disposable batteries per year and that purchase of these batteries accounts for 20–40% of household expense. As a result many organizations are seeking way to meet the need for village energy. This paper presents a case study of one such experience. In this study the efforts to meet the lighting needs of a cluster of eight rural villages with a population of approximately 8,000 people are discussed. A key aspect of this discussion is the challenge of creating a continuing and sustainable village lighting solution. In this case the technology chosen to implement a lighting system was a distributed micro-grid managed locally in each village. The success of this lighting grid has been in large part due to the continuing support of the local micro-grid system both financially and through continued engagement to maintain and upgrade the micro-grid systems.",2012
"Exploration of the Use of Design Methods With the Design Structure Matrix for Integrating New Technologies Into Large Complex Systems","Integrating products of basic technology research and development efforts into Large Complex Systems (LCSs) requires systematic approaches. It has been observed that because of the complexity associated with LCSs, no single structured design method will suffice for integrating new technologies into an LCS. In this work, we explore through the literature how an integrated design approach involving the Design Structure Matrix (DSM) with several design methods (mainly those involving other matrix-based methods) might support the introduction of new technologies into large complex facilities. The survey presented in the paper could provide support for future investigations on how to align the outcomes of R&D processes with the requirements of introducing new technologies in target LCSs. Also it could help in developing future understandings about transitioning basic outcomes of R&D into technology products and services.",2012
"Optimal Process Architectures for Distributed Design Using a Social Network Model","Distributed design systems fundamentally preserve individual design subsystem secrecy by limiting communication across subsystems. The natural secrecy of distributed design makes it difficult for design process managers to determine the appropriate order of subsystems in the design process. In this paper, we discuss a social network theory based heuristic to prescribe the optimal order of design subsystems. We call the order of the design subsystems process architecture and we leverage concepts like ‘distance,’ ‘bridging,’ and degree centrality’ to analyze the aggregate design system and identify preferable solution process architectures. Our network theory approach only requires a manager to know which subsystems share design information. We distinguish this research from previous work by empirically validating the heuristic against a genetic algorithm for 80 randomly generated distributed design systems. The heuristic performs well against the genetic algorithm and beats it in the majority of cases. Moreover, it does so without requiring any function evaluations.",2012
"A Mathematical Programming-Based Approach for Architecture Selection","Modern systems are difficult to design because there are a significant number of potential alternatives to consider. The specification of an alternative includes an architecture (which describes the components and connections of the system) and component sizings (the sizing parameter for each component). In current practice, designers rely mainly on their experience and intuition to select a desired architecture without much computational support and then spend most of their effort on optimizing component sizings. In this paper, an approach for representing an architecture selection as a mixed-integer linear programming optimization is presented; existing solvers are then used to identify promising candidate architectures at early stages of the design process. Mathematical programming is a common optimization technique, but it is rarely used for architecture selection because of the difficulty of manually formulating an architecture selection as a mathematical program. In this paper, the formulation is presented in a modular fashion so that model transformations can be applied to transform a problem formulation that is convenient for designers into the mathematical programming optimization. A modular superstructure representation is used to model the design space; in a superstructure a union of all potential architectures is represented as a set of discrete and continuous variables. Algebraic constraints are added to describe both acceptable variable combinations and system behavior to allow the solver to eliminate clearly poor alternatives and identify promising alternatives. The framework is demonstrated on the selection of an actuation subsystem for a hydraulic excavator, although the solution approach would be similar for most mechanical systems.",2012
"Design of Cellular Self-Organizing Systems","Technology development is facing increased challenges as engineers begin to tackle the problem domains with greater uncertainty. Future engineered systems must be able to function in unpredictable environments such as deep ocean, rough terrain, and outer space while performing uncertain tasks like hazardous waste cleanup and search-and-rescue missions. Furthermore, the increasing size of engineered systems introduces unplanned interdependencies of components. Complex systems can provide the adaptability in order to manage uncertainties that traditional systems cannot. As the uncertainty of the problem domain increases, engineering design methods must be advanced in order to properly address the changing needs and constraints. This paper proposes a new approach inspired by natural phenomena in order to extend the design envelope towards an ",2012
"An Analysis of Complexity Measures for Product Design and Development","Complexity metrics have been developed for multiple applications such as consumer products, software, trajectory selection and assembly systems. Although existing complexity metrics were developed to reduce product design and development costs, their lack of simplicity in formulation and robustness has limited their applicability. This paper proposes a standard methodology for comparing and evaluating these metrics and introduces dimensions of complexity that should be considered towards the goal of developing a generalizable product complexity measure. To this end, this paper introduces variables that integrate multiple facets of complexity into a single metric.",2012
"Identifying Key Product Attributes and Their Importance Levels From Online Customer Reviews","Identifying customer needs and preferences is one of the most important tasks in design process. Typically, a variation of interview based approaches is used to conduct need and preference analysis. In this paper, a new approach based on text mining online (internet based) customer reviews to supplement traditional methods of need and preference analysis is considered. The key idea underlying the proposed approach is to partition online customer generated product reviews into segments that evaluate the individual attributes of a product (e.g zoom capability and support of different image formats in a camcorder). Additionally, the proposed method also identifies the importance (ranking) that customers place on each product attributes. The method is demonstrated on 100 customer reviews submitted for camcorders on epinions.com over a two year period.",2012
"On Design Preference Elicitation With Crowd Implicit Feedback","We define preference elicitation as an interaction, consisting of a sequence of computer queries and human implicit feedback (binary choices), from which the user’s most preferred design can be elicited. The difficulty of this problem is that, while a human-computer interaction must be short to be effective, query algorithms usually require lengthy interactions to perform well. We address this problem in two steps. A black-box optimization approach is introduced: The query algorithm retrieves and updates a user preference model during the interaction and creates the next query containing designs that are both likely to be preferred and different from existing ones. Next, a heuristic based on accumulated elicitations from previous users is employed to shorten the current elicitation by querying preferred designs from previous users (the “crowd”) who share similar preferences to the current one.",2012
"Quantifying Customer Sacrifice for Use in Product Customization Problems","Every time a customer selects a product from the shelf they make a purchase decision based on trade-offs between available offerings. The available products often exhibit feature excess at a price premium, feature deficiency at a price discount, or some combination of both. By purchasing one of these products a customer experiences some degree of sacrifice. This paper proposes the use of choice-based conjoint analysis and hierarchical Baysian modeling to calculate the perceived utility of a customer’s ideal product and the perceived utility of the best current alternative in the market. A customer’s sacrifice gap, a quantity that mass customization seeks to minimize, is defined as the difference between these values. This paper quantifies a market-average sacrifice gap and uses it in a theoretical product platform customization scenario. This scenario examines the effects of offering customization options on one attribute of a product at a time on a customer-centric objective (sacrifice gap) and a firm-centric objective (aggregate contribution). The results are also used to examine how customer sacrifice is minimized at an individual-level.",2012
"Modeling and Identification for Rotary Geometric Errors of Five-Axis Machine Tools With R-Test Measurement","The main purpose of this study is to use an R-test measurement device to estimate the geometric location error of the axis of rotation of five-axis machine tools. The error model of CNC machine tool describes the relationship between the individual error source and its effects on the overall position errors. This study based ISO230 to construct a geometric error model used to measure errors in the five-axis machine tools for the R-test measurement device. This model was then used to reduce the five-axis geometric error model based solely on the location error of the axis of rotation. Moreover, based on the simplified model and the overall position errors measured by the R-test with path K4, the location errors of rotary axes and ball position errors can be estimated very accurately with the least square estimation method. Finally, paths K1 and K2 were used as testing paths. The results of the test showed that the model built in this study is accurate and is effective in estimating errors.",2012
"Representing Stresses That Arise in Parallel Assemblies That Contain Imperfect Geometry Allowed by Tolerances","This paper concerns the role of geometric imperfections on assemblies in which the location of a target part is dependent on supports at two features. In some applications, such as a turbo-machine rotor that is supported by a series of parts at each bearing, it is the interference or clearance at a functional target feature, such as at the blades that must be controlled. The first part of the paper relates the limits of location for the target part to geometric imperfections of other parts when stacked-up in parallel paths. In other applications where parts are flexible, deformations are induced when parts in parallel are assembled together by clamping. Presuming that perfectly manufactured parts have been designed to fit perfectly together and produce zero deformations, the clamping-induced deformations result entirely from the imperfect geometry produced during manufacture. The magnitudes and types of these deformations are a function of part dimensions and material stiffnesses, and they are limited by design tolerances that control manufacturing variations. The last part of the paper relates the limits on stresses in parts to functional tolerance limits that must be set at the beginning of a tolerance analysis of parts in any assembly.",2012
"Determining Optimal Building Direction for Rapid Prototyping Based on Unit Sphere Searching","Although Rapid prototyping, as a material-additive process, is able to create complex geometries that traditional material-removal processes cannot achieve, it still suffers from its limitation due to the layer manufacturing nature. In order to minimize staircase effects, selection of building orientation is an important step in implementing rapid prototyping processes. This paper presents a method to select the optimal direction that leads to minimized volumetric error in building a part from layer manufacturing processes. A unit sphere is discretized first to give potential directions in a 3-D space, and then each facet comprising an STL geometric model is mapped onto the discretized unit sphere as a great circle individually. Both an exhaustive search and a chain searching strategy are presented to identify the globally optimal direction for building a 3-D geometry. At the end of the paper, examples are presented to show the effectiveness of the method.",2012
"Research on Mechanism Model of Fixture Location Errors Analysis","A new methodology using mechanism to model fixture location error (FLE) is proposed. Typically, there are four elements related to accumulation and propagation of FLE in workpiece-fixture system (WFS), which are machining feature, process datum, location datum and locators, and the relative position variations among them correspond to various FLEs. In this research, the four elements are treated as link mechanism components, the concatenation between location datum and locators is represented by contact pair, which is transformed to kinematic pair in equivalent mechanism, and geometry & dimension tolerances are simulated by link mechanisms. The rules mapping from FLE elements to their equivalent mechanisms are established, and by which the WFS is transformed into an equivalent mechanism model system. The method is studied that the process parameters of WFS are calculated by the structure parameters and kinematic parameters of equivalent mechanism. Thus, the solution of location error can be realized by means of the position analysis methods of mechanism.",2012
"Combining Case Based Reasoning and Shape Matching Based on Clearance Analyzes to Support the Reuse of Components","For manufacturing companies it is important to develop and produce products that meet requirements from customers and investors. One key factor in meeting these requirements is the efficiency of the product development process. Design automation is a powerful tool to increase efficiency in that process resulting in shortened lead-time, improved product performance, and ultimately decreased cost. Further, automation is beneficial as it increases the ability to adapt products to new product specifications, which is critical to some categories of products.",2012
"Filling N-Sided Holes Using Trimmed B-Spline Surfaces","Using one single trimmed B-Spline surface to fill an n-sided hole is a much desired operation in CAD, but few papers have addressed this issue. The paper presents the method of using trimmed B-Spline surfaces to fill n-sided holes based on energy minimization or variational technique. The method is efficient and robust, and takes less than one second to fill n-sided holes with high quality B-Spline surfaces under complex constraints.",2012
"Novel Geometrical Approach to Designing Flow Channels","Many natural systems that transport heat, energy or fluid from a distributed volume to a single flow channel exhibit an analogous appearance to trees (examples include bronchial tubes, watersheds, lightening, and blood vessels). Several authors have proceeded with analytical methods to develop fractal or pseudo-fractal designs analogous to these natural instances. This implicates an implicit belief in some designers that there is an optimal attribute to this ‘tree-like’ appearance. A novel explanation for the appearance of these systems is presented in this paper. Natural systems follow the path of least resistance; or in other words, minimize transport effort. Effort is required to overcome all forms of friction (an unavoidable consequence of motion). Therefore effort minimization is analogous to transport distance (path length) minimization. Effort due to friction will be integrated over the total transport distance. Leveraging this observation a simple, geometric explanation for the emergent ‘tree-like’ architecture of many natural systems is now achievable. Note that this ‘tree’ effect occurs when most of the flow volume exhibits diffusion, with a small percentage of interdigitated high flow velocity channels. One notable application of our novel method, ",2012
"Free-Form Optimal Conjugation Design","The paper presents the first optimal conjugation design methodology based on free-form conjugation modeling theory. The methodology is implemented for planar circular gearing and general for any planar gearing. According to the previous research of free-form conjugation, conjugate profiles are modeled by contact path geometries or cutter geometries which are represented by NURBS (non-uniform rational B-splines). The interchangeability between control points and interpolation points of NURBS are introduced in general to offer reasonable constraints of special interpolation conditions in conjugation design. To adapt to the flexibility brought by free-form techniques, the determination of an important conjugate property—contact ratio is carried out through geometric relationship. To make use of NURBS for optimal design, conjugate properties and their differentiations are represented by important parameters and then by control points and interpolation points of NURBS. The interested properties are relative curvature, specific sliding ratio and nominal Hertz contact stress, which are the main factors of gear efficiency and wear. The properties are well-known for their difficulties in optimization. The paper shows that with appropriate manipulations in mathematics and programming, it is feasible for gradient based optimization methods, which are accurate and fast in convergence. The methodology is consistent with the regular optimization of geometry design, and can be integrated into geometry design systems. The examples show the effectiveness of the proposed optimization framework.",2012
"Probabilistic Approach for Digital Human Kinematic and Dynamic Reliabilities","Traditionally, deterministic methods have been applied in digital human modeling (DHM). Transforming the deterministic approach of digital human modeling into a probabilistic approach is natural since there is inherent uncertainty and variability associated with DHM problems. Typically, deterministic studies in this field ignore this uncertainty or try to limit the uncertainty by employing optimization procedures. Due to the variability in the inputs, a deterministic study may not be enough to account for the uncertainty in the system. Probabilistic design techniques allow the designer to predict the likelihood of an outcome while also accounting for uncertainty, in contrast to deterministic studies. The purpose of this study is to incorporate probabilistic approaches to a deterministic DHM problem that has already been studied, analyzing human kinematics and dynamics. The problem is transformed into a probabilistic approach where the human kinematic and dynamic reliabilities are determined. The kinematic reliability refers to the probability that the human end-effector position (and/or orientation) falls within a specified distance from the desired position (and/or orientation) in an inverse kinematic problem. The dynamic reliability refers to the probability that the human end-effector position (and/or velocity) falls within a specified distance from the desired position (and/or velocity) along a specified trajectory in the workspace. The dynamic equations of motion for DHM are derived by the Lagrangian backward recursive dynamics formulation.",2012
"Trajectory Estimation of Human Mass Center Based on an Inertia Identification Approach","Mass center of a human body is not a fixed point on the human body because the inertia distribution of the human body changes with body posture. Real-time estimation of the location of human mass center is often required for many biomechanical or biomedical applications. This is not an easy task if the inertia properties of the human’s body segments are unknown. This paper presents a technique for estimating the trajectory of the human mass center based on a recently developed inertia properties identification technology which was derived based on the impulse-momentum principle. The proposed technique assumes a human body as a general treelike multibody system, such that the mass center of the human is predictable with the knowledge of the barycentric parameters of the human. The latter can be identified using inertia identification method. This technique is advantageous because it requires only the 3D motion capture data as its primary input and does not need to know the inertia and geometric parameters of individual body segments of the human. The paper presents a dynamic simulation based study of the proposed estimation technique and also describes an ongoing experimental testing.",2012
"Optimization-Based Prediction of Aiming and Kneeling Military Tasks Performed by a Soldier","The objective of this study is to predict the “Aiming While Standing” and “Aiming While Kneeling” motion tasks for a soldier (human) using a full-body, three dimensional digital human model. The digital human is modeled as a 55 degree of freedom branched mechanism. Six degrees of freedom specify the global position and orientation of the coordinate frame attached to the pelvis of the digital human and 49 degrees of freedom represent the revolute joints which model the human joints and determine the kinematics of the entire digital human. Motion is generated by a multi-objective optimization approach minimizing the mechanical energy and joint discomfort simultaneously. A sequential quadratic programming (SQP) algorithm in SNOPT is used to solve the nonlinear optimization problem. The optimization problem is subject to constraints which represent the limitations of the environment, the digital human model and the motion task. Design variables are the joint angle profiles. All the forces, inertial, gravitational as well as external, are known, except the ground reaction forces. The feasibility of the generation of that arbitrary motion by using the given ground contact areas is ensured by using the well known Zero Moment Point (ZMP) constraint. During the kneeling motion, different parts of the body come in contact and lose contact with the ground which is modeled using a general approach. The ground reaction force on each transient ground contact area is determined using the equations of motion. It is assumed that enough friction exists that allow the human to generate reaction forces as determined by the ZMP constraint. Using these ground reaction forces, the required torques at all joints are calculated by the recursive Lagrangian formulation. Using the given method, we can predict realistic motions for the “Aiming While Standing” and “Aiming While Kneeling” tasks. The optimization approach is able to very well predict the “Natural Point of Aim” which is a well known concept for soldiers. In other words, the approach is able to predict the most comfortable final orientation of the feet on the ground for engaging a specific target. We also predict cases where the orientation of the soldier’s feet are enforced. Many virtual experiments have been conducted by changing the target location in the 3D space, changing the anthropometry of the soldier, adding armor to different joints, changing the variable parameters of the rifle, adding backpack and using different weapons.",2012
"Predicting Energy Consumption in Humans Using Joint Space Methods","Humans act as transducers that transform chemical energy from food, water, and air into mechanical work and the thermal energy of heat loss. Although this energy expenditure can be experimentally measured, methods of predicting energy expenditure have not been broadly studied. This work introduces a new formulation of metabolic energy consumption based on muscle physiology and the equations of motion for the human body. Kinematic and kinetic data from a gait experiment and an over-arm throwing simulation are used to illustrate and validate this new model. The results extend the capabilities of dynamic human modeling to include metabolic energy prediction in general tasks. This novel formulation is useful for the investigation of human performance with applications in physical therapy, rehabilitation, and sports.",2012
"Towards Robustly Stable Musculo-Skeletal Simulation of Human Gait: Merging Lumped and Component-Based Modeling Approaches","The objective of work presented in this paper is to increase the center-of-mass stability of human walking and running in musculo-skeletal simulation. The approach taken is to approximate the whole-body dynamics of the low-dimensional Spring-Loaded Inverted Pendulum (SLIP) model of locomotion in the OpenSim environment using existing OpenSim tools. To more directly relate low-dimensional dynamic models to human simulation, an existing OpenSim human model is first modified to more closely represent bilateral above-knee amputee locomotion with passive prostheses. To increase stability further beyond the energy-conserving SLIP model, an OpenSim model based upon the Clock-Torqued Spring-Loaded-Inverted-Pendulum (CT-SLIP) model of locomotion is also created. The result of this work is that a multi-body musculo-skeletal simulation in Open-Sim can approximate the whole-body sagittal-plane dynamics of the passive SLIP model. By adding a plugin controller to the OpenSim environment, the Clock-Torqued-SLIP dynamics can be approximated in OpenSim. To change between walking and running, only one parameter representing the preferred period of a stride is changed. The result is a robustly stable simulation of the center-of-mass locomotion for both walking and running that could serve as a first step toward increasingly anatomically accurate and robustly stable musculo-skeletal simulations.",2012
"Genetic Algorithms Applied to Affordance Based Design","Affordance based design (ABD) theory has been presented in several papers and has interested several researchers in the field of design. One criticism of ABD is that the number of affordances identified can be very large, and therefore, the approach may not be amenable for automation. This paper presents a computer based implementation of a process to improve design using affordances.",2012
"A Classifier-Guided Sampling Method for Computationally Expensive, Discrete-Variable, Discontinuous Design Problems","Metamodel-based design is a well-established method for providing fast and accurate approximations of expensive computer models to enable faster optimization and rapid design space exploration. Traditionally, a metamodel is developed by fitting a surface to a set of training points that are generated with an expensive computer model or simulation. A requirement of this process is that the function being approximated is continuous. However, many engineering problems have variables that are discrete and a function response that is discontinuous in nature. In this paper, a classifier-guided sampling method is presented that can be used for optimization and design space exploration of expensive computer models that have discrete variables and discontinuous responses. The method is tested on a set of example problems. Results show that the method significantly improves the rate of convergence towards known global optima, on average, when compared to random search.",2012
"Adaptive Orthonormal Basis Functions for High Dimensional Metamodeling With Existing Sample Points","High Dimensional Model Representation (HDMR) is a tool for generating an approximation of an input-output model for a multivariate function. It can be used to model a black-box function for metamodel-based optimization. Recently the authors’ team has developed a radial basis function based HDMR (RBF-HDMR) model that can efficiently model a high dimensional black-box function and, moreover, to uncover inner variable structures of the black-box function. This approach, however, requests a complete new, although optimized, set of sample points, as dictated by the methodology, while in engineering design practice one often has many existing sample data. How to utilize the existing data to efficiently construct a HDMR model is the focus of this paper. We first identify the Random-Sampling HDMR (RS-HDMR), which uses orthonormal basis functions as HDMR component functions and existing sample points can be used to calculate the coefficients of the basis functions. One of the important issues related to the RS-HDMR is that in theory the basis functions are obtained based on the continuous integrations related to the orthonormality conditions. In practice, however, the integrations are approximated by Monte Carlo summation and thus the basis functions may not satisfy the orthonormality conditions. In this paper, we propose new and adaptive orthonormal basis functions with respect to a given set of sample points for RS-HDMR approximation. RS-HDMR models are built for different test functions using the standard and new adaptive basis functions for different number of sample points. The relative errors for both models are calculated and compared. The results show that the models that are built using the new basis functions are more accurate.",2012
"Objective–Oriented Sequential Sampling for Simulation Based Robust Design Considering Multiple Sources of Uncertainty","Sequential sampling strategies have been developed for managing complexity when using computationally expensive computer simulations in engineering design. However, much of the literature has focused on objective-oriented sequential sampling methods for deterministic optimization. These methods cannot be directly applied to robust design which must account for uncontrollable variations in certain input variables (i.e., noise variables). Obtaining a robust design that is insensitive to variations in the noise variables is more challenging. Even though methods exist for sequential sampling in design under uncertainty, the majority of the existing literature does not systematically take into account the interpolation uncertainty that results from limitations on the number of simulation runs, the effect of which is inherently more severe than in deterministic design. In this paper, we develop a systematic objective-oriented sequential sampling approach to robust design with consideration of both noise variable uncertainty and interpolation uncertainty. The method uses Gaussian processes to model the costly simulator and quantify the interpolation uncertainty within a robust design objective. We examine several criteria, including our own proposed criteria, for sampling the design and noise variables and provide insight into their performance behaviors. We show that for both of the examples considered in this paper the proposed sequential algorithm is more efficient in finding the robust design solution than a one-shot space filling design.",2012
"Indicator of Feasibility for Layout Problems","Layout optimization problems deal with the search for an optimal spatial arrangement of components inside a container. Global performances of many engineering products and systems sharply depend on layout design. Because of the great complexity of most real-world layout applications, recent studies focus on the development of efficient optimization algorithms used to solve layout problems. These algorithms, which have to take into consideration multi-constraint and multi-objective optimization problems, aim to be generic to adapt to lots of layout problems. However, the solving of these layout problems is time consuming and designers know if an optimal solution is available for their problem at the end of the optimization process. Designer cannot know a priori if the layout problem can be solved. Then, this paper proposes a new indicator to assess the feasibility of layout problems. This indicator is based on the layout description of the problem and the formulation of designer’s requirements. In particular, it takes into account the non-overlap constraints between layout components. It allows the designer to know if the layout problem can be solved before running the optimization algorithm. After defining the new indicator of feasibility, this paper tests it on a simple layout application.",2012
"A Nested Extreme Response Surface Approach for RBDO With Time-Dependent Probabilistic Constraints","A primary concern in practical engineering design is ensuring high system reliability throughout a product life-cycle subject to time-variant operating conditions and component deteriorations. Thus, the capability to deal with time-dependent probabilistic constraints in reliability-based design optimization is of vital importance in practical engineering design applications. This paper presents a nested extreme response surface (NERS) approach to efficiently carry out time-dependent reliability analysis and determine the optimal designs. The NERS employs kriging model to build a nested response surface of time corresponding to the extreme value of the limit state function. The efficient global optimization technique is integrated with the NERS to extract the extreme time responses of the limit state function for any given system design. An adaptive response prediction and model maturation mechanism is developed based on mean square error (MSE) to concurrently improve the accuracy and computational efficiency of the proposed approach. With the nested response surface of time, the time-dependent reliability analysis can be converted into the time-independent reliability analysis and existing advanced reliability analysis and design methods can be used. The NERS is integrated with RBDO for the design of engineered systems with time-dependent probabilistic constraints. Two case studies are used to demonstrate the efficacy of the proposed NERS approach.",2012
"Predicting the Thermal Performance for the Multi-Objective Vehicle Underhood Packing Optimization Problem","The ever increasing demands towards improvement in vehicle performance and passenger comfort have led the automotive manufacturers to further enhance the design in the early stages of the vehicle development process. Though, these design changes enhance the overall vehicle performance to an extent, the placement of these components under the car hood also plays a vital role in increasing the vehicle performance. In the past, a study on the automobile underhood packaging or layout problem was conducted and a multi-objective optimization routine with three objectives namely, minimizing center of gravity height, maximizing vehicle components accessibility and maximizing survivability (for army vehicles) has been setup to determine the optimal locations of the underhood components. The previous study did not consider thermal performance as an objective. This study asserts the necessity of including thermal performance as an objective and makes an assessment of the several available thermal analyses that are performed on the automotive underhood to evaluate the thermal objective. A Neural Network approximation of the CFD analysis conducted over the automotive underhood is presented in this paper. The results obtained from the Neural Network are compared with the CFD results, showing good agreement. The Neural Network model is included in the multi-objective optimization routine and new layout results are obtained. A non-deterministic evolutionary multi-objective algorithm (AMGA-2) is used to perform the optimization process.",2012
"Approximation Assisted Multiobjective Optimization With Combined Global and Local Metamodeling","Approximation Assisted Optimization (AAO) is widely used in engineering design problems to replace computationally intensive simulations with metamodeling. Traditional AAO approaches employ global metamodeling for exploring an entire design space. Recent research works in AAO report on using local metamodeling to focus on promising regions of the design space. However, very limited works have been reported that combine local and global metamodeling within AAO. In this paper, a new approximation assisted multiobjective optimization approach is developed. In the proposed approach, both global and local metamodels for objective and constraint functions are used. The approach starts with global metamodels for objective and constraint functions and using them it selects the most promising points from a large number of randomly generated points. These selected points are then “observed”, which means their actual objective/constraint function values are computed. Based on these values, the “best” points are grouped in multiple clustered regions in the design space and then local metamodels of objective/constraint functions are constructed in each region. All observed points are also used to iteratively update the metamodels. In this way, the predictive capabilities of the metamodels are progressively improved as the optimizer approaches the Pareto optimum frontier. An advantage of the proposed approach is that the most promising points are observed and that there is no need to verify the final solutions separately. Several numerical examples are used to compare the proposed approach with previous approaches in the literature. Additionally, the proposed approach is applied to a CFD-based engineering design example. It is found that the proposed approach is able to estimate Pareto optimum points reasonably well while significantly reducing the number of function evaluations.",2012
"The Skewboid Method: A Simple and Effective Approach to Pareto Relaxation and Filtering","The concept of Pareto optimality is the default method for pruning a large set of candidate solutions in a multi-objective problem to a manageable, balanced, and rational set of solutions. While the Pareto optimality approach is simple and sound, it may select too many or too few solutions for the decision-maker’s needs or the needs of optimization process (e.g. the number of survivors selected in a population-based optimization). This inability to achieve a target number of solutions to keep has caused a number of researchers to devise methods to either remove some of the non-dominated solutions via Pareto filtering or to retain some dominated solutions via Pareto relaxation. Both filtering and relaxation methods tend to introduce many new adjustment parameters that a decision-maker (DM) must specify.",2012
"A Genetic Algorithm Approach for Technology Characterization","It is important for engineers to understand the capabilities and limitations of the technologies they consider for use in their systems. Several researchers have investigated approaches for modeling the capabilities of a technology with the aim of supporting the design process. In these works, the information about the physical form is typically abstracted away. However, the efficient generation of an accurate model of technical capabilities remains a challenge. Pareto frontier based methods are often used but yield results that are of limited use for subsequent decision making and analysis. Models based on parameterized Pareto frontiers—termed Technology Characterization Models (TCMs)—are much more reusable and composable. However, there exists no efficient technique for modeling the parameterized Pareto frontier. The contribution of this paper is a new algorithm for modeling the parameterized Pareto frontier to be used as a model of the characteristics of a technology. The proposed algorithm uses fundamental concepts from multiobjective genetic optimization and machine learning to generate a model of the technology frontier.",2012
"Design and Optimization of Multiple Microchannel Heat Transfer Systems Based on Multiple Prioritized Preferences","Multiple microchannel heat transfer systems have been developed for the urge of rapid and effective cooling of the electronic devices, which have become smaller and more powerful but also produced more heat. Two different types of single-phase liquid cooling, including the straight and U-shaped microchannel heat sinks, have been utilized to reduce the temperature of the electronic chips. The cooling performances however depend on the preferences of different factors such as the thermal resistances, the pressure drops, and the heat flows at the solid-fluid interfaces. Lower thermal resistance represents higher temperature reduction; lower pressure drop means lower usage of the pumping power; and higher heat flows indicates more effective cooling between the heat spreader and the liquid. In this paper, an optimization strategy based on the prioritized performances has been developed to find the optimal design variables for multiple objectives: minimal thermal resistances, minimal pressure drops and maximal heat flows. The fuzzy and correlated preferences are modeled by the Gaussian membership functions with respect to different levels of the objective function values. The overall performances are formulated based on the prioritized preferences and maximized on the Pareto-optimal solution set to find the solutions for various preference conditions. Two case studies have been discussed. The first case considered the prioritized preferences based on uni-objective function values while the second one focused on the preferences of the thermal resistances and the efficiency measures, correlatively evaluated by the flow rates, pressure drops, and heat flows.",2012
"Use Scenarios for Design Space Exploration With a Dynamic Multiobjective Optimization Formulation","In a recent publication, we presented a new strategy for engineering design and optimization, which we termed formulation space exploration. The formulation space for an optimization problem is the union of all variable and design objective spaces identified by the designer as being valid and pragmatic problem formulations. By extending a computational search into this new space, the solution to any optimization problem is no longer predefined by the optimization problem formulation. This method allows a designer to both diverge the design space during conceptual design and converge onto a solution as more information about the design objectives and constraints becomes available. Additionally, we introduced a new way to formulate multiobjective optimization problems, allowing the designer to change and update design objectives, constraints, and variables in a simple, fluid manner that promotes exploration. In this paper, we investigate three use scenarios where formulation space exploration can be utilized in the early stages of design when it is possible to make the greatest contributions to development projects. Specifically, we look at s-Pareto frontier generation in the formulation space, formulation space boundary exploration, and a new way to perform inverse optimization. The benefits of these methods are illustrated with the conceptual design of an impact driver.",2012
"Application of Pontryagin’s Maximum Principle to Statistical Process Control Optimization","The optimization of a statistical process control with a variable sampling interval is studied. A control performance index is the expected loss, caused by delay in detecting process change. It is to be minimized by a proper choice of a sampling interval. The mathematical model of this problem is a nonstandard variational calculus problem with two types of constraints, an isoperimetric constraint and two geometric constraints. The integrands in the cost functional and the isoperimetric constraint are independent of the derivative of the minimizing function. Therefore, the classical Euler-Lagrange equation approach is not applicable when analyzing this extremal problem. The optimization problem depends on the signal-to-noise ratio parameter. The original problem is transformed to an equivalent optimal control problem. Based on the value of the parameter, the latter is decomposed into two simpler problems, solved by application of Pontryagin’s Maximum Principle. The theoretical results are evaluated by numerical simulations.",2012
"Network Target Coordination for Optimal Design of Decomposed Systems With Consensus Optimization","The complexity of managing multidisciplinary engineering systems offers an unprecedented opportunity to investigate decomposition methods, which separate a system into a number of smaller subsystems that can be designed in multiple physical locations and coordinate the design of the subsystems to collaboratively achieve the original system design. This paper studies a network target coordination model for optimizing subsystems that are distributed as multiple agents in a network. To solve these coupled subsystems concurrently, we consider the “consensus optimization” approach by incorporating subgradient algorithms so that the master problem or auxiliary design variables required by most distributed coordination methods are not needed. The method allows each agent to conduct its optimization by locally solving for coupling variables with the information obtained from other agents in the network in an iteratively improving process. The convergence results of a geometric programming problem that satisfies the convexity assumption is provided. Moreover, two non-convex examples are tested to investigate the convergence characteristics of the proposed methods.",2012
"Metamodel Based Design Automation: Applied on Multidisciplinary Design Optimization of Industrial Robots","Intricate and complex dependencies between multiple disciplines require iterative intensive optimization processes. To this end, multidisciplinary design optimization (MDO) has been established as a convincing concurrent technique to manage inherited complexities.",2012
"Plant-Limited Co-Design of an Energy-Efficient Counterbalanced Robotic Manipulator","Modifying the design of an existing system to meet the needs of a new task is a common activity in mechatronic system development. Often engineers seek to meet requirements for the new task via control design changes alone, but in many cases new requirements are impossible to meet using control design only; physical system design modifications must be considered. Plant-Limited Co-Design (PLCD) is a design methodology for meeting new requirements at minimum cost through limited physical system (plant) design changes in concert with control system redesign. The most influential plant changes are identified to narrow the set of candidate plant changes. PLCD provides quantitative evidence to support strategic plant design modification decisions, including tradeoff analyses of redesign cost and requirement violation. In this article the design of a counterbalanced robotic manipulator is used to illustrate successful PLCD application. A baseline system design is obtained that exploits synergy between manipulator passive dynamics and control to minimize energy consumption for a specific pick-and-place task. The baseline design cannot meet requirements for a second pick-and-place task through control design changes alone. A limited set of plant design changes is identified using sensitivity analysis, and the PLCD result meets the new requirements at a cost significantly less than complete system redesign.",2012
"Design of Mechanical Structures Considering Harmonic Loads Using Level Set-Based Topology Optimization","This paper presents an optimum design method for mechanical structures considering harmonic loads using a level set-based topology optimization method and the Finite Element Method (FEM). First, we briefly discuss the level set-based topology optimization method. Second, a topology optimization problem is formulated for a dynamic elastic design problem using level set boundary expressions. The objective functional is set to minimize the displacement at specific boundaries. Based on this formulation, the topological sensitivities of the objective functional are derived. Next, a topology optimization algorithm is proposed that uses the FEM to solve the equilibrium and adjoint equations, and when updating the level set function. Finally, several numerical examples are provided to confirm the validity and utility of the proposed method.",2012
"The Improved Quadrilateral Discretization Model for the Discrete Topology Optimization of Structures","In the discrete topology optimization, material state is either solid or void and there is no topology uncertainty problem caused by intermediate material state. In this paper, the improved quadrilateral discretization model is introduced for the discrete topology optimization of structures. The design domain is discretized into quadrilateral design cells and each quadrilateral design cell is further subdivided into 16 triangular analysis cells. All kinds of dangling quadrilateral design cells and sharp-corner triangular analysis cells are removed in the improved quadrilateral discretization model to promote the material utilization. To make the designed structures safe, the local stress constraint is directly imposed on each triangular analysis cell. To circumvent the geometrical bias against the vertical design cells, the binary bit-array genetic algorithm is used to search for the optimal topology. The effectiveness of the proposed improved quadrilateral discretization model and its related discrete topology optimization method is verified by two topology optimization examples of structures.",2012
"Applications of 3D Level Set Topology Optimization","Level set topology optimization defines the solution using the level set function values stored at the nodes of a regular finite element grid. These values represent a signed distance function which indicates the distance from each node to the structural boundaries. During optimization, nodal sensitivities are used to update the level set function values, moving the boundaries to create a more optimal structure. This paper presents two applications of the 3D level set topology optimization procedure aiming to minimize structural compliance subject to a volume constraint. The first application is the internal structure of a light subsonic aircraft wing. The results suggest that an alternative arrangement of ribs and sparse may be a more optimal solution for wing structures. The second application is the internal trabecular bone structure of an os-calcis. Comparison of the modeled optimal structure and the real internal structure suggest the internal bone structure is mechanically optimal.",2012
"Steps in Transforming Shapes Generated With Generative Design Into Simulation Models","This paper introduces a platform that combines shape grammars with conventional simulation and analysis methods. The premise of this combination is to create an approach to synthesizing optimal shapes considering criteria requiring heat transfer and stress analysis for their evaluation. The necessary mechanisms and issues for integrating shape grammars with standard simulation systems are described. The benefits, challenges and future outlook of this approach with regards to traditional design synthesis systems are explored. Further, possible future research projects to extend the work are presented.",2012
"Meta-Material Design of the Shear Layer of a Non-Pneumatic Wheel Using Topology Optimization","The meta-material design of the shear layer of a non-pneumatic wheel was completed using topology optimization. In order to reduce the hysteretic rolling loss, an elastic material is used and the shear layer microstructure is defined to achieve high compliance comparable to that offered by the elastomeric materials. To simulate the meta-material properties of the shear layer, the volume averaging analysis, instead of more popular homogenization methods, is used as the relative size of the shear layer places realistic manufacturing constraints on the size of unit cells used to generate the meta-material. In this design scenario the properties predicted by the homogenization methods are not accurate since the homogenization scaling assumptions are violated. A number of optimal designs are shown to have meta-material properties similar to those of the linear elastic properties of elastomers, making them good meta-material candidates for the shear layer of the non-pneumatic wheel.",2012
"Topology Optimization of Blank Geometry for the Sheet Forming Process","The newly developed element exchange method (EEM) for topology optimization is applied to the problem of blank shape optimization for the sheet-forming process. EEM uses a series of stochastic operations guided by the structural response of the model to switch solid and void elements in a given domain to minimize the objective function while maintaining the specified volume fraction. In application of EEM to blank optimization, a sheet forming simulation model is developed using Abaqus/Explicit. With the goal of minimizing the variability in wall thickness of the formed component, a subset of solid (i.e., high density) elements with the highest increase in thickness is exchanged with a consistent subset of void (i.e., low density) elements having the highest decrease in thickness so that the volume fraction remains constant. The EEM operations coupled with finite element simulations are repeated until the optimum blank geometry (i.e., boundary and initial thickness) is found. The developed numerical framework is applied to blank optimization of a benchmark problem. The results show that EEM is successful in generating the optimum blank geometry efficiently and accurately.",2012
"Casting and Milling Restrictions in Topology Optimization via Projection-Based Algorithms","Projection-based algorithms are arising as a powerful tool for continuum topology optimization. They use independent design variables that are projected onto element space to create structure topology. The projection functions are designed so that geometric properties, such as the minimum length scale of features, are naturally achieved. They therefore offer an efficient means for imposing geometry-related design specifications and/or manufacturing constraints. This paper presents recent advances in projection-based algorithms, including topology optimization under manufacturing constraints related to milling and casting processes. The new advancements leverage the logic of recently proposed algorithms for Heaviside projection, including eliminating continuation methods on projection parameters and potential for using multiple design variables to achieve active projection of each phase used in design. The primary advantages of such an approach are that manufacturing restrictions are achieved naturally, without need for additional constraints, and that sensitivity calculations are efficient and straightforward. The primary drawback of the approach is that the so-called neighborhood maps require storage for efficient processing when using unstructured meshing.",2012
"Improved Clustering Algorithm for Design Structure Matrix","For clustering a large Design Structure Matrix (DSM), computerized algorithms are necessary. A common algorithm by Thebeau uses stochastic hill-climbing to avoid local optima. The output of the algorithm is stochastic, and to be certain a very good clustering solution has been obtained, it may be necessary to run the algorithm thousands of times. To make this feasible in practice, the algorithm must be computationally efficient. Two algorithmic improvements are presented. Together they improve the quality of the results obtained and increase speed significantly for normal clustering problems. The proposed new algorithm is applied to a cordless handheld vacuum cleaner.",2012
"Application of the Generational Variety Index: A Retrospective Study of iPhone Evolution","The generational variety index (GVI) helps identify the components of product variants that are most likely to require redesign in the future. These components can then be embedded with the flexibility required for them to be easily modifiable; the remaining components can be designed into a platform. This paper describes the application of the GVI technique in studying the evolution of the Apple iPhone, which was first released in 2007 and has since undergone multiple redesigns. The analysis includes the five generations of the iPhone (original, 3G, 3GS, 4, and 4S) and focuses primarily on mechanical sub-systems. The results of the analysis and subsequent design recommendations are compared with the actual design evolution of the iPhone product line. For certain subsystems, this comparison reveals a divergence in Apple’s design decision-making from the evolution recommended by the GVI technique. Limitations include its retrospective nature and the use of only publicly available data.",2012
"A Bisociative Design Framework for Knowledge Discovery Across Seemingly Unrelated Product Domains","The Bisociative Design framework proposed in this work aims to quantify hidden, previously unknown design synergies/insights across seemingly unrelated product domains. Despite the overabundance of data characterizing the digital age, designers still face tremendous challenges in transforming data into knowledge throughout the design processes. Data driven methodologies play a significant role in the product design process ranging from customer preference modeling to detailed engineering design. Existing data driven methodologies employed in the design community generate mathematical models based on data relating to a specific domain and are therefore constrained in their ability to discover novel design insights beyond the domain itself (I.e., cross domain knowledge).",2012
"Selecting and Optimizing a Regulation Compliant Robust Vehicle Portfolio Mix: An Approach and a Case Study","Our goal is to select a robust vehicle portfolio mix and optimize its design attributes such that contribution margin is maximized while being regulation compliant under varying fuel prices. Compliance to regulation is measured in terms of the Corporate Average Fuel Economy or CAFE. We formulate this vehicle portfolio optimization problem as a mixed integer non-linear programming problem, both under static and varying fuel price scenarios. We demonstrate our approach using a case study in which an in-house market simulator is employed for incorporating consumer preferences in portfolio decisions. This market simulator uses real-time preferences from tens of thousands of shoppers and captures preference heterogeneity using different Logit coefficients for each shopper and hence is computationally expensive. Also, it does not explicitly model the influence of fuel price in predicting demand. To overcome these issues and to facilitate portfolio optimization we use meta-models of the market simulator. Our results show that while remaining regulation compliant it is also possible to achieve significant improvement in the portfolio’s contribution margin. In some scenarios, the improvements in contribution margin are more than 40% when compared to the traditional approach of using expert judgment to decide the portfolio mix.",2012
"Global Product Family Design: Multi-Objective Optimization and Design Concept Exploration","Global product family design is the problem in which product variants and supply chain configuration are simultaneously designed. It has become a significant concern of manufacturing industries under globalization. Its context is not only complicated under various factors and their interactions but also vague under strategic decision making. In this paper, first, a multi-objective mixed-integer formulation of simultaneous design of module commonalization and supply chain configuration is developed under the criteria on quality, cost and delivery, and an optimization algorithm for obtaining Pareto optimal solutions is configured by using a neighborhood cultivation genetic algorithm and simplex method. Then, this paper investigates into design concept exploration on the optimality and compromise in global product family design with data-mining techniques, a principal component analysis technique and a self-organizing map technique. This paper demonstrates some numerical case studies for ascertaining the validity and promise of the proposed mathematical model and computational techniques for supporting the designer’s decision making toward the excellence in global product family design.",2012
"Comprehensive Product Platform Planning (CP3) Using Mixed-Discrete Particle Swarm Optimization and a New Commonality Index",,2012
"Identification of Platform Variables in Product Family Design Using Sensitivity Analysis","Product family design strategies based on a common core platform have emerged as an efficient and effective means of providing product variety. The main goal in product platform design is to maximize internal commonality within the family while managing the inherent loss in product performance. Therefore, identification and selection of platform variables is a key aspect when designing a family of products. Based on previous research, the Product Platform Constructal Theory Method (PPCTM) provides a systematic approach for developing customizable products, while allowing for multiple levels of commonality, multiple product specifications, and balancing the tradeoffs between commonality and performance. However, selection of platform variables and the modes for managing product variety are not guided by a systematic process in this method. When developing a platform with more than a few variables, a quantitative method is needed for selecting the optimal platform variable hierarchy. In this paper we present an augmented PPCTM which includes sensitivity analysis of platform variables, such that hierarchical rank is conducted based on the impact of the variables on the product performance. This method is applied to the design of a line of customizable finger pumps.",2012
"A Functionally–Aware Product Schematic Clustering Algorithm","The formation of modules is an important step in establishing a product’s architecture. This paper proposes a clustering algorithm that creates functionally cohesive and loosely coupled modules in a product’s architecture. The algorithm seeks to group together product elements to form functionally similar and loosely-coupled modules. The algorithm also has the advantage of not requiring any user-defined starting parameters that are necessary in some other clustering algorithms. The proposed algorithm is demonstrated on a laser printer and the results are compared to the results from an existing algorithm.",2012
"PSS Business Case Map: Supporting Idea Generation in PSS Design","This paper proposes the PSS Business Case Map as a tool to support designers’ idea generation in PSS design. The map visualizes the similarities among PSS business cases in a two-dimensional diagram. To make the map, PSS business cases are first collected by conducting, for example, a literature survey. The collected business cases are then classified from multiple aspects that characterize each case such as its product type, service type, target customer, and so on. Based on the results of this classification, the similarities among the cases are calculated and visualized by using the Self-Organizing Map (SOM) technique. A SOM is a type of artificial neural network that is trained using unsupervised learning to produce a low-dimensional (typically two-dimensional) view from high-dimensional data. The visualization result is offered to designers in a form of a two-dimensional map, which is called the PSS Business Case Map. By using the map, designers can figure out the position of their current business and can acquire ideas for the servitization of their business.",2012
"A Product-Service System Design Framework Based on a Business Ecosystem","Nowadays, enterprises’ efforts are focused on increasing their product values with additional services and contents to satisfy diverse customer needs in competitive market environments. Trends of integrating services and products lead to the emergence of a product-service system (PSS). To enable designers and manufactures to implement and embody a PSS solution in new product development, there is a need for a comprehensive design framework to facilitate the design factors of PSS in complex business environments. The objective of this research is to propose a product-service system design framework to identify design factors for products and services in the context of a business ecosystem. In this paper, we introduce primary and secondary functions to understand customer purchasing motivations, which can be represented as the design factors. A survey on representative IT products is conducted to identify the design factors in terms of PSS. A business ecosystem is a group of entities associated with PSS. With the emergence of PSS, competitions in homogeneous market segment now become confrontations among different business ecosystems. In the proposed framework, we define a Product-Service platform (PS platform) as interface for customers toward PSS manufacturers, its supplier, and contents providers in the business ecosystem. Further, we discuss PS platform’s roles and advantages with case studies involving electronic consumer products.",2012
"Revisiting the Research Field of Product-Service Systems Development","The research within the Product-Service Systems (PSS) field aims to support manufacturing industries’ ability to provide value in terms of a service offer to its customers, simultaneously taking a more holistic approach to eco-sustainability. The industrial idea of providing customer benefits in parallel with robust products is not new, yet equipping engineers to conduct innovation and applying a service perspective in the early design stages is noticed as fairly radical. The purpose in this paper is two-fold. First, to describe research efforts within the PSS field seen through our engineering design lenses, second, to explore and discuss plausible directions and by that identify “white spots” on the map, which may be seen as relevant directions for future research.",2012
"Product-Service Systems Design With Business Model Strategy Integrated: A Case Study of Urban Umbrella Rental Service System","The value creation paradigm in industry has recently been shifting toward value creation through Product-Service Systems (PSS) where product and service elements are tightly integrated as systems to provide functional fulfillment. A successful PSS should satisfy economical, ecological and experience values. A systematic design method for PSS has been developed where activities of stakeholders are designed to support those values reflecting diverse context elements. Also business model aspects of PSS are considered using the category of the business model canvas together with strategies and protocols obtained from various real world cases. To illustrate the PSS design method with business model strategy integrated, a case of urban umbrella rental service system is presented.",2012
"A Reliability Approach to Inverse Simulation Under Uncertainty","Inverse simulation is an inverse process of a direct simulation. During the process, the simulation input variables are identified for a given set of simulation output variables. Uncertainties such as random parameters may exist in engineering applications of inverse simulation. A reliability method is developed in this work to estimate the probability distributions of unknown simulation input. The First Order Reliability Method is employed and modified so that the inverse simulation is embedded within the reliability analysis algorithm. This treatment avoids the separate executions of reliability analysis and inverse simulation and consequently maintains high efficiency. In addition, the means and standard deviations of unknown input variables can also be obtained. A particle impact problem is presented to demonstrate the proposed method for inverse simulation under uncertainty.",2012
"Time-Dependent Reliability Analysis by a Sampling Approach to Extreme Values of Stochastic Processes","Maintaining high accuracy and efficiency is a challenging issue in time-dependent reliability analysis. In this work, an accurate and efficient method is proposed for limit-state functions with the following features: The limit-state function is implicit with respect to time, and its input contains stochastic processes; the stochastic processes include only general strength and stress variables, or the limit-state function is monotonic to these stochastic processes. The new method employs random sampling approaches to estimate the distributions of the extreme values of the stochastic processes. The extreme values are then used to replace the corresponding stochastic processes, and consequently the time-dependent reliability analysis is converted into its time-invariant counterpart. The commonly used time-invariant reliability method, the First Order Reliability Method, is then applied for the time-variant reliability analysis. The results show that the proposed method significantly improves the accuracy and efficiency of time-dependent reliability analysis.",2012
"Sequential Quadratic Programming for Robust Optimization With Interval Uncertainty","Uncertainty plays a critical role in engineering design as even a small amount of uncertainty could make an optimal design solution infeasible. The goal of robust optimization is to find a solution that is both optimal and insensitive to uncertainty that may exist in parameters and design variables. In this paper, a novel approach, ",2012
"Optimal Sample Augmentation and Resource Allocation for Design With Inadequate Uncertainty Data","Uncertainty modeling in reliability-based design optimization problems requires a large amount of measurement data that are generally too costly in engineering practice. Instead, engineers are constantly challenged to make timely design decisions with only limited information at hand. In the literature, Bayesian binomial inference techniques have been used to estimate the reliability values of functions of uncertainties with limited samples. However, existing methods assume one sample as the entire set of measurements with one for each uncertain quantity while in reality one sample is one measurement on a specific quantity. As a result, effective yet efficient allocating resources in sample augmentation is needed to reflect the relative contributions of uncertainties on the final optimum. We propose a sample augmentation process that uses the concept of sample combinations. Uncertain quantities are sampled with respect to their relative ‘importance’ while the impacts of bad measurements, which affect the evaluation of reliability inference, are alleviated via a Markov-Chain Monte Carlo filter. The proposed method could minimize the efforts and resources without assuming distributions for uncertainties. Several examples are used to demonstrate the validity of the method in product development.",2012
"A Novel Second-Order Reliability Method (SORM) Using Non-Central or Generalized Chi-Squared Distributions","This paper proposes a novel second-order reliability method (SORM) using non-central or general chi-squared distribution to improve the accuracy of reliability analysis in existing SORM. Conventional SORM contains three types of errors: (1) error due to approximating a general nonlinear limit state function by a quadratic function at most probable point (MPP) in the standard normal U-space, (2) error due to approximating the quadratic function in U-space by a hyperbolic surface, and (3) error due to calculation of the probability of failure after making the previous two approximations. The proposed method contains the first type of error only which is essential to SORM and thus cannot be improved. However, the proposed method avoids the other two errors by describing the quadratic failure surface with the linear combination of non-central chi-square variables and using the linear combination for the probability of failure estimation. Two approaches for the proposed SORM are suggested in the paper. The first approach directly calculates the probability of failure using numerical integration of the joint probability density function (PDF) over the linear failure surface and the second approach uses the cumulative distribution function (CDF) of the linear failure surface for the calculation of the probability of failure. The proposed method is compared with first-order reliability method (FORM), conventional SORM, and Monte Carlo simulation (MCS) results in terms of accuracy. Since it contains fewer approximations, the proposed method shows more accurate reliability analysis results than existing SORM without sacrificing efficiency.",2012
"Simulation-Based Design of Wind-Turbine Sealing Solutions Using a Systematic Approach to Robust Concept Exploration","Designing sealing solutions in wind turbines made of elastomeric materials that are robust to various uncertainties from manufacturing errors, material imperfections and wind turbine operation is a challenging task. In this paper, we focus on the simulation-based design of material properties and geometrical dimensions of such sealing solutions. We use a systematic approach to robust concept exploration based on a multi-objective decision formulation, the compromise Decision Support Problem (cDSP). Besides using the rather traditional Archimedean or standard utility function based goal formulations in the cDSP, we leverage the conjoint analysis approach to facilitate the preference elicitation process for the various stakeholders involved in the complex product development processes in industrial practice. By parametrically tailoring geometrical dimensions and material properties, characteristics that are superior to those of more heuristic sealing designs and less sensitive to imperfections in the processing and manufacturing routes as well as operation of the wind turbine are achieved. We compare base-line and various robust solutions from traditional Archimedean and standard or conjoint analysis utility function goal formulations. Thereby, we show that using the conjoint analysis within a systematic approach to robust concept exploration is well-suited for industrial practice. The systematic approach to robust concept exploration not only yields superior solutions validated by sealing prototypes in wind turbine operation, it also fosters product development efficiency by applying design of experiment and meta-modeling techniques instead of focusing on a more heuristic product development process to achieve sealing designs.",2012
"Integrating Immersive Computing Technology With Mixed-Integer Nonlinear Programming for Disassembly Sequence Planning Under Uncertainty","Disassembly sequence planning at the early conceptual stage of design leads to enormous benefits including simplification of products, lower assembly and disassembly costs, and design modifications which result in increased potential profitability of end-of-life salvaging operations. However, in the early design stage, determining the best disassembly sequence is challenging. First, the required information is not readily available and very time-consuming to gather. In addition, the best solution is sometimes counterintuitive, even to those with experience and expertise in disassembly procedures. Integrating analytical models with Immersive Computing Technology (ICT) can help designers overcome these issues. A two-stage procedure for doing so is introduced in this paper. In the first stage, a stochastic programming model together with the information obtained through immersive simulation is applied to determine the optimal disassembly sequence, while considering uncertain outcomes, such as time, cost and the probability of causing damage. In the second stage, ICT is applied as a tool to explore alternative disassembly sequence solutions in an intuitive way. The benefit of using this procedure is to determine the best disassembly sequence, not only by solving the analytic model, but also by capturing human expertise. The designer can apply the obtained results from these two stages to analyze and modify the product design. An example of a Burr puzzle is used to illustrate the application of the method.",2012
"Computationally Efficient Imprecise Uncertainty Propagation in Engineering Design and Decision Making","Modeling uncertainty through probabilistic representation in engineering design is common and important to decision making that considers risk. However, representations of uncertainty often ignore elements of “imprecision” that may limit the robustness of decisions. Further, current approaches that incorporate imprecision suffer from computational expense and relatively high solution error. This work presents the Computationally Efficient Imprecise Uncertainty Propagation (CEIUP) method which draws on existing approaches for propagation of imprecision and integrates sparse grid numerical integration to provide computational efficiency and low solution error for uncertainty propagation. The first part of the paper details the methodology and demonstrates improvements in both computational efficiency and solution accuracy as compared to the Optimized Parameter Sampling (OPS) approach for a set of numerical case studies. The second half of the paper is focused on estimation of non-dominated design parameter spaces using decision policies of Interval Dominance and Maximality Criterion in the context of set-based sequential design-decision making. A gear box design problem is presented and compared with OPS, demonstrating that CEIUP provides improved estimates of the non-dominated parameter range for satisfactory performance with faster solution times. Parameter estimates obtained for different risk attitudes are presented and analyzed from the perspective of Choice Theory leading to questions for future research. The paper concludes with an overview of design problem scenarios in which CEIUP is the preferred method and offers opportunities for extending the method.",2012
"Concurrent Design Optimization and Calibration-Based Validation Using Local Domains Sized by Bootstrapping","The design optimization process relies often on computational models for analysis or simulation. These models must be validated to quantify the expected accuracy of the obtained design solutions. It can be argued that validation of computational models in the entire design space is neither affordable nor required. In previous work, motivated by the fact that most numerical optimization algorithms generate a sequence of candidate designs, we proposed a paradigm where design optimization and calibration-based model validation are performed concurrently in a sequence of variable-size local domains that are relatively small compared to the entire design space. A key element of this approach is how to account for variability in test data and model predictions in order to determine the size of the local domains at each stage of the sequential design optimization process. In this paper, we discuss two alternative techniques for accomplishing this: parametric and nonparametric bootstrapping. The parametric bootstrapping assumes a Gaussian distribution for the error between test and model data and uses maximum likelihood estimation to calibrate the prediction model. The nonparametric bootstrapping does not rely on the Gaussian assumption providing therefore, a more general way to size the local domains for applications where distributional assumptions are difficult to verify, or not met at all. If distribution assumptions are met, parametric methods are preferable over nonparametric methods. We use a validation literature benchmark problem to demonstrate the application of the two techniques, emphasizing that results cannot be compared. Which technique to use depends on whether the Gaussian distribution assumption is appropriate based on available information.",2012
"A Virtual Testing Framework for Engineering Product Development","Virtual testing is a new engineering development trend to design, evaluate, and test new engineered products. This research proposes a virtual testing framework for new product development using three successive steps: (i) statistical model calibration, (ii) hypothesis test for validity check and (iii) virtual qualification. Statistical model calibration first improves the predictive capability of a computational model over a calibration domain. Next, the hypothesis test is performed under limited observed data to see if a calibrated model is sufficiently predictive for virtual testing of a new design. A u-pooling metric is employed for the hypothesis test to measure the degree of mismatch between predicted and observed results while considering uncertainty in the u-pooling metric due to the lack of experimental data. The calibrated model can be rejected only when the measured metric of the calibrated model strongly suggest that the null hypothesis—the calibrated model is valid—is false. If the null hypothesis is accepted, the virtual qualification process can be executed with a qualified model for new product developments. The qualification process builds a design decision matrix to aid in rational decision-making on the product developments. A computational model of a tire tread block was used to demonstrate the effectiveness of the proposed framework.",2012
"Efficient Sampling Methods for Tradeoff Studies Under Uncertainty","Tradeoff studies help designers better understand how different design considerations relate to one another and to make decisions. Generally a tradeoff study involves a systematic multi-criteria evaluation of various alternatives for a particular system or subsystem. After evaluating these alternatives, designers eliminate those that perform poorly using the Pareto dominance criterion and explore more carefully those that remain.",2012
"Simulating Drift-Diffusion Processes With Generalized Interval Probability","The Fokker-Planck equation is widely used to describe the time evolution of stochastic systems in drift-diffusion processes. Yet, it does not differentiate two types of uncertainties: aleatory uncertainty that is inherent randomness and epistemic uncertainty due to lack of perfect knowledge. In this paper, a generalized Fokker-Planck equation based on a new generalized interval probability theory is proposed to describe drift-diffusion processes under both uncertainties, where epistemic uncertainty is modeled by the generalized interval while the aleatory one is by the probability measure. A path integral approach is developed to numerically solve the generalized Fokker-Planck equation. The resulted interval-valued probability density functions rigorously bound the real-valued ones computed from the classical path integral method. The new approach is demonstrated by numerical examples.",2012
"Sampling-Based RBDO Using Probabilistic Sensitivity Analysis and Virtual Support Vector Machine","In this paper, a sampling-based RBDO method using a classification method is presented. The probabilistic sensitivity analysis is used to compute sensitivities of probabilistic constraints with respect to random variables. Since the probabilistic sensitivity analysis requires only the limit state function, and not the response surface or sensitivity of the response, an efficient classification method can be used for a sampling-based RBDO. The proposed virtual support vector machine (VSVM), which is a classification method, is a support vector machine (SVM) with virtual samples. By introducing virtual samples, VSVM overcomes the deficiency in existing SVM that uses only classification information as their input. In this paper, the universal Kriging method is used to obtain locations of virtual samples to improve the accuracy of the limit state function for highly nonlinear problems. A sequential sampling strategy effectively inserts new samples near the limit state function. In sampling-based RBDO, Monte Carlo simulation (MCS) is used for the reliability analysis and probabilistic sensitivity analysis. Since SVM is an explicit classification method, unlike implicit methods, computational cost for evaluating a large number of MCS samples can be significantly reduced. Several efficiency strategies, such as the hyper-spherical local window for generation of the limit state function and the Transformations/Gibbs sampling method to generate uniform samples in the hyper-sphere, are also applied. Examples show that the proposed sampling-based RBDO using VSVM yields better efficiency in terms of the number of required samples and the computational cost for evaluating MCS samples while maintaining accuracy similar to that of sampling-based RBDO using the implicit dynamic Kriging (D-Kriging) method.",2012
"Confidence Level Estimation and Design Sensitivity Analysis for Confidence-Based RBDO","In practical engineering problems, often only limited input data are available to generate the input distribution model. The insufficient input data induces uncertainty on the input distribution model, and this uncertainty will cause us to lose confidence in the optimum design obtained using the reliability-based design optimization (RBDO) method. Uncertainty on the input distribution model requires us to consider the reliability analysis output, which is defined as the probability of failure, to follow a probabilistic distribution. This paper proposes a new formulation for the confidence-based RBDO method and design sensitivity analysis of the confidence level. The probability of the reliability analysis output is obtained with consecutive conditional probabilities of input distribution parameters and input distribution types using a Bayesian approach. The approximate conditional probabilities of input distribution parameters and types are suggested under certain assumptions. The Monte Carlo simulation is applied to practically calculate the output distribution, and the copula is used to describe the correlated input distribution types. A confidence-based RBDO problem is formulated using the derived the distribution of output. In this new formulation, the probabilistic constraint is modified to include both the target reliability and the target confidence level. Finally, the sensitivity of the confidence level, which is a new probabilistic constraint, is derived to support an efficient optimization process. Using accurate surrogate models, the proposed method does not require generation of additional surrogate models during the RBDO iteration; it only requires several evaluations of the same surrogate models. Hence, the efficiency of the method is obtained. For the numerical example, the confidence level is calculated and the accuracy of the derived sensitivity is verified when only limited data are available.",2012
"Constraint Programming Simulation of a Distributed Set-Based Design Framework With Control Indicators","In the New Product Development processes, there are usually multiple players interacting through coupled design activities with conflicting design objectives. We use Set-based design approach and Constraint Satisfaction Problem solving techniques to deal with the multiplayer, multi-objective design problem. In order to observe the states of the players during the design process, we develop satisfaction and progress indicators and combine them into a wellbeing indicator. In this paper we simulate different player behaviors on a two-player, multi-objective engineering design problem of a hollow cylindrical cantilever beam. During the simulation cases, at each simulation iteration automatic players are prioritized regarding their wellbeing states for proposing constraints in order to find a set of consistent solutions that improve their satisfaction states. Therefore during the simulation process, while solution space converges to a single solution, epistemic uncertainty is reduced and players’ satisfaction states are improved in wellbeing equilibrium.",2012
"Model Validation Metric and Model Bias Characterization for Dynamic System Responses Under Uncertainty","Quantification of the accuracy of analytical models (math or computer simulation models) and characterization of the model bias are two essential processes in model validation. Available model validation metrics, whether qualitative or quantitative, do not consider the influence of the number of experimental data for model accuracy check. In addition, quantitative measure from the validation metric does not directly reflect the level of model accuracy, i.e. from 0% to 100%, especially when there is a lack of experimental data. If the original model prediction does not satisfy accuracy criteria compared to the experimental data, instead of revising the model conceptually, characterization of the model bias may be a more practical approach to improve the model accuracy because there is probably no ideal model which can predict the actual physical system with no error. So far, there is a lack of effective approaches that can accurately characterize the model bias for multiple dynamic system responses. To overcome these limitations, the first objective of this study is to develop a model validation metric for model accuracy check considering different number of experimental data. Specifically, a validation metric using the Bhattacharya distance (B-distance) is proposed with three notable benefits. First of all, the metric directly compares the distributions of two set of uncertain system responses from model prediction and experiment rather than the distribution parameters (e.g. mean and variance). Second, the B-distance quantitatively measures the degree of accuracy from 0% to 100% between the distributions of the uncertain system responses. Third, reference accuracy metric with respect to different number of experimental data can be effectively obtained so that hypothesis test can be performed to identify whether the two distributions are identical or not in a probability manner. The second objective of this study is to propose an effective approach to accurately characterize the model bias for dynamic system responses. Specially, the model bias is represented by a generic random process, where realizations of the model bias at each time step could follow arbitrary distributions. Instead of using the traditional Bayesian or Maximum Likelihood Estimation (MLE) approach, we propose a novel and efficient approach to identify the model bias using a generic random process modeling technique. A vehicle safety system with 11 dynamic system responses is used to demonstrate the effectiveness of the proposed approach.",2012
"A Novel HCA Framework for Simulating the Cellular Mechanisms of Bone Remodeling","Each year, bone metabolic diseases affect millions of people of all ages, genders, and races. Common diseases such as osteopenia and osteoporosis result from the disruption of the bone remodeling process and can place an individual at a serious fracture risk. Bone remodeling is the complex process by which old bone is replaced with new tissue. This process occurs continuously in the body and is carried out by bone cells that are regulated by numerous metabolic and mechanical factors. The remodeling process provides for various functions such as adaptation to mechanical loading, damage repair, and mineral homeostasis. An improved understanding of this process is necessary to identify patients at risk of bone disease and to assess appropriate treatment protocols.",2012
"An Approach for Uni-Level Reliability Based Design Optimization Using Cross-Entropy Method","In this paper a novel approach is proposed to solve the reliability based design optimization (RBDO) problem, which is translated into a single-level problem using Karush-Kuhn-Tucker (KKT) conditions. The transformation of a bi-level RBDO problem into a single-level problem using KKT conditions introduces several equality constraints in the single-level problem definition. Presence of multiple equality constraints poses numerical difficulty to the gradient based optimizers, hence a robust algorithm to solve the single-level RBDO problem is proposed in this paper using an alternative approach. The proposed approach uses an exterior penalty based cross-entropy (CE) method to solve the uni-level RBDO problem. This approach is shown to be robust in handling equality constraints. The three example problems solved in this paper also shows that the algorithm works well with different starting points used for the design variables.",2012
"Design of Crashworthy Structures With Controlled Energy Absorption in the HCA Framework","This work presents a novel method for designing crashworthy structures with controlled energy absorption based on the use of compliant mechanisms. This method helps in introducing flexibility at desired locations within the structure, which in turn reduces the peak force at the expense of a reasonable increase in intrusion. For this purpose, the given design domain is divided into two subdomains: flexible (FSD) and stiff (SSD) subdomains. The design in the flexible subdomain is governed by the compliant mechanism synthesis approach for which output ports are defined at the interface between the two subdomains. These output ports aid in defining potential load paths and help the user make better use of a given design space. The design in the stiff subdomain is governed by the principle of a fully-stressed design for which material is distributed to achieve uniform energy distribution within the design space. Together, FSD and SSD provide for a combination of flexibility and stiffness in the structure, which is desirable for most crash applications.",2012
"Topography Optimization of Shell Structures Under Transient Loading: A Comparative Approach","Topography optimization is an innovative technique that can significantly improve the response of certain type of structures. The most challenging aspect of topography optimization is the sensitivity analysis. In this manuscript two methods to approximate the sensitivities for problems in topography optimization are introduced. The gradient is supplanted with either a stochastic approximation, or a physical approximation. Initially, an overview of the state-of-the-art in topography optimization is presented, and some key issues are explored. Subsequently, the technique is outlined, and the proposed methods are introduced. Furthermore, a numerical example in which a structure composed of shell elements is subject to a blast load is provided. This example is solved employing stochastic gradient approximation, and approximate gradient. They are compared to the widely used finite differences approximation. It is possible to observe that the proposed method significantly reduces the computational effort required to solve the problem, while considerably improving the objective function.",2012
"Multiple-Target Selective Disassembly Sequence Planning With Disassembly Sequence Structure Graphs","Modern green products must be easy to disassemble. Selective disassembly is used to access and remove specific product components for reuse, recycling, or remanufacturing. Early related studies developed various heuristic or graph-based approaches for single-target selective disassembly. More recent research has progressed from single-target to multiple-target disassembly, but disassembly model complexity and multiple constraints, such as fastener constraints and disassembly directions, still have not been considered thoroughly. In this study, a new graph-based method using disassembly sequence structure graphs (DSSGs) was developed for multiple-target selective disassembly sequence planning. The DSSGs are built using expert rules, which eliminate unrealistic solutions and minimize graph size, which reduces searching time. Two or more DSSGs are combined into one DSSG for accessing and removing multiple target components. In addition, a genetic algorithm is used to decode graphical DSSG information into disassembly sequences and optimize the results. Using a GA to optimize results also reduces searching time and improves overall performance, with respect to finding global optimal solutions. Case studies show that the developed method can efficiently find realistic near-optimal multiple-target selective disassembly sequences for complex products.",2012
"Pre-Life and End-of-Life Combined Profit Optimization With Predictive Product Lifecycle Design","The Predictive Product Lifecycle Design (PPLD) model that is proposed in this paper enables a company to optimize its product lifecycle design strategy by considering pre-life and end-of-life at the initial design stage. By combining lifecycle design and predictive trend mining technique, the PPLD model can reflect both new and remanufactured product market demands, capture hidden and upcoming trends, and finally provide an optimal lifecycle design strategy in order to maximize profit over the span of the whole lifecycle. The outcomes are lifecycle design strategies such as product design features, the need for buy-backs at the end of its life, and the quantity of products remanufacturing. The developed model is illustrated with an example of a cell phone lifecycle design. The result clearly shows the benefit of the model when compared to a traditional Pre-life design model. The benefit would be increased profitability, while saving more natural resources and reducing wastes for manufacturers own purposes.",2012
"To Extend, or to Shorten: Optimal Lifetime Planning","Extending the life of a product through remanufacturing or refurbishing is generally regarded as being “greener” than new production, as it avoids the resource consumption and waste generation associated with the new production; however, when considering improved performance of new products, extending the lifetime of less efficient, less productive old products may not always be greener than new production. Shortening the product’s life by early replacement with a newer, more efficient product can be a better option, as “Cash-for-Clunker” programs have claimed. This paper presents a generic model to decide optimal lifetime strategy for a product. Three different lifetime strategies—to maintain, to extend, and to shorten the current lifetime—are compared from an environmental perspective, for a given time horizon. The average environmental impact per unit production is used as the basis for a fair comparison. Applied with an optimization technique, the model can also identify the optimal lifetime length of a product. To illustrate, the developed model is applied to an example of complex heavy-duty, off-road equipment.",2012
"Promoting and Managing End-of-Life Closed-Loop Scenarios of Products Using a Design for Disassembly Evaluation Tool","In recent years, environmentally conscious design has become a fundamental approach for industries which have to consider the variable environment during the design process. Waste management is one of the most important aspects to be handled, to reduce the disposal in landfills and to encourage the sustainable 3R approach: Reuse, Recycling and Remanufacturing. Product disassembly is an essential phase of the product lifecycle, necessary to evaluate the End-of-Life (EoL) strategies and to reduce environmental impact. In order to minimize the impact on production and costs it is very important to consider EoL scenarios during the embodiment design phase, when designer’s decisions influence product structure. Design for Disassembly (DFD) is a powerful method to reduce disassembly time and costs. However, there are no useful tools which provide guidelines to improve the product disassemblability or promote specific EoL scenarios.",2012
