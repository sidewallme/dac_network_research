title,Abstract,year
"Integrated Multiscale Robust Design Considering Microstructure Evolution and Material Properties in the Hot Rolling Process","A transmission gear is generally produced by a sequence of several processes from steelmaking to final machining and surface treatment. The intermediate processes such as hot rolling induce microstructure evolution and phase transformation which play a significant role in determining the mechanical properties and fatigue strength of gears. Therefore, these intermediate processes should be carefully considered in determining the performance and properties of the end product.",2013
"Evolution of Meso-Structures for Non-Pneumatic Tire Development: A Case Study","The evolution of meso-structures in the development of the shear band of Michelin’s non-pneumatic tire, the Tweel, is presented in this paper. Designers and researchers at Clemson University worked on a research projects with Michelin to support NIST efforts in fuel efficiency improvement and NASA efforts in manned exploration systems. The goal of each was to replace the elastomeric material of shear band with materials which can tolerate harsh temperatures and shear loads or to replace the materials with linear elastic low-hysteretic loss materials. The concepts initially proposed by ideation method were prototyped for physical testing. A case study examining the documentation reports for each project is conducted to provide a reflective understanding of how the evolution in the projects occurred. The goal of developing this retrospective is to try to identify guidelines and approaches that could be integrated into a designer driven systematic approach for custom design of meso-structures.",2013
"Hot Forging of Automobile Steel Gear Blanks: An Exploration of the Solution Space","In this paper, we explore the design space associated with the realization of automobile steel gear blanks manufactured using hot forging. ABAQUS, a commercially available software package, is used to simulate the hot forging process. Surrogate models are developed and used to model the solution space. Then the compromise Decision Support Problem (cDSP) is used to explore the feasible space. Results are presented as ternary plots that allow a designer to visualize the trade-offs between competing goals such as reducing the size of the underfill, reducing the flash (excess material), waste of forged materials, energy required for deformation and crack susceptibility, on various possible solutions. The efficacy of the exploration of the design space and trade-offs between competing solutions is illustrated.",2013
"Inverse Design of Manufacturing Process Chains","Finding the input specifications to obtain the specified performance of a component being designed is an essential activity of a designer. However, obtaining solutions for this inverse problem is a complex task; especially when there are multiple steps with many-to-one mappings at each step in the forward problem. This complexity is further augmented in the presence of uncertainty of the parameters and models used.",2013
"An Approach to Robust Process Design for Continuous Casting of Slab","Continuous casting is a crucial step in the production of a variety of steel products. Its performance is measured in terms of conflicting objectives including productivity, yield, quality and production costs. These are conflicting in the sense that, if the productivity is increased, there is a reduction in other performance parameters. These performance parameters are greatly influenced by operating conditions such as casting speed, superheat, mold oscillation frequency, and secondary cooling conditions. An optimized solution for continuous casting process can be obtained. However uncertainty in operating parameters which affects the performance of caster is rarely considered. Moreover, the solution obtained is optimal with respect to a particular performance measure and does not provide a balance between all. In this paper an integrated design framework has been developed based on metamodels and the compromise Decision Support Problem (cDSP). The framework developed deals with uncertainty and yields robust solutions for performance measures. Further, the design space for continuous casting has been explored for different scenarios to determine satisficing solutions. The utility of the framework has been illustrated for providing decision support when an existing configuration for continuous casting is unable to meet the requirements. This approach can be instantiated for other unit operations involved in steel manufacturing and then may be integrated to simulate the entire production cycle of steel manufacturing. This in turn will enable development of materials with specific properties and reduce the time and cost incurred in the development of new materials and their manufacturing.",2013
"Exploration of the Design Space in Continuous Casting Tundish","Due to the stringent requirements of industry, it has become extremely important to have a careful control over the required performance and properties of steels. Performance and properties of advanced high strength steel depend significantly on its cleanliness. Cleanliness is achieved by restricting the inclusion count to a permissible limit. Over the past few years, there has been increased use of tundish, a device that acts as a buffer between ladle and mold, for controlling inclusions. Apart from facilitating inclusion removal, tundish also maintains low dead volume and thermal and chemical homogeneity, which is required for smooth casting operation. Thus, performance of the tundish operation greatly influences the properties and quality of the cast slab. Tundish performance is generally assessed using parameters such as inclusion removal efficiency, dead volume within tundish and effectiveness in maintaining the desired amount of superheat. But, the aforesaid parameters are conflicting in nature. Managing the conflict and providing a satisficing solution based on the customer requirements become essential.",2013
"A Multi-Scale Materials Modeling Method With Seamless Zooming Capability Based on Surfacelets","In multi-scale materials modeling, it is desirable that different levels of details can be specified in different regions of interest without the separation of scales so that the geometric and physical properties of materials can be designed and characterized. Existing materials modeling approaches focus on the representation of the distributions of material compositions captured from images. In this paper, a multi-scale materials modeling method is proposed to support interactive specification and visualization of material microstructures at multiple levels of details, where designer’s intent at multiple scales is captured. This method provides a feature-based modeling approach based on a recently developed surfacelet basis. It has the capability to support seamless zoom-in and zoom-out. The modeling, operation, and elucidation of materials are realized in both the surfacelet space and the image space.",2013
"Bayesian Network Classifiers and Design Flexibility Metrics for Set-Based, Multiscale Design With Materials Design Applications","A set-based approach is presented for solving multi-scale or multi-level design problems. The approach incorporates Bayesian network classifiers (BNC) for mapping design spaces at each level and flexibility metrics for intelligently narrowing the design space as the design process progresses. The approach is applied to a hierarchical composite materials design problem, specifically, the design of composite materials with macroscopic mechanical stiffness and loss properties surpassing those of conventional composites. This macroscopic performance is achieved by embedding small volume fractions of negative stiffness (NS) inclusions in a host material. To design these materials, the set-based, multilevel design approach is coupled with a hierarchical modeling strategy that spans several scales, from the behavior of microscale NS inclusions to the effective properties of a composite material containing those inclusions and finally to the macroscopic performance of components. The approach is shown to increase the efficiency of multi-level design space exploration, and it is particularly appropriate for top-down, performance-driven design, as opposed to bottom-up, trial-and-error modeling. The design space mappings also build intuitive knowledge of the problem and promising regions of the design space, such that it is almost trivial to identify designs that yield preferred system-level performance.",2013
"A Machine Learning-Based Design Representation Method for Designing Heterogeneous Microstructures","In designing microstructural materials systems, one of the key research questions is how to represent the microstructural design space quantitatively using a descriptor set that is sufficient yet small enough to be tractable. Existing approaches describe complex microstructures either using a small set of descriptors that lack sufficient level of details, or using generic high order microstructure functions of infinite dimensionality without explicit physical meanings. We propose a new machine learning-based method for identifying the key microstructure descriptors from vast candidates as potential microstructural design variables. With a large number of candidate microstructure descriptors collected from literature covering a wide range of microstructural material systems, a 4-step machine learning-based method is developed to eliminate redundant microstructure descriptors via image analyses, to identify key microstructure descriptors based on structure-property data, and to determine the microstructure design variables. The training criteria of the supervised learning process include both microstructure correlation functions and material properties. The proposed methodology effectively reduces the infinite dimension of the microstructure design space to a small set of descriptors without a significant information loss. The benefits are demonstrated by an example of polymer nanocomposites optimization. We compare designs using key microstructure descriptors versus using empirically-chosen microstructure descriptors to validate the proposed method.",2013
"Constraint Satisfaction Approach to the Design of Multi-Component, Multi-Phase Alloys","The development of new materials must start with an understanding of their phase stability. Researchers have used the CALPHAD method to develop self-consistent databases encoding the thermodynamics of phases. In this ",2013
"Efficient Filtering in Topology Optimization via B-Splines","This paper presents a B-spline based approach for topology optimization of three-dimensional (3D) problems where the density representation is based on B-splines. Compared with the usual density filter in topology optimization, the new B-spline based density representation approach is advantageous in both memory usage and CPU time. This is achieved through the use of tensor-product form of B-splines. As such, the storage of the filtered density variables is linear with respect to the effective filter size instead of the cubic order as in the usual density filter. Numerical examples of 3D topology optimization of minimal compliance and heat conduction problems are demonstrated. We further reveal that our B-spline based density representation resolves the bottleneck challenge in multiple density per element optimization scheme where the storage of filtering weights had been prohibitively expensive.",2013
"Domain Composition Method for Structural Optimization","Conventionally, design domain of topology optimization is predefined and is not adjusted in the design optimization process since designers are required to specify the design domain in advance. However, it is difficult for a fixed design domain to satisfy design requirements such as domain sizing adjustment or boundaries change. In this paper, Domain Composition Method (DCM) for structural optimization is presented and it deals with the design domain adjustment and the material distribution optimization in one framework. Instead of treating design domain as a whole, DCM divides domain into several subdomains. Additional scaling factors and subdomain transformations are applied to describe changes between different designs. It then composites subdomains and solve it as a whole in the updated domain. Based on the domain composition, static analysis with DCM and sensitivity analysis are derived. Consequently, the design domain and the topology of the structure are optimized simultaneously. Finally, the effectiveness of the proposed DCM for structural optimization is demonstrated through different numerical examples.",2013
"Projection-Based Topology Optimization Using Discrete Object Sets","We look to expand the reach of continuum topology optimization to include the design of ‘structures’ that gain functionality or are specifically manufactured from discrete, non-overlapping objects. While significant advancements have been made in restricting the geometric properties of topology-optimized structures, including restricting the minimum and maximum length scale of features, continuum topology optimization is still largely limited to monolithic structures. A wide variety of structures and materials, however, gain their stiffness or functionality from discrete objects, such as fiber-reinforced composites. This work examines a recently developed method for optimizing the distribution of discrete objects (2d inclusions) across a design domain and extends the approach to variable shape and variable sized objects that must be selected from a designer-defined set. This essentially enables simultaneous optimization of object sizes, shapes, and/or locations within the projection framework, without need for additional constraints. As in traditional topology optimization, gradient-based optimizers are used with sensitivity information estimated via the adjoint method, solved using finite element analysis. The algorithm is demonstrated on benchmark problems in structural design for the case where the objects are stiff inclusions embedded in a compliant matrix material.",2013
"A Hybrid Approach to Polygon Offsetting Using Winding Numbers and Partial Computation of the Voronoi Diagram","In this paper we present a new, efficient algorithm for computing the “raw offset” curves of 2D polygons with holes. Prior approaches focus on (a) complete computation of the Voronoi Diagram, or (b) pair-wise techniques for generating a raw offset followed by removal of “invalid loops” using a sweepline algorithm. Both have drawbacks in practice. Robust implementation of Voronoi Diagram algorithms has proven complex. Sweeplines take ",2013
"Topology Preserving Digitization of Physical Prototypes Using Deformable Subdivision Models","Physical prototyping is an important stage of product design where designers have a chance to physically evaluate and alter digitally created surfaces. In these scenarios, designers generate a digital model, manufacture and alter the prototype as needed, and redigitize the prototype through scanning. Despite the variety of reverse engineering tools, redigitizing the prototypes into forms amenable to further digital editing remains a challenge. This is because current digitization methods cannot take advantage of the key elements of the original digital model such as the wireframe topology and surface flows. This paper presents a new reverse engineering method that augments conventional digitization with the knowledge of the original digital model’s curve topology to enhance iterative shape design activities. Our algorithm takes as input a curve network topology forming a subdivision control cage and a 3D scan of the physically modified prototype. To facilitate the digital capture of the physical modifications, our algorithm performs a series of registration, correspondence and deformation calculations to compute the new configuration of the initial control cage. The key advantage of the proposed technique is the preservation of the edge flows and initial topology while transferring surface modifications from prototypes. Our studies show that the proposed technique can be particularly useful for bridging the gap between physical and digital modeling in the early stages of product design.",2013
"Challenges in Designing and Manufacturing Fully Optimized Functional Gradient Material Objects","Functionally Gradient Materials (FGM) smoothly transition from one material to another within a single object, allowing engineers to customize the physical response of different regions of the object by modifying the material composition at each region. New FGM research makes design, manufacturing, and use of FGM objects a promising alternative to homogeneous objects or composites with one direction of gradation. Heterogeneous anisotropic artifacts can be manufactured with specific 3D printing processes and potentially bring significant increases in functionalities. However, many challenges exist while designing and manufacturing these objects. This paper explores these challenges and suggests needed research. In particular the ability to model FGM objects, setup and run optimization algorithms, create manufacturing process plans, and control the manufacturing process all need more research and better software tools. In addition, researchers must rigorously test optimally designed FGM objects in order to validate the FGM object properties and the FGM design process before adoption of FGM objects by industry is likely to occur.",2013
"Optimization of Tooth Root Profile of Spur Gears for Maximum Load-Carrying Capacity","Increasing the strength of the gear tooth is a recurrent demand from industry. The authors report a novel approach to the design of tooth-root profile of spur gears using cubic splines, with the aim of investigating the effect of tooth-root geometry on stress concentration in order to increase the gear tooth strength by optimizing the root profile. An iterative co-simulation procedure, consisting of tooth-root profile shape synthesis via nonlinear programming and finite element analysis software tools is conducted, for the purpose of forming the tooth-root geometry design with the minimum stress concentration. The proposed design was verified to be capable of reducing the stress concentration by 21% over its conventional circular-filleted counterpart. Hence, the results showcase an innovative and sound methodology for the design of the tooth-root profile to increase gear load-carrying capacity.",2013
"Automatic Detection and Extraction of Tolerance Stacks in Mechanical Assemblies","Tolerances are specified by a designer to allow reasonable freedom by a manufacturer for imperfections and inherent variability without compromising performance. It takes knowledge and experience to create a good tolerance scheme. It is a tedious process driven by the type of parts, their features and controls needed for each one of them. In this paper, we investigate the extent to which GD&T schema development can be automated. Automated tolerance schema generation, requires identifying critical tolerance loops. The tolerance loop is a loop of dimensions between faces of features governing assembly conditions. For this purpose, the first major step is to identify mating features called assembly features. Also, in order to create the tolerance chains we need Local Constraints (assembly feature relationships), Global constraints (part feature relationships) and directions of control. In addition, we have to identify feature patterns since they have associated tolerances according to Dimensioning and Tolerancing Standard ASME Y14.5M. Directions in which these loops lie are also needed; we call them Direction of Control (DoC). Then we can create the GD&T schema, allocate tolerance values, and prepare it for tolerance evaluation. In this paper, we present an approach to automatically identify the dimensional loops based on assembly requirements. Assignment of tolerance values will be covered in future works as it is based on design function.",2013
"A Formal Model of Human Interactions for Service Ecosystem Design","In this digital era, the natures of services are becoming increasingly complex and diverse due to the convergences between the existing human-centered services and other supportive device services, or interactions between heterogeneous services. Expecting this trend is to be accelerated more, new scientific and engineered approaches to the service design are needed more than ever. In this context, a service designed conceptually and abstractly has significant limitations that keep the customers’ satisfaction from advancing above a certain level. Hence, in an initial service design phase, a service delivery process can be expressed quantitatively through a systematic analysis of its natures and the goals. In this paper, a human-centered complex service system is newly defined as service ecosystem. This study proposes a method for designing service processes as a Discrete Event System (DES) in formal ways by utilizing the concepts of affordance, preference, and a Affordance-based Finite State Automata (FSA) modeling methodology. The proposed design method suggests a model framework that focuses on service actions that reflects related properties of customers, employees, and environment entities and their interactions quantitatively. The formally expressed relations between properties of service entities such as customer, employee, and service environment provide guidelines for service designers in a more scientific way than traditional ones. In addition, it is expected that it will enable to add computational to the human-centered service system design and control and develop effective reusable controllers for complex service deliveries as well.",2013
"Identifying Glenoid Geometries Through Radial Basis Functions for Implant Design","Total Shoulder Arthroplasty is performed on patients to restore range of motion of the shoulder and decrease pain caused by osteoarthritis at the glenohumeral joint. The glenohumeral joint is a slightly unstable ball and socket joint, where muscles hold the humerus in contact with the glenoid, located on the scapula. Improper sizing or alignment of the implant can cause the surgery to fail to restore mobility to the shoulder or only restore mobility for a limited time. Additionally, placement of the glenoid implant on the scapula is complicated by the limited view available during surgery and the deformation of the glenoid caused by osteoarthritis. Implant designs must take into account the large amount of variability present in both intact and osteoarthritic joints. The purpose of this research is to provide a morphable glenoid representation for the scapula to assist with preoperative planning and implant design. CT scans of healthy and osteoarthritic glenoids were provided by Hershey Medical Center for this study. Principal component analysis and radial basis functions are used to represent a range of potential glenoid geometries, both with and without osteoarthritis. This parametric model can be used to guide the design and sizing of implants. This approach should be extensible to the modeling of other bony surfaces, which can improve both implant design and surgical procedure.",2013
"Advancing Design Through the Creation and Visualization of Virtual Population Representing U.S. Civilians","Visualization of the U.S. civilian population provides perspective on the variability of body size and shape, enabling designers and engineers to better understand the needs of their target users. This paper presents a virtual population of digital human models, representative of the U.S. civilian population, and the methods used to create it. It is based on the observed variability in stature, BMI, and waist circumference in the data from the 2007–2010 CDC NHANES survey. The NHANES data were used in conjunction with regression models to develop the required model parameters. The models were then presented such that the variability and distribution of variability in anthropometry can be easily seen.",2013
"A Simultaneous Computing Framework for Metamodel-Based Design Optimization","At the aim of reducing the computational time of engineering design optimization problems using metamodeling technologies, we developed a flexible distributed framework independent of any third-part parallel computing software to implement simultaneous sampling during metamodel-based design optimization procedures. In this paper, the idea and implementation of hardware configuration, software structure, the main functional modular and interfaces of this framework are represented in detail. The proposed framework is capable of integrating black-box functions and legacy software for analyzing and common MBDO methods for space exploring. In addition, a message-based communication infrastructure based on TCP/IP protocol is developed for distributed data exchange. The Client/Server architecture and computing budget allocation algorithm considering software dependency enable samples to be effectively allocated to the distributed computing nodes for simultaneous execution, which gives rise to decreasing the elapsed time and improving MBDO’s efficiency. Through testing on several numerical benchmark problems, the favorable results demonstrate that the proposal framework can evidently save the computational time, and is practical for engineering MBDO problems.",2013
"Problem Formulations for Simulation-Based Design Optimization Using Statistical Surrogates and Direct Search","Typical challenges of simulation-based design optimization include unavailable gradients and unreliable approximations thereof, expensive function evaluations, numerical noise, multiple local optima and the failure of the analysis to return a value to the optimizer. The remedy for all these issues is to use surrogate models in lieu of the computational models or simulations and derivative-free optimization algorithms. In this work, we use the ",2013
"An Approach Towards Generating Surrogate Models by Using RBFN With a Priori Bias","In this paper, an approach to generate surrogate models constructed by radial basis function networks (RBFN) with a priori bias is presented. RBFN as a weighted combination of radial basis functions only, might become singular and no interpolation is found. The standard approach to avoid this is to add a polynomial bias, where the bias is defined by imposing orthogonality conditions between the weights of the radial basis functions and the polynomial basis functions. Here, in the proposed a priori approach, the regression coefficients of the polynomial bias are simply calculated by using the normal equation without any need of the extra orthogonality prerequisite. In addition to the simplicity of this approach, the method has also proven to predict the actual functions more accurately compared to the RBFN with a posteriori bias. Several test functions, including Rosenbrock, Branin-Hoo, Goldstein-Price functions and two mathematical functions (one large scale), are used to evaluate the performance of the proposed method by conducting a comparison study and error analysis between the RBFN with a priori and a posteriori known biases. Furthermore, the aforementioned approaches are applied to an engineering design problem, that is modeling of the material properties of a three phase spherical graphite iron (SGI). The corresponding surrogate models are presented and compared.",2013
"A Global High Dimensional Metamodeling Approach With the Ability of Using Non-Uniform Sampling","In engineering design, spending excessive amount of time on physical experiments or expensive simulations makes the design costly and lengthy. This issue exacerbates when the design problem has a large number of inputs, or of high dimension. High Dimensional Model Representation (HDMR) is one powerful method in approximating high dimensional, expensive, black-box (HEB) problems. One existing HDMR implementation, Random Sampling HDMR (RS-HDMR), can build a HDMR model from random sample points with a linear combination of basis functions. The most critical issue in RS-HDMR is that calculating the coefficients for the basis functions includes integrals that are approximated by Monte Carlo summations, which are error prone with limited samples and especially with non-uniform sampling. In this paper, a new approach based on Principal Component Analysis (PCA), called PCA-HDMR, is proposed for finding the coefficients that provide the best linear combination of the bases with minimum error and without using any integral. Benchmark problems are modeled using the method and the results are compared with RS-HDMR results. With both uniform and non-uniform sampling, PCA-HDMR built more accurate models than RS-HDMR for a given set of sample points.",2013
"Concurrent Surrogate Model Selection (COSMOS) Based on Predictive Estimation of Model Fidelity","One of the primary drawbacks plaguing wider acceptance of surrogate models is their low fidelity in general. This issue can be in a large part attributed to the lack of automated model selection techniques, particularly ones that do not make limiting assumptions regarding the choice of model types and kernel types. A novel model selection technique was recently developed to perform optimal model search concurrently at three levels: (i) optimal model type (e.g., RBF), (ii) optimal kernel type (e.g., multiquadric), and (iii) optimal values of hyper-parameters (e.g., shape parameter) that are conventionally kept constant. The error measures to be minimized in this optimal model selection process are determined by the Predictive Estimation of Model Fidelity (PEMF) method, which has been shown to be significantly more accurate than typical cross-validation-based error metrics. In this paper, we make the following important advancements to the PEMF-based model selection framework, now called the Concurrent Surrogate Model Selection or COSMOS framework: (i) The optimization formulation is modified through binary coding to allow surrogates with differing numbers of candidate kernels and kernels with differing numbers of hyper-parameters (which was previously not allowed). (ii) A robustness criterion, based on the variance of errors, is added to the existing criteria for model selection. (iii) A larger candidate pool of 16 surrogate-kernel combinations is considered for selection — possibly making COSMOS one of the most comprehensive surrogate model selection framework (in theory and implementation) currently available. The effectiveness of the COSMOS framework is demonstrated by successfully applying it to four benchmark problems (with 2–30 variables) and an airfoil design problem. The optimal model selection results illustrate how diverse models provide important tradeoffs for different problems.",2013
"Global Optimization With Quantum Walk Enhanced Grover Search","One of the significant breakthroughs in quantum computation is Grover’s algorithm for unsorted database search. Recently, the applications of Grover’s algorithm to solve global optimization problems have been demonstrated, where unknown optimum solutions are found by iteratively improving the threshold value for the selective phase shift operator in Grover rotation. In this paper, a hybrid approach that combines continuous-time quantum walks with Grover search is proposed. By taking advantage of quantum tunneling effect, local barriers are overcome and better threshold values can be found at the early stage of search process. The new algorithm based on the formalism is demonstrated with benchmark examples of global optimization. The results between the new algorithm and the Grover search method are also compared.",2013
"Investigation of Numerical Performance of Partitioning and Parallel Processing of Markov Chain (PPMC) for Complex Design Problems","Divide-and-conquer strategies have been utilized to perform evaluation calculations of complex network systems, such as reliability analysis of a Markov chain. This paper focuses on partitioning of Markov chain for a multi-modular redundant system and the fast calculation using parallel processing. The complexity of Markov chain is first reduced by eliminating the connections with low transition probabilities associated with a threshold parameter. The transition probability matrix is then reordered and partitioned such that a worse-case reliability is evaluated through the calculations in only the diagonal sub-matrices of the transition probability matrix. Since the calculations of the sub-matrices are independent to each other, the numerical efficiency can be greatly improved using parallel computing. The numerical results showed the selection of threshold parameter is a key factor to numerical efficiency. In this paper, the sensitivity of the numerical performance of Partitioning and Parallel-processing of Markov Chain (PPMC) to the threshold parameter has been investigated and discussed.",2013
"A System Uncertainty Propagation Approach With Model Uncertainty Quantification in Multidisciplinary Design","The performance of a multidisciplinary system is inevitably affected by various sources of uncertainties, usually categorized as aleatory (e.g. input variability) or epistemic (e.g. model uncertainty) uncertainty. In the framework of design under uncertainty, all sources of uncertainties should be aggregated to assess the uncertainty of system quantities of interest (QOIs). In a multidisciplinary design system, uncertainty propagation refers to the analysis that quantifies the overall uncertainty of system QOIs resulting from all sources of aleatory and epistemic uncertainty originating in the individual disciplines. However, due to the complexity of multidisciplinary simulation, especially the coupling relationships between individual disciplines, many uncertainty propagation approaches in the existing literature only consider aleatory uncertainty and ignore the impact of epistemic uncertainty. In this paper, we address the issue of efficient uncertainty quantification of system QOIs considering both aleatory and epistemic uncertainties. We propose a spatial-random-process (SRP) based multidisciplinary uncertainty analysis (MUA) method that, subsequent to SRP-based disciplinary model uncertainty quantification, fully utilizes the structure of SRP emulators and leads to compact analytical formulas for assessing statistical moments of uncertain QOIs. The proposed method is applied to a benchmark electronics packaging problem. To demonstrate the effectiveness of the method, the estimated low-order statistical moments of the QOIs are compared to the results from Monte Carlo simulations.",2013
"System of Systems Approach to Air Transportation Design Using Nested Problem Formulation and Direct Search Optimization","Aircraft sizing, route network design, demand estimation and allocation of aircraft to routes are different facets of the air transportation optimization problem that can be viewed as individual “systems,” since they can be conducted independently. In fact, there is a large body of literature that investigates each of these as a stand-alone problem. In this regard, the air transportation design optimization problem can be viewed as an optimal system-of-systems (SoS) design problem. The resulting mixed variable programming problem cannot be solved all-in-one (AiO) because its size and complexity grow exponentially with increasing number of network nodes. In this work, we use a nested multidisciplinary formulation and the Mesh Adaptive Direct Search (MADS) optimization algorithm to solve the optimal SoS design problem. The expansion of a regional Canadian airline’s network to enable national operations is considered as an example.",2013
"Iteration Complexity of the Alternating Direction Method of Multipliers for Quasi-Separable Problems in Multi-Disciplinary Design Optimization","The Alternating Direction Method of Multipliers (ADMM) is a distributed algorithm suitable for quasi-separable problems in Multi-disciplinary Design Optimization. Previous authors have studied the convergence and complexity of the ADMM algorithm by treating it as an instance of the proximal point algorithm. In this paper, those previous results are extended to an alternate form of the ADMM algorithm applied to the quasi-separable problem. Secondly, a dynamic penalty parameter updating heuristic for the ADMM algorithm is introduced and compared against a previously proposed updating heuristic. The proposed updating heuristic was tested on a distributed linear model fitting example and performed favorably against the other heuristic and the fixed penalty parameter scheme.",2013
"Dual Residual in Augmented Lagrangian Coordination for Decomposition-Based Optimization","As system design problems increase in complexity, researchers seek approaches to optimize such problems by coordinating the optimizations of decomposed sub-problems. Many methods for optimization by decomposition have been proposed in the literature among which, the Augmented Lagrangian Coordination (ALC) method has drawn much attention due to its efficiency and flexibility. The ALC method involves a quadratic penalty term, and the initial setting and update strategy of the penalty weight are critical to the performance of the ALC. The weight in the traditional weight update strategy always increases and previous research shows that an inappropriate initial value of the penalty weight may cause the method not to converge to optimal solutions.",2013
"Solving Structure for Network-Decomposed Problems Optimized With Augmented Lagrangian Coordination","The complexity of design and optimization tasks of modern products which cannot be carried out by a single expert or by a single design team motivated the development of the field of decomposition-based optimization. In general, the process of decomposition-based optimization consists of two procedures: (1) Partitioning the original problem into sub-problems according to the design disciplines, components or other aspects; and (2) Coordinating these sub-problems to guarantee that the aggregation of their optimal solutions results in a feasible and optimal solution for the original problem. Much current work focuses on alternative approaches for these two procedures.",2013
"Cooperative Design Optimization (CDO) for Multidisciplinary Systems","Design engineers and decision-makers across various fields are constantly working to make optimal design decisions for multidisciplinary engineering systems in an effort to improve performance and reduce costs. The multiple disciplines that decision-makers are forced to consider can range from different physical components of a system, to competing physical phenomena influencing a component (e.g. flow forces and structural strength), to completely separate models of interest to a system (e.g. engineering performance and lifecycle cost). The common element that all these decision-making scenarios share is the presence of couplings between the considered disciplines (or subsystems). How the values for these coupling parameters are determined within a decision-making or optimization framework is the subject of countless research efforts. At present the multidisciplinary design optimization (MDO) community has settled on a few proven techniques such Collaborative Optimization (CO) and Analytic Target Cascading (ATC). However, current MDO techniques have issues that limit their effectiveness in solving various MDO problems. Many of these strategies require close coordination between subsystem optimization solvers, require significant effort by decision-makers to pose their problems in a suitable format, and/or can have large computational efficiency problems due to the fact that they involve solving nested optimization problems. In an effort to alleviate some of these issues and make MDO easier to implement and more computationally efficient, a new sequential MDO algorithm called Cooperative Design Optimization (CDO) is proposed. The CDO approach functions through a series of subsystem optimizations using a successively smaller cooperation space. The cooperation space is analogous to the design space of a traditional optimization problem, but includes only the coupling parameters that are a factor in multiple subsystems. A single iteration of the approach can be thought of as a negotiation between all of the subsystems regarding the boundaries of the cooperation space. To facilitate this cooperation, each subsystem optimization problem is restructured as a multi-objective optimization problem so that a Pareto set of optimal solutions, or agreeable design alternatives, are produced. As a result, after all subsystem optimizations are accomplished, each subsystem’s obtained Pareto optimal solution sets are compared with respect to the coupling parameters and new bounds in the cooperation space are determined for the next iteration. This process allows each subsystem to be optimized completely independently within the boundaries of the agreed upon cooperation space while coordination is achieved through an analysis of obtained optimal solutions for each subsystem. Iterations, or negotiations, are repeated until an acceptable solution or set of solutions is obtained for all included subsystems through this cooperative process.",2013
"A Dynamic Service-Oriented Distributed Computing Framework for Evaluation of Computationally Expensive Black-Box Analyses","A practical, flexible, versatile, and heterogeneous distributed computing framework is presented that simplifies the creation of small-scale local distributed computing networks for the execution of computationally expensive black-box analyses. The framework is called the Dynamic Service-oriented Optimization Computing Framework (DSOCF), and is designed to parallelize black-box computation to speed up optimization runs. It is developed in Java and leverages the Apache River project, which is a dynamic Service-Oriented Architecture (SOA). A roulette-based real-time load balancing algorithm is implemented that supports multiple users and balances against task priorities, which is superior to the rigid pre-set wall clock limits commonly seen in grid computing. The framework accounts for constraints on resources and incorporates a credit-based system to ensure fair usage and access to computing resources. Experimental testing results are shown to demonstrate the effectiveness of the framework.",2013
"Application of Analytical Target Cascading for Engine Calibration Optimization Problem","This paper presents the development of an Analytical Target Cascading (ATC) Multidisciplinary Design Optimization (MDO) framework for a steady-state engine calibration optimization problem. The implementation novelty of this research is the use of the ATC framework to formulate the complex multi-objective engine calibration problem, delivering a considerable enhancement compared to the conventional 2-stage calibration optimization approach [1]. A case study of a steady-state calibration optimization of a Gasoline Direct Injection (GDI) engine was used for the calibration problem analysis as ATC. The case study results provided useful insight on the efficiency of the ATC approach in delivering superior calibration solutions, in terms of “global” system level objectives (e.g. improved fuel economy and reduced particulate emissions), while meeting “local” subsystem level requirements (such as combustion stability and exhaust gas temperature constraints). The ATC structure facilitated the articulation of engineering preference for smooth calibration maps via the ATC linking variables, with the potential to deliver important time saving for the overall calibration development process.",2013
"Evaluation of System Reconfigurability Based on Usable Excess","The challenge of designing complex engineered systems with long service lives can be daunting. As customer needs change over time, such systems must evolve to meet these needs. This paper presents a method for evaluating the reconfigurability of systems to meet future needs. Specifically we show that excess capability is a key factor in evaluating the reconfigurability of a system to a particular need, and that the overall system reconfigurability is a function of the system’s reconfigurability to all future needs combined. There are many examples of complex engineered systems; for example, aircraft, ships, communication systems, spacecraft and automated assembly lines. These systems cost millions of dollars to design and millions to replicate. They often need to stay in service for a long time. However, this is often limited by an inability to adapt to meet future needs. Using an automated assembly line as an example, we show that system reconfigurability can be modeled as a function of usable excess capability.",2013
"Using Interfaces to Drive Module Definition: Investigating the Impact of a New Design Dependency Measure","Structural representations for interfaces between modules and components in a product vary widely in the literature. After reviewing several structural approaches to interface definition, a new weighted design dependency measure is described. The new representation takes into account both six different types of interfaces as well as their relative strength and frequency within a product architecture. The resulting design dependency measure provides a means for designers to quantify the change resistance in a product. In this paper, we investigate the use of this new design dependency measure to drive module identification. Specifically, we compare the resulting modules obtained by optimizing Design Structure Matrices (DSMs) using standard 0-1 representations of the interfaces to those obtained using the new design dependency measure. The results indicate that the weighted design dependency measure leads to more a logical definition of modules that maximizes within module dependencies and minimizes interactions between modules.",2013
"Solving the Reconfigurable Design Problem for Multiability With Application to Robotic Systems","Systems that can be reconfigured are valuable in situations where a single artifact must perform several different functions well, and are especially important in cases where system demands are not known a priori. Design of reconfigurable systems present unique challenges compared to fixed system design. Increasing reconfigurable capability improves system utility, but also increases system complexity and cost. In this article a new design strategy is presented for the design of reconfigurable systems for multiability. This study is limited to systems where all system functions are known a priori, and only continuous means of reconfiguration are considered. Designing such a system requires determination of (1) what system features should be reconfigurable, and (2) what should the range of reconfigurability of these features be. The new design strategy is illustrated using a reconfigurable delta robot, which is a parallel manipulator that can be adapted to perform a variety of manufacturing operations. In this case study the tradeoff between end effector stiffness and speed is considered over two separate manipulation tasks.",2013
"Designing Scalable Product Families for Black-Box Functions","Product family design optimization is a cost-efficient concept for achieving the best tradeoff between commonalization and diversification of products. When design functions are computationally intensive and thus viewed as black-boxes, the product family design becomes more challenging. In this study a two-stage platform configuration and product family design optimization method with generalized commonality is proposed for scale-based families involving black-box functions. The platform configuration is unknown and multiple sub-platforms are allowed. In this study, the main parameters used towards the family design include a non-conventional sensitivity analysis, the detachability property of each variable, and the variation of individual optimal values for each design variable. Metamodeling techniques are employed to provide both the non-conventional sensitivity and correlation intensities information, which leads to significant savings in the number of function calls. Efficiency of this method is tested through designing a scalable family of universal electric motors. Compared to a number of previously developed methods, the proposed method yields a design solution with acceptable performance loss after commonalization, and better value for the aggregated preference objective function while satisfying all the performance constraints.",2013
"Decision Topology Assessment in Engineering Design Under Uncertainty","The implications of decision analysis (DA) on engineering design are well known. Recently, the authors proposed decision topologies (DT) as a visual method for making design decisions and proved that they are consistent with normative decision analysis. This paper addresses the practical issue of assessing DTs for a decision maker (DM) using their responses, particularly under uncertainty. This is critical to encoding decision maker preferences so that further analysis and mathematical optimization can be performed using the correct set of preferences. We show how multiattribute DTs can be directly assessed from DM responses. Four methods are shown to evolutionarily assess DTs among which one that requires the DM to rank alternatives and another where a utility function is first assessed. It is also shown that preferences under uncertainty can be easily incorporated. In addition, we show that topologies can be constructed using single attribute topologies similarly to multi-linear functions in utility analysis. This incremental construction simplifies the process of topology construction. The reverse problem of inferring single attribute DTs is also presented. The proposed assessment methods are used on a design decision making problem of a welded beam.",2013
"Redundancy Allocation for Reliability Design of Engineering Systems With Failure Interactions","In reliability design, allocating redundancy through various optimization methods is one of the important ways to improve the system reliability. Generally, in these redundancy allocation problems, it is assumed that failures of components are independent. However, under this assumption failure rates can be underestimated since failure interactions can significantly affect the performance of systems. This paper first proposed an analytical model describing the failure rates with failure interactions. Then a Modified Analytic Hierarchy Process (MAHP) is proposed to solve the redundancy allocation problems for systems with failure interactions. This method decomposes the system into several blocks and deals with those down-sized blocks before diving deep into the most appropriate component for redundancy allocation. Being simple and flexible, MAHP provides an intuitive way to design a complex system and complex explicit objective functions for the entire system is not required in the proposed approach. More importantly, with the help of the proposed analytical failure interaction model, MAHP can capture the effect of failure interactions. Results from case studies clearly demonstrate the applicability of the analytical model for failure interactions and MAHP for reliability design.",2013
"Improve System Reliability and Robustness Using Axiomatic Design and Fault Tree Analysis","The reliability and robustness are critical aspects for engineering systems. In a complex system, usually multiple functions need to be achieved. In such cases, there exist optimal mappings between functions and physical components. In this paper, a new approach, Axiomatic Fault Tree Analysis (AFTA), is proposed to improve the system reliability and robustness simultaneously. In AFTA, system robustness is improved by using Axiomatic Design (AD) to decouple the system to an extent where a group of functions have a corresponding relationship with only one physical component. Moreover, Fault Tree Analysis and Axiomatic Design have been integrated in AFTA by using two new indices indicating the importance degrees of functions and physical components in a reliability sense, which provides a guideline for reliability improvement. A case study of a complex system is used to illustrate the procedure and applicability of AFTA. Theoretical calculations are used to verify AFTA for non-repairable systems, and multi-agent based simulations are for repairable systems. Both the theoretical and simulated results show that the system reliability has been improved significantly without large additional cost.",2013
"Efficient Global Optimization Reliability Analysis (EGORA) for Time-Dependent Limit-State Functions","If a limit-state function involves time, the associated reliability is defined within a period of time. The extreme value of the limit-state function is needed to calculate the time-dependent reliability, and the extreme value is usually highly nonlinear with respect to random input variables and may follow a multimodal distribution. For this reason, a surrogate model of the extreme response along with Monte Carlo simulation is usually employed. The objective of this work is to develop a new method, called the Efficient Global Optimization Reliability Analysis (EGORA), to efficiently build the surrogate model. EGORA is based on the Efficient Global Optimization (EGO) method. Different from the current method that generates training points for random variables and time independently, EGORA draws training points for the two types of input variables simultaneously and therefore accounts for their interaction effects. The other improvement is that EGORA only focuses on high accuracy at or near the limit state. With the two improvements, the new method can effectively reduce the number of training points. Once the surrogate model of the extreme response is available, Monte Carlo simulation is applied to calculate the time-dependent reliability. Good accuracy and efficiency of EGORA are demonstrated by three examples.",2013
"Incorporating Flexibility in the Design of Repairable Systems: Design of Microgrids","The authors have recently proposed a ‘decision-based’ framework to design and maintain repairable systems. In their approach, a multiobjective optimization problem is solved to identify the best design using multiple short and long-term statistical performance metrics. The design solution considers the initial design, the system maintenance throughout the planning horizon, and the protocol to operate the system. Analysis and optimization of complex systems such as a microgrid is however, computationally intensive. The problem is exacerbated if we must incorporate flexibility in terms of allowing the microgrid architecture and its running protocol to change with time. To reduce the computational effort, this paper proposes an approach that “learns” the working characteristics of the microgrid and quantifies the stochastic processes of the total load and total supply using autoregressive time-series. This allows us to extrapolate the microgrid operation in time and eliminate therefore, the need to perform a full system simulation for the entire long-term planning horizon. The approach can be applied to any repairable system. We show that building in flexibility in the design of repairable systems is computationally feasible and leads to better designs.",2013
"A Simulation Method to Estimate the Time-Varying Failure Rate of Dynamic Systems","The failure rate of dynamic systems with random parameters is time-varying even for linear systems excited by a stationary random input. In this paper, we propose a simulation-based method to estimate this time-varying failure rate. The input and output stochastic processes are discretized using a small time step to calculate the trajectories of the output stochastic process accurately through simulation. The planning horizon (time of interest) is then partitioned into a series of longer correlated time intervals and the Saddlepoint approximation (SPA) is employed to estimate the distribution of maximum response and thus obtain the probability of failure in each time interval. Using the same simulated trajectories with SPA, a time-dependent copula is built to provide the correlation between the response in each time interval and the response up to that time interval. The time-varying failure rate is finally estimated at each discrete time, using the probability of failure in each time interval and the correlation information from the estimated copula. The effectiveness of the proposed method is illustrated with a vehicle vibration example.",2013
"An Improved Stochastic Upscaling Method for Multiscale Engineering Systems","A stochastic multiscale modeling technique is proposed to construct coarse scale representation of a fine scale model for use in engineering design problems. The complexity of the fine scale heterogeneity under uncertainty is replaced with the homogenized coarse scale parameters by seeking agreement between the responses at both scales. Generalized polynomial chaos expansion is implemented to reduce the dimensionality of propagating uncertainty through scales and the computational costs of the upscaling method. It is integrated into a hybrid optimization procedure with the genetic algorithm and sequential quadratic programming. Two structural engineering problems that involve uncertainties in elastic material properties and geometric properties at fine scales are presented to demonstrate the applicability and merit of the proposed technique.",2013
"A Confidence-Based Adaptive Sampling Approach for Dynamic Reliability Analysis","Dynamic reliability is defined as the probability that an engineered system successfully performs the predefined functionality over a certain period of time considering time-variant operation condition and component deterioration. In practice, it is still a major challenge to conduce dynamic reliability analysis due to the prohibitively high computational costs. In this study, a confidence-based meta-modeling approach is proposed for efficient sensitivity-free dynamic reliability analysis, referred to as double-loop adaptive sampling (DLAS). In DLAS a Gaussian process (GP) model is constructed to approximate extreme system responses over time, so that Monte Carlo simulation (MCS) can be employed directly to estimate dynamic reliability. A qualitative confidence measure is proposed to evaluate the accuracy of dynamic reliability estimation while using the MCS approach based on developed GP models. To improve the confidence, a double-loop adaptive sampling scheme is developed to efficiently update the GP model in a sequential manner, by considering system input variables and time concurrently in double sampling loops. The model updating process can be terminated once the user defined confidence target is satisfied. The DLAS approach does not require computationally expensive sensitivity analysis, thus substantially improves the efficiency of dynamic reliability assessment. Two case studies are used to demonstrate the effectiveness of DLAS for dynamic reliability analysis.",2013
"An Integrated Performance Measure Approach for System Reliability Assessment","This paper presents an integrated performance measure approach (iPMA) for system reliability assessment considering multiple dependent failure modes. An integrated performance function is developed to envelope all component level failure events, thereby enables system reliability approximation by considering only one integrated system limit state. The developed integrated performance function possesses two critical properties. First, it represents exact joint failure surface defined by multiple component failure events, thus no error will be induced due to the integrated limit state function in system reliability computation. Second, smoothness of the integrated performance on system failure surface can be guaranteed, therefore advanced response surface techniques can be conveniently employed for response approximation. With the developed integrated performance function, the maximum confidence enhancement based sequential sampling method is adopted as an efficient component reliability analysis tool for system reliability approximation. To furthermore improve the computational efficiency, a new constraint filtering technique is developed to adaptively identify active limit states during the iterative sampling process without inducing any extra computational cost. One case study is used to demonstrate the effectiveness of system reliability assessment using the developed iPMA methodology.",2013
"Inverse Reliability Analysis for Approximated Second-Order Reliability Method Using Hessian Update","According to order of approximation, there are two types of analytical reliability analysis methods; first-order reliability method and second-order reliability method. Even though FORM gives acceptable accuracy and good efficiency for mildly nonlinear performance functions, SORM is required in order to accurately estimate the probability of failure of highly nonlinear functions due to its large curvature. Despite its necessity, SORM is not commonly used because the calculation of the Hessian is required. To resolve the heavy computational cost in SORM due to the Hessian calculation, a quasi-Newton approach to approximate the Hessian is introduced in this study instead of calculating the Hessian directly. The proposed SORM with the approximated Hessian requires computations only used in FORM leading to very efficient and accurate reliability analysis. The proposed SORM also utilizes the generalized chi-squared distribution in order to achieve better accuracy. Furthermore, an SORM-based inverse reliability method is proposed in this study as well. A reliability index corresponding to the target probability of failure is updated using the proposed SORM. Two approaches in terms of finding more accurate most probable point using the updated reliability index are proposed and compared with existing methods through numerical study. The numerical study results show that the proposed SORM achieves efficiency of FORM and accuracy of SORM.",2013
"Confidence-Based Method for Reliability-Based Design Optimization","An accurate input probabilistic model is necessary to obtain a trustworthy result in the reliability analysis and the reliability-based design optimization (RBDO). However, the accurate input probabilistic model is not always available. Very often only insufficient input data are available in practical engineering problems. When only the limited input data are provided, uncertainty is induced in the input probabilistic model and this uncertainty propagates to the reliability output which is defined as the probability of failure. Then, the confidence level of the reliability output will decrease. To resolve this problem, the reliability output is considered to have a probability distribution in this paper. The probability of the reliability output is obtained as a combination of consecutive conditional probabilities of input distribution type and parameters using Bayesian approach. The conditional probabilities that are obtained under certain assumptions and Monte Carlo simulation (MCS) method is used to calculate the probability of the reliability output. Using the probability of the reliability output as constraint, a confidence-based RBDO (C-RBDO) problem is formulated. In the new probabilistic constraint of the C-RBDO formulation, two threshold values of the target reliability output and the target confidence level are used. For effective C-RBDO process, the design sensitivity of the new probabilistic constraint is derived. The C-RBDO is performed for a mathematical problem with different numbers of input data and the result shows that C-RBDO optimum designs incorporate appropriate conservativeness according to the given input data.",2013
"Time-Dependent Reliability Analysis Using the Total Probability Theorem","A new reliability analysis method is proposed for time-dependent problems with limit-state functions of input random variables, input random processes and explicit in time using the total probability theorem and the concept of composite limit state. The input random processes are assumed Gaussian. They are expressed in terms of standard normal variables using a spectral decomposition method. The total probability theorem is employed to calculate the time-dependent probability of failure using a time-dependent conditional probability which is computed accurately and efficiently in the standard normal space using FORM and a composite limit state of linear instantaneous limit states. If the dimensionality of the total probability theorem integral (equal to the number of input random variables) is small, we can easily calculate it using Gauss quadrature numerical integration. Otherwise, simple Monte Carlo simulation or adaptive importance sampling is used based on a pre-built Kriging metamodel of the conditional probability. An example from the literature on the design of a hydrokinetic turbine blade under time-dependent river flow load demonstrates all developments.",2013
"Topology Optimization With Unknown-but-Bounded Load Uncertainty","In this paper, convex modeling based topology optimization with load uncertainty is presented. The load uncertainty is described using the non-probabilistic based unknown-but-bounded convex model, and the strain energy based topology optimization problem under uncertain loads is formulated. Unlike the conventional deterministic topology optimization problem, the maximum possible strain energy under uncertain loads is selected as the new objective in order to achieve a safe solution. Instead of obtaining approximated solutions as used before, an exact solution procedure is presented. The problem is first formulated as a single level optimization problem, and then rewritten as a two-level optimization problem. The upper level optimization problem is solved as a deterministic topology optimization with the load which generated from the worst structure response in the lower level problem. The lower level optimization problem is to identify this worst structure response, and it is found equivalent to an inhomogeneous eigenvalue problem. Three different cases are discussed for accurately evaluating the global optima of the lower level optimization problem, while the corresponding sensitivities are derived individually. With the function value and sensitivity information ready, the upper level optimization problem can be solved through existing gradient based optimization algorithms. The effectiveness of the proposed convex modeling based topology optimization is demonstrated through different numerical examples.",2013
"Applying Robust Design Optimization to Large Systems","While Robust Optimization has been utilized for a variety of design problems, application of Robust Design to the control of large-scale systems presents unique challenges to assure rapid convergence of the solution. Specifically, the need to account for uncertainty in the optimization loop can lead to a prohibitively expensive optimization using existing methods when using robust optimization for control. In this work, a robust optimization framework suitable for operational control of large scale systems is presented. To enable this framework, robust optimization uses a utility function for the objective, dimension reduction in the uncertainty space, and a new algorithm for evaluating probabilistic constraints. The proposed solution accepts the basis in utility theory, where the goal is to maximize expected utility. This allows analytic gradient and Hessian calculations to be derived to reduce the number of iterations required. Dimension reduction reduces uncertain functions to low dimensional parametric uncertainty while the new algorithm for evaluating probabilistic constraints is specifically formulated to reuse information previously generated to estimate the robust objective. These processes reduce the computational expense to enable robust optimization to be used for operational control of a large-scale system. The framework is applied to a multiple-dam hydropower revenue optimization problem, then the solution is compared with the solution given by a non-probabilistic safety factor approach. The solution given by the framework is shown to dominate the other solution by improving upon the expected objective as well as the joint failure probability.",2013
"An Efficient Reliability Analysis Method for Structures With Epistemic Uncertainty Using Evidence Theory","Evidence theory has a strong ability to deal with the epistemic uncertainty, based on which the uncertain parameters existing in many complex engineering problems with limited information can be conveniently treated. However, the heavy computational cost caused by its discrete property severely influences the practicability of evidence theory, which has become a main difficulty in structural reliability analysis using evidence theory. This paper aims to develop an efficient method to evaluate the reliability for structures with evidence variables, and hence improves the applicability of evidence theory for engineering problems. A non-probabilistic reliability index approach is introduced to obtain a design point on the limit-state surface. An assistant area is then constructed through the obtained design point, based on which a small number of focal elements can be picked out for extreme analysis instead of using all the elements. The vertex method is used for extreme analysis to obtain the minimum and maximum values of the limit-state function over a focal element. A reliability interval composed of the belief measure and the plausibility measure is finally obtained for the structure. Two numerical examples are investigated to demonstrate the effectiveness of the proposed method.",2013
