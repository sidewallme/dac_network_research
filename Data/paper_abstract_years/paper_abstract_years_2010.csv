title,Abstract,year
"Continuous Preference Trend Mining for Optimal Product Design With Multiple Profit Cycles","Product and design analytics is emerging as a promising area for the analysis of large-scale data and reflection of the extracted knowledge for the design of optimal system. The Continuous Preference Trend Mining (CPTM) algorithm and a framework that are proposed in this study address some fundamental challenges in the context of product and design analytics. The first contribution is the development of a new predictive trend mining technique that captures a hidden trend of customer purchase patterns from large accumulated transactional data. Different from traditional, static data mining algorithms, the CPTM does not assume the stationarity, and dynamically extract valuable knowledge of customers over time. By generating trend embedded future data, the CPTM algorithm not only shows higher prediction accuracy in comparison with static models, but also provide essential properties that could not be achieved with a previous proposed model: avoiding an over-fitting problem, identifying performance information of constructed model, and allowing a numeric prediction. The second contribution is a predictive design methodology in the early design stage. The framework enables engineering designers to optimize product design over multiple life cycles while reflecting customer preferences and technological obsolescence using the CPTM algorithm. For illustration, the developed framework is applied to an example of tablet PC design in leasing market and the result shows that the selection of optimal design is achieved over multiple life cycles.",2010
"Visual HDMR Model Refinement Through Iterative Interaction","In engineering design, time-consuming simulations may be needed to find the input-output relationship of a system. High Dimensional Model Representation (HDMR) alleviates the need for intensive simulation by approximating the system’s design space with a surrogate model. Although HDMR can provide an overview, specific regions of interest to the designer may require higher accuracy. This paper presents a tool to visualize and interactively improve HDMR accuracy in specified regions of the design space. Regions of the HDMR are selected by iterative brushing in two-dimensional scatterplot planes. Once a region is chosen, designers may concentrate sampling within its bounds to improve the model locally. Regions can be also improved by modeling the error with a localized radial basis function (RBF) metamodel. The effect of local refinement was further evaluated with localized performance metrics. Testing of the tool shows that it can effectively display and improve HDMR models in regions of interest, if there are variables which have a dominating influence on the output.",2010
"Design Analytics in Consumer Product Design: A Simulated Study","A growing area of research in the engineering community is the use of data and analytics for transforming information into knowledge to design better systems, products, and processes. Data-driven decisions can be made in the early, middle, and late stages in a design process where customer needs are identified and understood, a final concept for a design is chosen, and usage data from the deployed product is captured, respectively. Design Analytics (DA) is a paradigm for improving the core information-to-knowledge transformations in these stages of a design process resulting in better performing and functioning products that reflect both explicit and implicit customer needs. In this paper, a simulator is used to model usage of a hypothetical refrigerator and generate artificial data driven by four different customer behavior profiles with variation. The population of customers is randomly divided among the four behavior profiles so that the underlying customer preferences are unknown to the experimenter prior to data analysis. The purpose of the simulation is to illustrate the use of DA in the late stage of a design process to improve the transition from an existing product to the next generation product. Metrics are developed to analyze the product usage data, and both prevailing and subtle usage trends are identified. After conclusions are made, the study proceeds to the early and middle stages of a subsequent design process where a hypothetical next-generation refrigerator is conceptualized.",2010
"A Simulation Based Estimation of Crowd Ability and its Influence on Crowdsourced Evaluation of Design Concepts","Crowdsourced evaluation is a promising method for evaluating attributes of design concepts that require human input. One factor in obtaining good evaluations is the ratio of high-ability to low-ability participants within the crowd. In this paper we introduce a Bayesian network model capable of finding participants with high design evaluation ability, so that their evaluations may be weighted more than those of the rest of the crowd. The Bayesian network model also estimates a score of how well each design concept performs with respect to a design attribute without knowledge of the true scores. Monte Carlo simulation studies tested the quality of the estimations on a variety of crowds consisting of participants with different evaluation ability. Results suggest that the Bayesian network model estimates design attribute performance scores much closer to their true values than simply weighting the evaluations from all participants in the crowd equally. This finding holds true even when the group of high ability participants is a small percentage of the entire crowd.",2010
"A Scalable Preference Elicitation Algorithm Using Group Generalized Binary Search","We examine the problem of eliciting the most preferred designs of a user from a finite set of designs through iterative pairwise comparisons presented to the user. The key challenge is to select proper queries (i.e., presentations of design pairs to the user) in order to minimize the number of queries. Previous work formulated elicitation as a blackbox optimization problem with comparison (binary) outputs, and a heuristic search algorithm similar to Efficient Global Optimization (EGO) was used to solve it. In this paper, we propose a query algorithm that minimizes the expected number of queries directly, assuming that designs are embedded in a known space and user preference is a linear function of design variables. Besides its theoretical foundation, the proposed algorithm shows empirical performance better than the EGO search algorithm in both simulated and real-user experiments. A novel approximation scheme is also introduced to alleviate the scalability issue of the proposed algorithm, making it tractable for a large number of design variables or of candidate designs.",2010
"Selection of Precision Machining Cutting Parameters via a Modified Efficient Global Optimization Approach","The primary objective in precision machining is usually to attain excellent dimensional accuracy and surface finish. In addition, complimentary objectives such as cost and production rate are also important. Proper selection of cutting parameters can profoundly affect both primary and secondary machining performance objectives. While simplified and/or empirical models exist for machining processes, none of those models provides accurate prediction of the dynamic cutting forces, which in turn govern the obtainable quality of the machined surfaces. Finite element analysis (FEA) via ABAQUS/Explicit is adopted in this paper for predicting the machining dynamic cutting forces. Rake and clearance angles, as well as cutting speed are set as the design variables for optimization. Since the machining model requires significant computational resources, economizing the number of FEA runs is desirable. The optimization approach adopted is based off Efficient Global Optimization (EGO), where Kriging models are trained to predict the underlying behavior of the machining process via a finite set of sample points. New sample points are then generated via a multi-objective genetic algorithm that seeks locations of optima and/or high uncertainty in the Kriging models. Machining performance of the new samples is then evaluated via FEA, the Kriging models are re-trained and the process is repeated until one of termination criteria is met. The application study presented is an orthogonal cutting test for ultra-precision micro-cutting using diamond tools.",2010
"Automatic Reasoning for Defining Lathe Operations for Mill-Turn Parts","With the increase in computer-controlled hybrid machining (e.g. mill-turn machines), one needs to discern what features of a part are created during turning (i.e. with a lathe cutter) versus those created by milling. Given a generic part shape, it is desirable to extract the turnable and non-turnable features in order to obtain feasible machining plans. A novel approach for automating this division and for defining the resulting turning operations in a hybrid process is proposed in this paper. The algorithm is based on identifying the dominant rotational-axis and performing several non-uniform lateral cross-sections to quickly generate the “as lathed” model. The part is then subtracted from the original model to isolate the non-turnable features. Next, resulting model and features are translated to a label rich graph and fed into a grammar reasoning tool to produce feasible manufacturing plans. The setup design is also studied against the tolerances specified by the designer. Performance of the algorithm has been tested on several examples ranging from simple to complex parts.",2010
"Tolerance Analysis of Parallel Assemblies Using Tolerance-Maps® and a Functional Map Derived From Induced Deformations","This paper concerns about modeling tolerance accumulation in parallel assemblies using a spatial math model, the T-Map. In this paper, a specific case in 3D is discussed where an Accumulation Tolerance-Map is modeled when two parts arranged in parallel support a target part between the datum and the functional target feature. By understanding how much of variation from the supporting parts contribute to variations of the target feature, a designer can better utilize the tolerance budget when assigning values to individual tolerances.",2010
"Tolerance Plugin Module in Integrated Design","Increased use of recycled material in high-end structural components based on wrought alloys is the goal of the EC project Suplight. The project proposes a framework for multi objective optimization. In this paper, a tolerance plugin module used in this project is described. The tolerance plugin module aims at controlling if the geometrical variation requirements on part level are fulfilled. The variation in the part stems from variation in material and process parameters and the relationship between variation in process and material parameters is estimated using designed computer experiments. Moreover, the tolerance plugin module offers an automatically generated meta-model, based on principal component analysis, for handling part variation that allows for faster Monte Carlo simulations and a format that can be used in variation simulations in succeeding assembly steps. The functionality is illustrated using two cases studies; one for investigation of geometrical part variation due to a stamping process and one for investigation of geometrical part variation due to a sheet metal forming process.",2010
"Machining Feature Modeling and Process Intermediate Model Generation in Process Planning","In process planning of machined part, machining feature recognition and representation, feature-based generative process planning, and the process intermediate model generation are the key issues. While many research results have been achieved in recent years, the complete modeling of machining features, process operations, and the 3D models in process planning are still need further research to make the techniques to be applied in practical CAPP systems. In this paper, a machining feature definition and classification method is proposed for the purpose of process planning based on 3D model. Machining features are defined as the surfaces formed by a serious of machining operation. The classification scheme of machining features is proposed for the purpose of feature recognition, feature-based machining operations reasoning, and knowledge representation. Recognized from B-Rep representation of design model, machining features are represented by adjacent graph and organized by feature relations. The machining process plan is modeled as operations and steps, which is the combination and sequencing of machining feature’s process steps. The process intermediate models (PIM) are important for process documentation, analysis and NC programming. An automatic PIM generation approach is proposed using local operations directly on B-Rep model. The proposed data structure and algorithm is adopted in the development of CAPP tool on solid modeler ACIS/HOOPS.",2010
"Design of Nonlinear Dynamic Systems Using Surrogate Models of Derivative Functions","Optimization of nonlinear (or linear state-dependent) dynamic systems often requires system simulation. In many cases the associated state derivative evaluations are computationally expensive, resulting in simulations that are significantly slower than real-time. This makes the use of optimization techniques in the design of such systems impractical. Optimization of these systems is particularly challenging in cases where control and physical systems are designed simultaneously. In this article, an efficient two-loop method, based on surrogate modeling, is proposed for solving dynamic system design problems with computationally expensive derivative functions. A surrogate model is constructed for only the derivative function instead of the complete system analysis, as is the case in previous studies. This approach addresses the most expensive element of system analysis (i.e., the derivative function), while limiting surrogate model complexity. Simulation is performed based on the surrogate derivative functions, preserving the nature of the dynamic system, and improving estimation accuracy. The inner loop solves the system optimization problem for a given derivative function surrogate model, and the outer loop updates the surrogate model based on optimization results. This solution approach presents unique challenges. For example, the surrogate model approximates derivative functions that depend on both design and state variables. As a result, the method must not only ensure accuracy of the surrogate model near the optimal design point in the design space, but also the accuracy of the model in the state space near the state trajectory that corresponds to the optimal design. This method is demonstrated using two simple design examples, followed by a wind turbine design problem. In the last example, system dynamics are modeled using a linear state-dependent model where updating the system matrix based on state and design variable changes is computationally expensive.",2010
"Development of a Common Platform for Testing Metamodel Based Design Optimization Methods","Metamodel based design optimization (MBDO) algorithms have attracted considerable interests in recent years due to their special capability in dealing with complex optimization problems with computationally expensive objective and constraint functions and local optima. Conventional unimodal-based optimization algorithms and stochastic global optimization algorithms either miss the global optimum frequently or require unacceptable computation time. In this work, a generic testbed/platform for evaluating various MBDO algorithms has been introduced. The purpose of the platform is to facilitate quantitative comparison of different MBDO algorithms using standard test problems, test procedures, and test outputs, as well as to improve the efficiency of new algorithm testing and improvement. The platform consists of a comprehensive test function database that contains about 100 benchmark functions and engineering problems. The testbed accepts any optimization algorithm to be tested, and only requires minor modifications to meet the test-bed requirements.",2010
"Improved Trust Region Based MPS Method for High-Dimensional Expensive Black-Box Problems","Mode Pursuing Sampling (MPS) was developed as a global optimization algorithm for optimization problems involving expensive black box functions. MPS has been found to be effective and efficient for problems of low dimensionality, i.e., the number of design variables is less than ten. A previous conference publication integrated the concept of trust regions into the MPS framework to create a new algorithm, TRMPS, which dramatically improved performance and efficiency for high dimensional problems. However, although TRMPS performed better than MPS, it was unproven against other established algorithms such as GA. This paper introduces an improved algorithm, TRMPS2, which incorporates guided sampling and low function value criterion to further improve algorithm performance for high dimensional problems. TRMPS2 is benchmarked against MPS and GA using a suite of test problems. The results show that TRMPS2 performs better than MPS and GA on average for high dimensional, expensive, and black box (HEB) problems.",2010
"Indexing Methods for Discrete Mode Pursuing Sampling","Discrete Mode Pursuing Sampling (D-MPS) is a method used for optimization of expensive black-box functions with discrete variables. The type of discrete space where all possible combinations of discrete values of all variables are valid design points is called a full grid of points or a “regular grid” in this paper. A regular structure for sampling data is a requirement when D-MPS is applied. This paper presents two new indexing methods, i.e. “",2010
"Diagnostic Analysis of Metamodels’ Multivariate Dependencies and Their Impacts in Many-Objective Design Optimization","While computers are continually getting faster, physical models of complex systems grow more sophisticated and keep pace. Using metamodels can dramatically reduce the time it takes to evaluate a solution for a complex system; however, while the chief virtue of metamodels is that they approximate more computationally expensive models, this is also their main drawback. Metamodels are approximations, and as such they behave differently than the more sophisticated models they approximate. While the metamodels may be accurate approximations, they may also introduce new interdependencies in the response outputs that may hinder search algorithms during optimization. Understanding the impact of the approximation on the subsequent search thus becomes an important part of the problem as models that were computationally expensive ten to fifteen years ago may now run fast enough for use in optimization. In this paper, we use Sobol′ global sensitivity analysis to compare the search performance of a new auto-adaptive many objective evolutionary algorithm solving a challenging product family design problem with both the original analysis and a second-order response surface approximation of the original analysis. Interdependencies in the response outputs are found to result from the problem formulation used rather than the underlying model in this case. Search operator selection by the auto-adaptive evolutionary algorithm is shown to be consistent with the model sensitivities found by global sensitivity analysis.",2010
"Classifier-Guided Sampling for Discrete Variable, Discontinuous Design Space Exploration","Estimation of density algorithms (EDAs) have been developed for optimization of discrete, continuous, or mixed discrete and continuous simulation-based design problems. EDAs construct a probability distribution on the set of highest performing designs and sample the distribution for the next generation of solutions. In previous work, the authors have demonstrated how classifier-guided sampling can also be used for discrete variable, discontinuous design space exploration. In this paper we develop the rationale for using classifier-guided sampling as a simple step beyond EDAs that not only improves the characterization of the highest performing designs but also identifies the poorly performing designs and exploits that information for faster convergence to optimal solutions. The resulting method is novel in its use of Bayesian priors to model the inherent uncertainty in a probability distribution that is based on a limited number of samples from the design space. The new classifier-guided method is applied to several example problems and convergence rates are presented that compare favorably to random search and a basic EDA implementation.",2010
"A Data Mining Trajectory Clustering Methodology for Modeling Indoor Design Space Utilization","Traditionally, understanding indoor space utilization in a typical design setting has been based on observation methodologies, where researchers document team interactions, space utilization and design activities using qualitative observation techniques. The authors of this paper propose a data mining driven methodology aimed at modeling the utilization of indoor design spaces using trajectory pattern data. Using indoor Radio-frequency identification (RFID) technology, researchers are able to collect trajectory data which can then be used to quantify the distribution of space usage patterns over time and predict future regions of interest. The proposed methodology consists of two phases: i) trajectory partitioning and ii) line segment clustering. For the first phase, trajectories are partitioned into line segments, based on unique user characteristics. In the second phase, a data mining clustering algorithm is employed to group line segments into different clusters based on a distance function. Since individual trajectories may exhibit similar movement patterns, the proposed methodology can help designers better understand how design spaces are utilized and how team dynamics evolve over time, depending on the specific design task being executed. A 3,500 square foot design space was used for the semester long study that included design teams supervised by teaching assistants. The results provide insight into the underutilization of certain regions of the design space and proposes directions towards an optimal design space methodology.",2010
"A Z-Score-Based Method to Synthesize Anthropometric Datasets for Global User Populations","Globalized marketplaces are necessitating the consideration of the needs of users from a variety of national and international regions. Relevant body dimensions are known to play a key role in influencing users’ physical interactions with products. The main challenge in designing these products is the unavailability of comprehensive anthropometric databases for detailed analyses and decision-making. This paper presents a new method to this end. Z-scores are computed for each body measure of every individual in a reference population; this can be any population for which a comprehensive database is available. Next, descriptive statistical information (e.g., means, standard deviations, by-percentile values) from numerous studies and surveys are used to estimate distributions of the required body dimensions. Finally, the z-score values from the reference population are utilized to sample from the aforementioned distributions in order to synthesize the requisite virtual target population of users. The z-score method is demonstrated in the context of two existing populations: U.S. military in the late 1980s (ANSUR) and Japanese youth from the early 1990s. Despite certain stated limitations, which are topics of future work in this line of research, the method is shown to be accurate, easy-to-apply, and robust in terms of underlying assumptions.",2010
"Building of Usage Scenarios Space for Investigating the Fall Situations of the Elderly People","Assessing the number of users defined by a set of specific usage attributes in a given usage contextual situation is not always an obvious task in a market segmentation process. Although new approaches in design and marketing seem to be more sensitive to the adequacy of a design concept with the usage scenarios, these methods do not systematically consider the various usage situations. The present article puts forward a methodology intending to build a usage scenarios space in which the input data is thoroughly collected and validated. This methodology is applied to the complex and multifactorial issue of falls among the elderly in the Metropolitan France. In this paper, numerous medical publications have been made to study influential factors of fall situations. However, even solution providers for fall prevention and teleassistance ignore the real situational coverage of their solutions. As a result, “usage scenarios space” is built using an appropriate segmentation of usage contexts (here, fall situations) and user characteristics. These data are used for a design oracle to predict (simulate) the various and multiple usage scenarios.",2010
"Computer-Aided Customized Shape Design of an N95 Filtering Facepiece Respirator","A respirator protects its user by sealing the user’s face and filtering hazardous particles from environment. However, faceseal leakages of users-respirators always happen. First, this study investigated a computer-aided technique for designing a well fitted N95 filtering facepiece respirator (FFR) for a subject. The customized N95 FFR includes a customized contact area, a center filtering area, two straps and a nasal clip. Five base contact areas of National Institution for Occupational Safety and Health (NIOSH) headforms were created and the customized contact area was modeled using the mapping relationship between the subject and a NIOSH headform. The center filtering area was designed by considering constraints of N95 FFR shape. Second, this study used simulation-based approaches, including the FE method and computational fluid dynamics (CFD) method, to assess the performance of the customized N95 FFR on the subject. The contact pressure and the faceseal leakage from the subject-customized N95 FFR combination were compared with the results from the subject-existing N95 FFRs combinations. The comparison showed that the customized N95 FFR provided the subject an optimized contact pressure distribution and no faceseal leakage.",2010
"Potential Methods for Prediction of Onset of Slip in Gait During the Transition From Double Support to Single Support","This paper investigates two methods for the prediction of onset of slip in gait during the transition from single support to double support. The first method employs an optimization-based gait prediction simulation to determine the threshold of walking velocity that results in stable gait for one stride length and one subject. In order to determine the threshold, the simulation is carried out with progressively increasing walking velocities (initial conditions) until the gait becomes unstable. The zero moment point (ZMP) and support region are employed as criteria for stable gait, i.e. as long as the ZMP remains within the support region throughout the duration of gait, the gait is said to be stable. The second method employs a probabilistic simulation to predict the likelihood of slip, where the likelihood of slip is related to the available and required friction for gait.",2010
"Cognitive-Based Terminal State Prediction for Human Motion Planning","Every day, people are presented with tasks that are completed with very little mechanical effort such as turning a door knob, turning a screw driver, and grabbing a cup to move it to a new location and/or orientation. These tasks are often overlooked in the mechanical study of human movement due to the fact they carry with them very little biomechanical costs or effort. However, from a cognitive standpoint, these tasks carry high complexity. For example, the simple task of grabbing a cup and flipping it over is very easy mechanically and can be effortlessly achieved by a two year old. However, it is impossible to predict or simulate this motion without major intervention in the form of explicit constraints defining the task. The model itself cannot decide in which orientation the hand should assume in order to grasp the object. Also, it cannot decide where on the object the hand should be placed. These aspects must be assumed by the researcher and constrained in the formulation. In other words, digital human models, such as optimization-based motion prediction models, are unable to plan actions. This implies that for each task, a unique optimization formulation is needed in order to predict the motion/posture needed to complete each task. This paper presents a new method for task planning prediction within optimization based posture and motion prediction. It provides a new single optimization formulation that allows for the prediction of multiple unique manual manipulation tasks. The method is based on observations made from experimental studies on cognitive motor planning.",2010
"Hierarchical Bayesian Parameter Estimation for Modeling and Analysis of User Affective Influence","Traditional user experience (UX) models are mostly qualitative in terms of its measurement and structure. This paper proposes a quantitative UX model based on cumulative prospect theory. It takes a decision making perspective between two alternative design profiles. However, affective elements are well-known to have influence on human decision making, the prevailing computational models for analyzing and simulating human perception on UX are mainly cognition-based models. In order to incorporate both affective and cognitive factors in the decision making process, we manipulate the parameters involved in the cumulative prospect model to show the affective influence. Specifically, three different affective states are induced to shape the model parameters. A hierarchical Bayesian model with a technique called Markov chain Monte Carlo is used to estimate the parameters. A case study of aircraft cabin interior design is illustrated to show the proposed methodology.",2010
"Reconstructing Humans’ Hand Motion: Preliminary Results and Applications in the Design of Mechanical Fingers for Anthropomorphic Tasks","This paper reports the development of a low-cost sensor-based glove device using commercially available components that can be used to obtain position, velocity and acceleration data for individual fingers of the hand. Optical tracking of the human hand and finger motion is a challenging task due to the large number of degrees of freedom (DOFs) packed in a relatively small space. We propose methods to simplify the hand motion capture by utilizing accelerometers and adopting a reduced marker protocol.",2010
"Pareto Front Identification via Objective Vector Jacobian Matrix Singularity","This paper presents a method to identify the exact Pareto front for a multi-objective optimization problem. The developed technique addresses the identification of the Pareto frontier in the cost space and the Pareto set in the design space for both constrained and unconstrained optimization problems. The proposed approach identifies a ",2010
"Multi-Objective Robust Optimization Using Differential Evolution and Sequential Quadratic Programming","Multi-Objective Robust Optimization (MORO) can find Pareto solutions to multi-objective engineering problems while keeping the variation of the solutions being within an acceptable range when parameters vary. While the literature reports on many techniques in MORO, few papers focus on the implementation of Multi-Objective Differential Evolution (MODE) for robust optimization and the performance improvement of solutions. In this paper, MODE is first modified and implemented for robust optimization, formulating a new MODE-RO algorithm. To improve the solutions’ quality of MODE-RO, a new hybrid MODE-SQP-RO algorithm is further proposed, where Sequential Quadratic Programming (SQP) is incorporated to enhance the local search. In the hybrid algorithm, two criteria, indicating the convergence speed of MODE-RO and the switch between MODE and SQP are proposed respectively. One numerical and one engineering examples are tested to demonstrate the applicability and performance of the proposed algorithms. The results show that MODE-RO is effective in solving Multi-Objective Robust Optimization problems; while on the average, MODE-SQP-RO significantly improves the quality of robust solutions with comparable numbers of function evaluations.",2010
"Robustness Against Large Variations in Multi-Objective Optimization Problems","In the presence of multiple optimal solutions in multi-modal optimization problems and in multi-objective optimization problems, the designer may be interested in the robustness of those solutions to make a decision. Here, the robustness is related to the sensitivity of the performance functions to uncertainties. The uncertainty sources include the uncertainties in the design variables, in the design environment parameters, in the model of objective functions and in the designer’s preference. There exist many robustness indices in the literature that deal with small variations in the design variables and design environment parameters, but few robustness indices consider large variations. In this paper, a new robustness index is introduced to deal with large variations in the design environment parameters. The proposed index is bounded between zero and one, and measures the probability of a solution to be optimal with respect to the values of the design environment parameters. The larger the robustness index, the more robust the solution with regard to large variations in the design environment parameters. Finally, two illustrative examples are given to highlight the contributions of this paper.",2010
"Sensitivity Analysis in Quantified Interval Constraint Satisfaction Problems","Interval is an alternative to probability distribution in quantifying epistemic uncertainty for reliability analysis when there is a lack of data to fit a distribution with good confidence. It only requires the information of lower and upper bounds. The propagation of uncertainty is analyzed by solving interval-valued constraint satisfaction problems (CSPs). By introducing logic quantifiers, quantified constraint satisfaction problems (QCSPs) can capture more semantics and engineering intent than CSPs. Sensitivity analysis (SA) takes into account of variations associated with the structure and parameters of interval constraints to study to which extent they affect the output. In this paper, a global SA method is developed for QCSPs, where the effects of quantifiers and interval ranges on the constraints are analyzed based on several proposed metrics, which indicate the levels of indeterminacy for inputs and outputs as well as unsatisfiability of constraints. Two vehicle design problems are used to demonstrate the proposed approach.",2010
"Multi-Objective Optimization of a Disc Brake System by Using SPEA2 and RBFN","Many engineering design optimization problems involve multiple conflicting objectives, which today often are obtained by computational expensive finite element simulations. Evolutionary multi-objective optimization (EMO) methods based on surrogate modeling is one approach of solving this class of problems. In this paper, multi-objective optimization of a disc brake system to a heavy truck by using EMO and radial basis function networks (RBFN) is presented. Three conflicting objectives are considered. These are: 1) minimizing the maximum temperature of the disc brake, 2) maximizing the brake energy of the system and 3) minimizing the mass of the back plate of the brake pad. An iterative Latin hypercube sampling method is used to construct the design of experiments (DoE) for the design variables. Next, thermo-mechanical finite element analysis of the disc brake, including frictional heating between the pad and the disc, is performed in order to determine the values of the first two objectives for the DoE. Surrogate models for the maximum temperature and the brake energy are created using RBFN with polynomial biases. Different radial basis functions are compared using statistical errors and cross validation errors (PRESS) to evaluate the accuracy of the surrogate models and to select the most accurate radial basis function. The multi-objective optimization problem is then solved by employing EMO using the strength Pareto evolutionary algorithm (SPEA2). Finally, the Pareto fronts generated by the proposed methodology are presented and discussed.",2010
"Optimization Algorithms and ODE’s in MDO","There is a need for a stronger theoretical understanding of Multidisciplinary Design Optimization (MDO) within the field. Having developed a differential geometry framework in response to this need, we consider how standard optimization algorithms can be modeled using systems of ordinary differential equations (ODEs) while also reviewing optimization algorithms which have been derived from ODE solution methods. We then use some of the framework’s tools to show how our resultant systems of ODEs can be analyzed and their behaviour quantitatively evaluated. In doing so, we demonstrate the power and scope of our differential geometry framework, we provide new tools for analyzing MDO systems and their behaviour, and we suggest hitherto neglected optimization methods which may prove particularly useful within the MDO context.",2010
"Integrated Design and Multi-Objective Optimization of a Single Stage Heat-Pump Turbocompressor","Small scale turbomachines in domestic heat pumps reach high efficiency and provide oil-free solutions which improve heat-exchanger performance and offer major advantages in the design of advanced thermodynamic cycles. An appropriate turbocompressor for domestic air based heat pumps requires the ability to operate on a wide range of inlet pressure, pressure ratios and mass flows, confronting the designer with the necessity to compromise between range and efficiency. Further the design of small-scale direct driven turbomachines is a complex and interdisciplinary task. Textbook design procedures propose to split such systems into subcomponents and to design and optimize each element individually. This common procedure, however, tends to neglect the interactions between the different components leading to suboptimal solutions. The authors propose an approach based on the integrated philosophy for designing and optimizing gas bearing supported, direct driven turbocompressors for applications with challenging requirements with regards to operation range and efficiency. Using previously validated reduced order models for the different components an integrated model of the compressor is implemented and the optimum system found via multi-objective optimization. It is shown that compared to standard design procedure the integrated approach yields an increase of the seasonal compressor efficiency of more than 12 points. Further a design optimization based sensitivity analysis allows to investigate the influence of design constraints determined prior to optimization such as impeller surface roughness, rotor material and impeller force. A relaxation of these constrains yields additional room for improvement. Reduced impeller force improves efficiency due to a smaller thrust bearing mainly, whereas a lighter rotor material improves rotordynamic performance. A hydraulically smoother impeller surface improves the overall efficiency considerably by reducing aerodynamic losses. A combination of the relaxation of the 3 design constraints yields an additional improvement of 6 points compared to the original optimization process. The integrated design and optimization procedure implemented in the case of a complex design problem thus clearly shows its advantages compared to traditional design methods by allowing a truly exhaustive search for optimum solutions throughout the complete design space. It can be used for both design optimization and for design analysis.",2010
"A Deterministic and Probabilistic Approach for Robust Optimal Design of a 6-DOF Haptic Device","This work suggests a two-stage approach for robust optimal design of 6-DOF haptic devices based on a sequence of deterministic and probabilistic analyses with a multi-objective genetic algorithm and the Monte-Carlo method. The presented model-based design robust optimization approach consider simultaneously the kinematic, dynamic, and kinetostatic characteristics of the device in both a constant and a dexterous workspace in order to find a set of optimal design parameter values for structural configuration and dimensioning. Design evaluation is carried out based on local and global indices, like workspace volume, quasi-static torque requirements for the actuators, kinematic isotropy, dynamic isotropy, stiffness isotropy, and natural frequencies of the device. These indices were defined based on focused kinematic, dynamic, and stiffness models. A novel procedure to evaluate local indices at a singularity-free point in the dexterous workspace is presented. The deterministic optimization approach neglects the effects from variations of design variables, e.g. due to tolerances. A Monte-Carlo simulation was carried out to obtain the response variation of the design indices when independent design parameters are simultaneously regarded as uncertain variables. It has been observed that numerical evaluation of performance indices depends of the type of workspace used during optimization. To verify the effectiveness of the proposed procedure, the performance indices were evaluated and compared in constant orientation and in dexterous workspace.",2010
"Using a Goal-Switching Selection Operator in Multi-Objective Genetic Algorithm Optimization Problems","This paper demonstrates how solution quality for multiobjective optimization problems can be improved by altering the selection phase of a multiobjective genetic algorithm. Rather than the traditional roulette selection used in algorithms like NSGA-II, this paper adds a goal switching technique to the selection operator. Goal switching in this context represents the rotation of the selection operator among a problem’s various objective functions to increase search diversity. This rotation can be specified over a set period of generations, evaluations, CPU time, or other factors defined by the designer. This technique is tested using a set period of generations before switching occurs, with only one objective considered at a time. Two test cases are explored, the first as identified in the Congress on Evolutionary Computation (CEC) 2009 special session and the second a case study concerning the market-driven design of a MP3 player product line. These problems were chosen because the first test case’s Pareto frontier is continuous and concave while being relatively easy to find. The second Pareto frontier is more difficult to obtain and the problem’s design space is significantly more complex. Selection operators of roulette and roulette with goal switching were tested with 3 to 7 design variables for the CEC 09 problem, and 81 design variables for the MP3 player problem. Results show that goal switching improves the number of Pareto frontier points found and can also lead to improvements in hypervolume and/or mean time to convergence.",2010
"Examining the Impact of Aggregated Design Impulses on Process Architecture in Distributed Design","During the design of complex systems, a design process may be subjected to stochastic inputs, interruptions, and changes. These design impulses can have a significant impact on the transient response and converged equilibrium for the design system. We distinguish this research by focusing on the interactions between local and architectural impulses in the form of designer mistakes and dissolution, division, and combination impulses, respectively. We find that local impulses tend to slow convergence but systems subjected to dissolution/division impulses still favor parallel arrangements. The strategy to mitigate combination impulses is unaffected by the presence of local impulses.",2010
"Linking 10 Years of Modular Design Research: Alternative Methods and Tool Chain Sequences to Support Product Platform Design","Modular product platforms have been shown to provide substantial cost and time savings while still allowing companies to offer a variety of products. As a result, a multitude of product platform methods have been developed over the last decade within the design research community. However, comparison and integration of suitable methods is difficult since the methods have, for the most part, been developed in isolation from one another. In reviewing the literature in modularity and product platforms, we create a generic set of twelve platform design activities. We then examine a set of product platform development processes used at several different companies, and from this form a generic sequence of the activities. We then associate the various developed methods to the sequence, thereby enabling the chaining together of the various modular and platform design methods developed by the community.",2010
"An Approach for Managing Engineering Changes in Product Families","Product development is characterized by continuous updating of existing solutions in order to cope with new market requirements. Families of product variants are used to satisfy the needs of new potential customers and penetrate new market niches.",2010
"Comprehensive Product Platform Planning (CP3) for a Modular Family of Unmanned Aerial Vehicles","A product family with a common platform paradigm can increase the flexibility and responsiveness of the product-manufacturing process and help take away market share from competitors that develop one product at a time. The recently developed Comprehensive Product Platform Planning (CP",2010
"A Concept Selection Framework for Early Sorting of Reconfigurable System Designs","A seven-step framework for sorting proposed concepts of system changes / reconfigurations is presented that seeks to characterize the overall ramifications on system architecture. This framework is intended for use immediately following a concept generation phase. The framework uses three simple questions: “What level of the system design does this concept apply to?” “What levels of the system design does the concept impact?” and “What is the severity of this impact?” A flowchart leads the designer through these questions and assigns each concept a classification from one to five based on the answers. Class one concepts have little to no impact on the rest of the system architecture. They can be included with little fear of massive change propagation and system redesign. Class five concepts carry large changes to system architecture and therefore should be included only if they can be shown to be highly beneficial, or if there remains enough design freedom such that the cost of changing the system architecture is minimal. Meanwhile, class five concepts are likely to have much higher potential to create revolutionary design. A case study is used to demonstrate the application of the sorting framework in the context of a Mars rover mission. Several example concepts are provided to illustrate key insights from the case study. Convergence of the framework is explored by comparing the authors’ results to a second test done by a new design team.",2010
"Green Profit Maximization Through Joint Pricing and Production Planning of New and Remanufactured Products","To achieve “green profit” in their business, manufacturers who produce both new and remanufactured products must optimize their pricing and production decisions simultaneously. They must determine the buy-back price and take-back quantity of end-of-life products as well as the selling prices and production quantities of new and remanufactured products. With an aim to assist in optimal pricing and production planning, this paper presents a mixed-integer programming model that optimizes the three prices (of buyback, new and remanufactured products) and the corresponding production plan simultaneously. The model considers the two conflicting objectives of maximizing economic profitability and maximizing environmental impact saving. The model helps address potential barriers to remanufacturing, which include limited economic, and/or environmental sustainability of remanufacturing, imbalance between the supply of end-of-life products and the market demand for remanufactured products, and cannibalization of the sales of new products. The developed model is illustrated with an example of engine water pump.",2010
"Product-Service Integration for the Sustainable Publishing Process","Publishing is the process of developing and producing content for distribution to the public. In the past, the publishing process heavily relies on printing as the method of content production. This causes voracious consumption and waste of natural resources. In today’s sweeping trend of digitization that is featured by the increasing popularity of various smart devices, the publishing process is undergoing a profound transition from the traditional printing-reliant publishing model to the new digital publishing model. Such a transition brings great opportunities for the publishing process to achieve better sustainability by evolving towards a product service system. This paper intends to advance the publishing process from the product service integration perspective. Above all, a general product-service integration framework is developed to describe the interdependent relationships among key stakeholders and elements in the publishing value chain. Furthermore, several specific publishing PSS are discussed. Finally, these publishing PSS are evaluated and compared from the value creation perspective.",2010
"A Product-Service System Model for Identifying Design Factors","In competitive market environments, strategies that adding services to products for sales promotion are now moved to integrate products and services for satisfying diverse customer needs, and the number of these cases is gradually increasing. Trends of integrating products and services lead to the emergence of a product-service system (PSS). To implement and embody a PSS solution in new product development, a comprehensive design framework is allowed designers to facilitate the design factors of the PSS in complex business environments. The objective of this paper is to propose a PSS model to identify design factors for developing products and services by integrating object-oriented concepts and blueprinting in context of a business ecosystem. The proposed model is developed based on relationship between products and services matching with their design factors. The products and the services are then brought together to form a PSS. Functions and processes can be categorized to identify the design factors in different levels using the object-oriented concepts. Interaction between products and services lies on a PSS platform to form a product service system in blueprinting. To demonstrate of the effectiveness of the proposed model, we use a case study involving a smart phone.",2010
"A Probabilistic Graphical Model for Evaluating Variable Energy Consumption During the Use Stage of a Product’s Lifecycle","Although energy consumption during use can cause a majority of a product’s environmental impact, the relationship between a product’s usage context and its environmental performance is rarely considered in design evaluations. Probabilistic graphical models (PGMs) provide the capability of evaluating uncertainty and variability of product use in addition to correlating the results with aspects of the usage context. This research demonstrates a method for representing the usage context as a PGM through the use of a lightweight vehicle design example. The demonstration PGM is constructed from factors such as driver behavior, alternative driving schedules, and residential density, which are related to local conditional probability distributions derived from publicly available data sources. Unique scenarios are then assembled from sets of conditions on these factors to provide insight into sources of variance in lifetime energy use. The vehicle example demonstrates that implementation of realistic usage scenarios via a PGM can provide a much higher fidelity investigation of energy savings during use than commonly found in the literature and that distinct scenarios can have significantly different implications for the effectiveness of lightweight vehicle designs.",2010
"An Envelope Approach to Time-Dependent Reliability Analysis for Mechanisms","This work is concerned with the time-dependent mechanism reliability defined over a period of time where a certain motion output is required. An envelope approach is proposed to improve the accuracy of the time-dependent mechanism reliability analysis. The envelope function of the motion error over the time period is created. Since the envelope function is not explicitly related to time, the time-dependent problem is converted into a time-independent problem. Then the envelope function is approximated by piecewise hyper-planes. To find the expansion points of the hyper-planes, the approach linearizes the motion error at the means of random dimension variables, and this approximation is accurate because the tolerances or the variances of the dimension variables are small. Then the expansion points are found with the maximum probability density at the failure threshold. The time-dependent mechanism reliability is then estimated by a multivariable normal distribution function at the expansion points. As an example, analytical equations are derived for a four-bar function generating mechanism. The numerical example shows the significant accuracy improvement.",2010
"A Design Oriented Reliability Methodology for Fatigue Life Under Stochastic Loadings","Fatigue damage analysis is critical for systems under stochastic loadings. To estimate the fatigue reliability at the design level, a hybrid reliability analysis method is proposed in this work. The First Order Reliability Method (FORM), the inverse FORM, and the peak distribution analysis are integrated for the fatigue reliability analysis at the early design stage. Equations for the mean value, the zero upcrossing rate, and the extreme stress distributions are derived for problems where stationary stochastic processes are involved. Then the fatigue damage is analyzed with the peak counting method. The developed methodology is demonstrated by a simple mathematical example and is then applied to the fatigue reliability analysis of a shaft under stochastic loadings. The results indicate the effectiveness of the proposed method in predicting fatigue damage and reliability.",2010
"Probabilistic Inverse Simulation and its Application in Vehicle Accident Reconstruction","Inverse simulation is an inverse process of direct simulation. It determines unknown input variables of the direct simulation for a given set of simulation output variables. Uncertainties usually exist, making it difficult to solve inverse simulation problems. The objective of this research is to account for uncertainties in inverse simulation in order to produce high confidence in simulation results. The major approach is the use of the maximum likelihood methodology, which determines not only unknown deterministic input variables but also the realizations of random input variables. Both types of variables are solved on the condition that the joint probability density of all the random variables is maximum. The proposed methodology is applied to a traffic accident reconstruction problem where the simulation output (accident consequences) is known and the simulation input (velocities of the vehicle at the beginning of crash) is sought.",2010
"Robust Design of Gears With Material and Load Uncertainties","Traditionally gears are designed using design standards such as AGMA, ISO, etc. These design standards include a large number of “design factors” accounting for various uncertainties related to geometry, load and material uncertainties. As the knowledge about these uncertainties increases, it becomes possible to include them systematically in the gear design procedure, thereby reducing the number of empirical design factors. In this paper a method is proposed to eliminate two design factors (viz., factor of safety in contact and reliability factor) used in standard AGMA-based design procedures through the formal introduction of uncertainty in the magnitude of load and material properties. The proposed method is illustrated via the design of an automotive gear with a desired reliability, cost, and robustness. The solutions obtained are encouraging and in-line with the existing knowledge about gear design, and thus reinforces the possibility of schematically reducing the aforementioned design factors.",2010
"Advanced Robust Optimization Approach for Design Optimization With Interval Uncertainty Using Sequential Quadratic Programming","Uncertainty is inevitable in real world. It has to be taken into consideration, especially in engineering optimization; otherwise the obtained optimal solution may become infeasible. Robust optimization (RO) approaches have been proposed to deal with this issue. Most existing RO algorithms use double-looped structures in which a large amount of computational efforts have been spent in the inner loop optimization to determine the robustness of candidate solutions. In this paper, an advanced approach is presented where no optimization run is required to be performed for robustness evaluations in the inner loop. Instead, a concept of Utopian point is proposed and the corresponding maximum variable/parameter variation will be obtained by just solving a set of linear equations. The obtained robust optimal solution from the new approach may be conservative, but the deviation from the true robust optimal solution is very small given the significant improvement in the computational efficiency. Six numerical and engineering examples are tested to show the applicability and efficiency of the proposed approach, whose solutions and computational time are compared with those from a similar but double-looped approach, SQP-RO, proposed previously.",2010
"Time-Dependent Reliability of Dynamic Systems Using Subset Simulation With Splitting Over a Series of Correlated Time Intervals","Time-dependent reliability is the probability that a system will perform its intended function successfully for a specified time. Unless many and often unrealistic assumptions are made, the accuracy and efficiency of time-dependent reliability estimation are major issues which may limit its practicality. Monte Carlo simulation (MCS) is accurate and easy to use but it is computationally prohibitive for high dimensional, long duration, time-dependent (dynamic) systems with a low failure probability. This work addresses systems with random parameters excited by stochastic processes. Their response is calculated by time integrating a set of differential equations at discrete times. The limit state functions are therefore, explicit in time and depend on time-invariant random variables and time-dependent stochastic processes. We present an improved subset simulation with splitting approach by partitioning the original high dimensional random process into a series of correlated, short duration, low dimensional random processes. Subset simulation reduces the computational cost by introducing appropriate intermediate failure sub-domains to express the low failure probability as a product of larger conditional failure probabilities. Splitting is an efficient sampling method to estimate the conditional probabilities. The proposed subset simulation with splitting not only estimates the time-dependent probability of failure at a given time but also estimates the cumulative distribution function up to that time with approximately the same cost. A vibration example involving a vehicle on a stochastic road demonstrates the advantages of the proposed approach.",2010
"Accounting for Test Variability Through Sizing Local Domains in Sequential Design Optimization With Concurrent Calibration-Based Model Validation","We have recently proposed a new method for combined design optimization and calibration-based validation using a sequential approach with variable-size local domains of the design space and statistical bootstrap techniques. Our work was motivated by the fact that model validation in the entire design space may be neither affordable nor necessary. The method proceeds iteratively by obtaining test data at a design point, constructing around it a local domain in which the model is considered valid, and optimizing the design within this local domain. Due to test variability, it is important to know how many tests are needed to size each local domain of the sequential optimization process. Conducting an unnecessarily large number of tests may be inefficient, while a small number of tests may be insufficient to achieve the desired validity level. In this paper, we introduce a technique to determine the number of tests required to account for their variability by sizing the local domains accordingly. The goal is to achieve a desired level of model validation in each domain using the correlation between model data at the center and any other point in the local domain. The proposed technique is illustrated by means of a piston design example.",2010
"Reliability and Functionality of Repairable Systems Using a Minimal Set of Metrics: Design and Maintenance of a Smart Charging Microgrid","The definition of reliability may not be readily applicable for repairable systems. Our recent work has shown that multiple metrics are needed to fully account for the performance of a repairable system under uncertainty. Optimal tradeoffs among a minimal set of metrics can be used in the design and maintenance of these systems. A minimal set of metrics provides the most information about the system with the smallest number of metrics using a set of desirable properties. Critical installations such as a remote microgrid powering a military installation require a careful consideration of cost and repair strategies. This is because of logistical challenges in performing repairs and supplying necessary spare parts, particularly in unsafe locations. This paper shows how a minimal set of metrics enhances decision making in such a scenario. It enables optimal tradeoffs between critical attributes in decision making, while guaranteeing that all important performance measures are satisfied. As a result, cost targets and inventory planning can be achieved in an optimal way. We demonstrate the value of the proposed approach using a US Army smart-charging microgrid installation.",2010
"Preposterior Analysis to Select Experimental Responses for Improving Identifiability in Model Uncertainty Quantification","In physics-based engineering modeling and uncertainty quantification, distinguishing the effects of two main sources of uncertainty — calibration parameter uncertainty and model discrepancy — is challenging. Previous research has shown that identifiability can sometimes be improved by experimentally measuring multiple responses of the system that share a mutual dependence on a common set of calibration parameters. In this paper, we address the issue of how to select the most appropriate subset of responses to measure experimentally, to best enhance identifiability. We propose a preposterior analysis approach that, prior to conducting the physical experiments but after conducting computer simulations, can predict the degree of identifiability that will result using different subsets of responses to measure experimentally. We quantify identifiability via the posterior covariance of the calibration parameters, and predict it via the preposterior covariance from a modular Bayesian Monte Carlo analysis of a multi-response Gaussian process model. The proposed method is applied to a simply supported beam example to select two out of six responses to best improve identifiability. The estimated preposterior covariance is compared to the actual posterior covariance to demonstrate the effectiveness of the method.",2010
"A Maximum Confidence Enhancement Based Sequential Sampling Scheme for Simulation-Based Design","This paper presents a maximum confidence enhancement based sequential sampling approach for simulation-based design under uncertainty. In the proposed approach, the ordinary Kriging method is adopted to construct surrogate models for all constraints and thus Monte Carlo simulation (MCS) is able to be used to estimate reliability and its sensitivity with respect to design variables. A cumulative confidence level is defined to quantify the accuracy of reliability estimation using MCS based on the Kriging models. To improve the efficiency of proposed approach, a maximum confidence enhancement based sequential sampling scheme is developed to update the Kriging models based on the maximum improvement of the defined cumulative confidence level, in which a sample that produces the largest improvement of the cumulative confidence level is selected to update the surrogate models. Moreover, a new design sensitivity estimation approach based upon constructed Kriging models is developed to estimate the reliability sensitivity information with respect to design variables without incurring any extra function evaluations. This enables to compute smooth sensitivity values and thus greatly enhances the efficiency and robustness of the design optimization process. Two case studies are used to demonstrate the proposed methodology.",2010
"Simulating Stochastic Diffusions by Quantum Walks","Stochastic differential equation (SDE) and Fokker-Planck equation (FPE) are two general approaches to describe the stochastic drift-diffusion processes. Solving SDEs relies on the Monte Carlo samplings of individual system trajectory, whereas FPEs describe the time evolution of overall distributions via path integral alike methods. The large state space and required small step size are the major challenges of computational efficiency in solving FPE numerically. In this paper, a generic continuous-time quantum walk formulation is developed to simulate stochastic diffusion processes. Stochastic diffusion in one-dimensional state space is modeled as the dynamics of an imaginary-time quantum system. The proposed quantum computational approach also drastically accelerates the path integrals with large step sizes. The new approach is compared with the traditional path integral method and the Monte Carlo trajectory sampling.",2010
"First-Order Reliability Analysis of Vehicle Safety in Highway Horizontal Curves","This study presents a reliability analysis of vehicle sideslip and rollover in highway horizontal curves, mainly focusing on exit ramps and interchanges. To accurately describe failure modes of a ground vehicle, analytic models for sideslip and rollover are derived considering nonlinear characteristics of vehicle behavior using the commercial software, TruckSim",2010
"Sampling-Based Approach for Design Optimization in the Presence of Interval Variables","This paper proposes a methodology for sampling-based design optimization in the presence of interval variables. Assuming that an accurate surrogate model is available, the proposed method first searches the worst combination of interval variables for constraints when only interval variables are present or for probabilistic constraints when both interval and random variables are present. Due to the fact that the worst combination of interval variables for probability of failure does not always coincide with that for a performance function, the proposed method directly uses the probability of failure to obtain the worst combination of interval variables when both interval and random variables are present. To calculate sensitivities of constraints and probabilistic constraints with respect to interval variables by the sampling-based method, the behavior of interval variables at the worst case is defined by utilizing the Dirac delta function. Then, Monte Carlo simulation is applied to calculate constraints and probabilistic constraints with the worst combination of interval variables, and their sensitivities. The important merit of the proposed method is that it does not require gradients of performance functions and transformation from X-space to U-space for reliability analysis after the worst combination of interval variables is obtained, thus there is no approximation or restriction in calculating the sensitivities of constraints or probabilistic constraints. Numerical results indicate that the proposed method can search the worst case probability of failure with both efficiency and accuracy and that it can perform design optimization with mixture of random and interval variables by utilizing the worst case probability of failure search.",2010
"Stochastic Kriging for Random Simulation Metamodeling With Finite Sampling","As a metamodeling method, Kriging has been intensively developed for deterministic design in the past few decades. However, Kriging is not able to deal with the uncertainty of many engineering processes. By incorporating the uncertainty of data, Stochastic Kriging methods has been developed to analyze and predict random simulation results, but the results cannot fit the problem with uncertainty well. In this paper, deterministic Kriging are extended to stochastic space theoretically, where a novel form of Stochastic Kriging that fully considers the intrinsic uncertainty of data and number of replications is proposed on the basis of finite inputs. It formulates a more reasonable optimization problem via a stochastic process, and then derives the spatial correlation models underlying a random simulation. The obtained results are more general than Kriging, which can fit well with many uncertainty-based problems. Three examples will illustrate the method’s application through comparison with the existing methods: the novel method shows that the results are much closer to reality.",2010
"Finite Element Modeling and Analysis of Orthotropic Butt Welds","Butt welds with orthotropic behavior are widely applied in mechanical and structural designs. Since welds cannot always be perfect in practice, it is important to understand the weld’s stress behavior under different imperfect geometries. In this paper research has been performed to investigate the relationship between stress intensity factors and change of geometry of orthotropic butt welds. Finite element methods were applied to simulate weld geometries. The simulation was performed using ANSYS software assuming two beams are welded together with a discontinuity at the bottom of the weld. The combined beams and the butt weld are then considered to be one piece of glued structure. The discontinuity in the structure is used to model a crack and lack of weld penetration. By changing three important factors of the weld geometry under uniform axial static loads, the trend of stress intensity factor behavior versus change of geometry has been investigated. Both single and double sided butt welds were considered in this paper. The results of this investigation will be a helpful tool for design engineers in deciding the best weld geometry in applications.",2010
"Theoretical and Experimental Characterization of the 34.6 2D Lattice Material","In this paper, the effective mechanical properties of the 34 .6 lattice material are characterized theoretically and experimentally. The characterized properties include the stiffness and the strength of the material. A detailed description of the design procedure of the test specimens is presented. Three quasi-static tests are performed, namely, uniaxial tension, uniaxial compression and pure shear. The comparison of the experimental data to the theoretical results shows that the former are in good agreement with the latter. The maximum error obtained of 13% is acceptable according to the literature on experimental studies of cellular solids.",2010
"A Viscoelastic Based Mechanism for Improving Spring-In Angle Predictions in Compression Molded Thermoplastic Matrix Composites","In this paper, dimensional distortion during the compression molding of thermoplastic matrix composites, typically described as spring-in or spring forward, is investigated through a finite element model. Spring-in is the reduction of the enclosed angle of two surfaces on the final component shape with respect to the original mold shape. Spring-in of thermoplastic matrix composites has typically been attributed to the difference in the thermal expansion of in-plane and through thickness directions of the composite. However, using this mechanism alone during modeling has not shown complete agreement with the experimental data. A new meso-level mechanism based on the viscoelasticity effect of the thermoplastic matrix is proposed. With this mechanism, the predicted spring-in angle can be in good agreement with experiments.",2010
"The Effect of Carbon Nanotubes on the Natural Frequencies of Microcantilever Beams","In this study, the natural frequencies and mode shapes of carbon nanotube (CNT) reinforced polymer composite microcantilever beams are investigated by means of a micromechanical model and the three-dimensional finite element analysis. Microcantilever beams are made of Poly vinyl chloride (PVC) and reinforced with multi-wall carbon nanotubes (MWCNTs). MWCNTs can be distributed along the length/width/thickness of the nanocomposite beam. To validate the accuracy and effectiveness of the model, a direct comparison of results is made with an analytical solution for a test case. Next, various material types of the nanocomposite microcantilever beam are introduced and the effect of different distribution patterns and the weight-percents (wt%) of MWCNTs on the first six natural frequencies and mode shapes is found.",2010
"Experimental Damage Characterization of Hexagonal Honeycombs Subjected to In-Plane Shear Loading","Experimental study on the damage of hexagonal honeycombs under in–plane shear loading does not appear to be available in the literature. In this paper, shear damage behaviors of five different hexagonal mesostructures are investigated with rapid prototyped polycarbonate (PC) honeycomb coupon samples and proper design of a fixture for shear loading. Effective shear stress-strain curves of PC honeycomb coupons are generated for each shear test and the corresponding local cell wall failure is investigated. Two different failure modes of PC honeycombs were observed primarily depending on the cell wall thickness: The PC honeycombs having a lower cell wall thickness induce the plastic post buckling, resulting in preventing propagation of initial cracks through the cell wall end up with higher plastic load bearing. On the other hand, the failure mode of the honeycombs having a high cell wall thickness is the cell wall fracture by crack propagation through wall without severe buckling.",2010
"Design of Chiral Honeycomb Meso-Structures for High Shear Flexure","Chiral honeycombs are auxetic cellular structures that exhibits negative Poison’s ratio. Chiral honeycombs are structures arranged in an array of cylinders connected by ligaments. Four different configurations of these geometries with 4- and 6- ligaments attached are investigated for its use in shear layer of non-pneumatic wheel. The objective of the study is to find an ideal geometry for the shear layer while meeting its requirements of shear properties (about 6.5 MPa effective shear modulus and 0.15 maximum effective shear strain) with polycarbonate as base material. Finite Element (FE) based numerical tests are carried out and optimum chiral meso-structures are found for the target shear properties. Parametric studies on geometries are also conducted to find the effect of geometries on the target properties. The effect of cell wall thickness is studied and the optimum thickness is suggested to meet the target requirements. Effect of direction of shear loading has been studied on each different configuration in order to minimize the effect of direction of loading.",2010
"A Study of the Effect of Geometry Changes on the Structural Stiffness of a Composite D-Spar","The paper examines the impact of varying two geometric cross-section parameters of an advance composite D-spar on its structural stiffness. For a given blade topology, the orientation of the D-spar web with respect to the beam axis and the distance of the D-spar web from the leading edge of the blade have been selected here as the variables of study, as they govern the elastic properties of the composite cross-section. A code has been developed to calculate the matrix terms of the Euler-Bernoulli cross-sectional stiffness utilizing the closed form expressions of the structural properties formulated by assuming both Thin-Walled composite Beam theory (TWB) and Classical Laminate Theory. The code has been validated through the Variational Asymptotic Beam Sectional analysis (VABS) for the cross-sectional stiffness matrix. Two cases have been studied for a quasi-isotropic laminate D-spar. The first is for a symmetric airfoil, whereas the second is for an unsymmetrical airfoil. The variation of the stiffness parameters for the quasi-isotropic D-spar including the coupling parameters has been visualized into parametric maps. The paper also examines the impact that these geometric variables have on the stiffness-to-mass ratio to show that along with the ply orientations they play a major role in the aeroelastic tailoring and structural optimization of a composite blade.",2010
"Low-Velocity Impact Behavior of Aluminum-Fiber/Epoxy Laminates: A Comprehensive Experimental Study","Considering many potential applications of fiber reinforced metal laminates (FMLs) in sensitive structures, it is necessary to understand their mechanical behavior under impact loads. In this study, low velocity impact tests based on ASTM D7136 have been conducted on FMLs made of 1050 aluminum sheets and various types of fiber reinforced polymer (FRP) layers; namely E-Glass, Kevlar 49, and carbon T300 plain woven in the epoxy resin. Projectile energy, fiber type and the number of successive impacts are selected among important parameters that can affect the performance of FMLs. In particular, the effects of these parameters on the absorbed energy, contact force, front and rear face damage areas, central deflection and permanent deformation of FMLs have been investigated. For determining the damage area and central deflection of the specimens, an image processing method is adapted.",2010
"Safety of Spur Gear Design Under Non-Ideal Conditions With Uncertainty","The current practice of gear design is based on the Lewis bending and Hertzian contact models. The former provides the maximum stress on the gear base, while the latter calculates the contact pressure at the contact point between the gear and pinion. Both calculations are obtained at the reference configuration with ideal conditions; i.e., no tolerances and clearances. The first purpose of this paper is to compare these two analytical models with the numerical results, in particular, using finite element analysis. It turns out that the estimations from the two analytical equations are closely matched with those of the numerical analysis. The numerical analysis also yields the variation of contact pressures and bending stresses according to the change in the relative position between gear and pinion. It has been shown that both the maximum bending stress and contact pressure occur at non-reference configurations, which should be considered in the calculation of a safety factor. In reality, the pinion-gear assembly is under the tolerance of each part and clearance between the parts. The second purpose of this report is to estimate the effect of these uncertain parameters on the maximum bending stress and contact pressure. For the case of the selected gear-pinion assembly, it turns out that due to a 0.57% increase of clearance, the maximum bending stress is increased by 4.4%. Due to a 0.57% increase of clearance, the maximum contact pressure is increased by 17.9%.",2010
"Subfunctions as Parts of Functions: Some Formal Problems","In this paper a proof is presented that shows that the relation between technical functions and their subfunctions in functional descriptions of products can formally not be taken as a relation of parthood. Technical functions of two specific classes are modelled as well as their composition. In this modelling functions are taken as transformations of tokens of flows of energy, material and signals, which makes them proper instances of functions on many engineering accounts of functions. Then it is proved that the relations between the considered functions and their subfunctions do not in general meet the basic postulates of mereology, the theory of parthood relations. The ramification of this proof is that in engineering ontologies the relation between subfunctions and functions should not be described as a formal parthood relation.",2010
"Development and Application of a Patent-Based Design Around Process","This research proposes a patent-based design process by systematically integrating patent information, the rules of patent infringement judgment, strategies of designing around patents, and innovation design methodologies. The purpose of the process is to systematically generate new design concepts that are local variations of one of the concerned patents but does not infringe with existing patents. The basic idea is to consider patent infringement before engineering design concepts are actually generated. In this process, first the designer conducts standard patent analysis to identify the related patents to be designed around. Each patent is then symbolized by a “design matrix” converted from the technology/function matrix of the patent. A design-around algorithm is developed to generate a new design matrix that does not infringe with design matrices of existing patents. Then the new design matrix is transformed back into a real engineering design using the “contradiction matrix” in TRIZ. A computerized design-around tool based on the innovative patent-based design process is also developed.",2010
"A Port-Based Agent Approach to Guiding Concept Generation for Customizing Modular Varieties","As the description of design requirements at the earlier design stage is inaccurate and vague, it is difficult to figure out functional structure of a product and make sense product configuration. Therefore, it plays an important role to formally represent the process of design for product development in the conceptual design stage. Furthermore, port, as the location of intended interaction, is crucial to capture component concept and realize conceptual design for multi-solution generation. Agent is considered as an effective approach to collaboratively implementing design problem solving and reasoning. Combining both port and agent may be employed to generate new concepts of the product in order to customize product scheme varieties. In this paper, the product module attributes are firstly described. The objective is to implement modeling of design process for obtaining system new concepts to guide multi-solution generation. Secondly, an effective approach to decomposing design process is presented to describe the process of structure generations and product decomposition by formal representation. According to properties of modularity for product development and component connections, we can calculate the number of component connections and density of components. In addition, product module division and coupling degree analysis are conducted, and coupling degrees are calculated by considering the correspondence ratio and the cluster independence. A port-based knowledge building process is described for functional modeling. A port-agent collaborative design framework is given and describes different agent functions to help designers to obtain new design schemes. Finally, a case study is presented to describe the modeling process of conceptual design.",2010
"Requirement Change Propagation Prediction Approach: Results From an Industry Case Study","This paper presents an industry case study investigating change propagation due to requirement changes. This paper makes use of a change propagation prediction tool, ΔDSM, to identify if the propagated changes could have been identified and predicted. The study used an automation firm’s client project as the study subject. The project entailed 160 requirements, changing over the span of 15 month. Engineering change notifications were developed for each change and documented under the firm’s data management system. This study makes use of the change notifications to identify if any of the change were as a result of a previous change. The findings of this paper indicated the changes that occurred could have been predicted as the ΔDSM was able to predict affected requirements. This was identified by finding subsequent requirements in the engineering change notification documentation that the ΔDSM indicated might change.",2010
"A Graph Theory Based Method for Functional Decoupling of a Design With Complex Interaction Structure","The primary objective in design is to achieve the target value of the design’s functional requirement. In design with multiple functional requirements, one way a design fails is the inability to converge to the multiple target values in spite of iterative adjustment of the design parameters. This is symptom of a design that fails to perform in the presence of functional coupling. Functional coupling occurs when two or more functional requirements are affected by a common set of design parameters. It is particularly difficult to identify and break when it involves inter-relation loops created among large number of functional requirements, typical of a large complex system. This paper presents a structured method based on the graph theory to effectively identify and eliminate functional couplings in a design. Use of the graph theory in this context is natural by the fact that inter-relations among functional requirements and design parameters can be represented by a digraph. Each inter-relation corresponds to an arc of the digraph, and functional coupling is equivalent to a cycle in it. The proposed method consists of: 1) represent interactions among functional requirements and design parameters as a digraph, 2) construct the cycle matrix for the digraph, 3) identify those candidate sets of arcs that, if removed, will destroy all cycles in the digraph, and 4) examine engineering feasibility of the candidate solutions. Once target interactions, i.e. arcs, are determined, the design parameters responsible for those interactions are modified to implement the solution. To demonstrate the effectiveness of the proposed method, we apply it to a large complex system, the car door to body, involving 28 functional requirements and design parameters.",2010
"Mass Customization: A Review of the Paradigm Across Marketing, Engineering and Distribution Domains","Introduced nearly 25 years ago, the paradigm of mass customization (MC) has largely not lived up to its promise. Despite great strides in information technology, engineering design practice, and manufacturing production, the necessary process innovations that can produce products and systems with sufficient customization and economic efficiency have yet to be found in wide application. In this paper, the state-of-the-art in MC is explored in order to answer the question of “why not?” and to highlight areas for specific research in the MC paradigm. To establish perspective for this work, we consider MC to be a product development approach which allows for the production of goods — after a customer places an order — which minimize the tradeoff between the ideal product and the available product by fulfilling the needs and preferences of individuals functionally, emotionally and anthropologically. Results of this research were generated by reviewing 88 papers from various journals that span three domains of interest (marketing, engineering, and distribution) and explore proposed methodologies, specific information inputs and outputs, proposed metrics, and barriers toward the implementation of MC. Qualitatively, we show that the lack of MC in application is due to two factors: 1) a lack of marketing tools capable of capturing individual needs that can be mapped to the technical space; and 2) a lack of information relation mechanisms that connect the domains of marketing, engineering, and distribution. In the end it is our belief that MC is realizable and that eventually it will emerge as a dominant paradigm in the design and delivery of products and systems. However, pursuing the opportunities for research presented in this work will hopefully speed this emergence.",2010
"Automated Concept Generation Using Branched Functional Models","This paper discusses a new concept generation technique that improves upon a previous automated concept generation theory and algorithm developed by Bryant, et al. at the University of Missouri – Rolla. The previous automated concept generation algorithm utilizes the design knowledge present in a repository to produce an array of partial concept solutions. While the previous algorithm is capable of handling branched functional models, it does not efficiently remove all of the infeasible partial solutions to leave only whole concepts in the final results. A matrix-based algorithm is presented in this paper that utilizes the result from the previous concept generation algorithm and solves for complete solutions of branched concepts. The presented algorithm eliminates incomplete and infeasible concepts or components from the results and generates a set of full solutions for further analysis by a designer. The details of the algorithm are described in this paper, and a peanut-sheller example is used to illustrate the effective use of the algorithm for producing branched concept variants.",2010
"Development of Game Theoretic Protocols for Multilevel Design","The effectiveness of the use of game theory in addressing multi-objective design problems has been illustrated. For the most part, researchers have focused on design problems at single level. In this paper, we illustrate the efficacy of using game theoretic protocols to model the relationship between multidisciplinary engineering teams and facilitate decision making at multiple levels. We will illustrate the protocols in the context of an underwater vehicle with three levels that span material and geometric modeling associated with microstructure mediated design of the material and vehicle.",2010
"Exploring Automated Concept Generator Output Through Principal Component Analysis","During conceptual design it is desirable to produce many potential solutions. Recently, computational tools have emerged to help designers more fully explore possible solutions. These automated concept generators use knowledge from existing products and the desired functionality of the new design to suggest solutions. While research has shown these tools can increase the variety of solutions developed, they often provide unmanageably large sets of poorly differentiated results. This work proceeds from the hypothesis that automated concept generator output includes many permutations of a relatively few principal solution variants. A method to discover these underlying solution types from the initial concept generator output is proposed. The proposed method employs principal component analysis for variable reduction followed by cluster analysis for classification. The method is applied to the automatically generated solutions of three sample design problems. Preliminary evidence of the utility and efficiency of the proposed method is presented based upon those sample problems. Finally, a method for extending the proposed technique to much larger solution sets is discussed.",2010
"Concept Opportunity Diagrams: A Visual Modeling Method to Find Multifunctional Design Concepts","A transforming product is a system that has different functionality when physically changed or reconfigured into a different state. This increased functionality allows diverse customer needs to be met in a single product. Transforming devices have become more prevalent in recent years, as customers desire both increased capabilities and reduced complexity to reduce waste in our society. When designing a multifunctional product that transforms from one state to another, it can be difficult to conceptualize a design that does not reduce effectiveness or provide a compromise in either state. Transformational Design Theory has been developed and shows basic principles and facilitators that enable transformation to occur within a product space. An illustrative example is a chair designed to flip over to be used as a table. Flip is one of the 19 facilitators that are found in transformation design. This is also an example of expose/cover, a transformation design principle. Certain principles and facilitators are more prevalent than others in different design domains (such as tools, storage, organisms etc.). If we know the states that exist within the transformer, concept opportunity diagrams can be used to determine the opportunities for transformation within each state. When the diagrams are paired with a constituent relationship chart specific to each domain, new design concepts may be facilitated. This technique creates a cognitive process for designers where they process a series of questions when creating the concept opportunity diagram. The diagram will help them understand the unanticipated additional design space of each state. The Constituent Relationship Chart is a tool that allows them to apply their knowledge of these states to the facilitator hierarchy so that prospective facilitators can directly contribute to originally unforeseen design concepts. This paper presents this twofold process known as the Transformer Diagram Matching Method and shows the results on a fully functioning prototype of an office supply transformer. Although the proposed process is detailed, it allows the designer to find a large number of quality concepts they would not have foreseen otherwise. Our original concept generation processes produced thirty eight ideas, but this process added another thirty two ideas to the design space. The paper indicates specifically how this method can be integrated in with the standard transformational design process as well as suggests strategies for implementation within other design techniques.",2010
"Modular Product Configuration: An Automatic Tool for Eliciting Design Knowledge From Parametric CAD Models","The offer of tailored products is a key factor to satisfy specific customer needs in the current competitive market. Modular products can easily support customization in a short time. Design process, in this case, can be regarded as a configuration task where solution is achieved through the combination of modules in overall product architecture. In this scenario efficient configuration design tools are evermore important. Although many tools have been already proposed in literature, they need further investigation to be applicable in real industrial practice, because of the high efforts required to implement system and the lack of flexibility in products updating. This work describes an approach to overcome drawbacks and to introduce a product independent configuration system which can be useful in designing recurrent product modules. To manage configuration from the designer perspective, the approach is based on Configurable Virtual Prototypes (CVP). In particular, the definition of geometrical models is analyzed providing a tool for eliciting and reusing knowledge introduced by parametric template CAD models. Semantic rules are used to recognize parts parameterization and assembly mating constraints. The approach is exemplified through a case study.",2010
"Reconfigurable Products and Their Means of Reconfiguration","Reconfigurable systems are able to meet the increasingly diverse needs of consumers. A reconfigurable system is able to change its configuration repeatedly and reversibly to match the customer’s needs or the surrounding environment, allowing the system to meet multiple requirements. In this paper, a sample of reconfigurable products was studied to better understand the methods used to achieve different configurations. Four methods of reconfiguration were discovered. This expands work previously done in a parallel field with products that transform where only three methods were identified. In order to support the findings of this paper, the variations were identified and example products were presented that clearly show the need for at least one additional method of reconfiguring. A case study is also provided to illustrate the benefits of incorporating four principles, as apposed to three, into the concept generation phase of new reconfigurable product development.",2010
"Optimal Adaptable Design Considering Changes of Requirements, Configurations and Parameters in the Whole Product Life-Cycle","Adaptable design is a new design approach to create an adaptable product to replace multiple products for satisfying the different requirements in the product life-cycle. In this research, a method to identify the optimal product considering changes of requirements, configurations and parameters in the whole product life-cycle is introduced. The requirements, configurations and parameters of the adaptable product are modeled as functions of the life-cycle time parameter. The adaptable product is changed to different configurations and parameters to satisfy the different requirements in different life-cycle time periods. The evaluation measures, which are achieved from configurations and parameters, are also changed in different life-cycle time periods. The optimal product, modeled by its configurations and parameters, considering the whole product life-cycle is identified through optimization. A case study is provided to demonstrate how the introduced method can be employed for solving engineering problems.",2010
"State Transition in Reconfigurable Systems","Reconfigurable systems that maintain a high level of performance under changing operational conditions and requirements are an ongoing research challenge. Many existing systems are able to work under narrow operational conditions only. They should perform the configuration transitions from the current state to a desired one smoothly. The question is: how to find the optimal state transition trajectory that configures the system from an initial state to a desired state? We present a method to determine steps (list of actions to be taken) of state transitions for reconfigurable systems. This method makes use of graph search algorithms that solve shortest path problems and that are commonly used in routing: Dijkstra’s algorithm and the A* algorithm. The method is applied to the design of a reconfigurable printer, which has to change its configuration to achieve maximum performance (e.g. high quality of print) when operational conditions are changing (e.g. speed of printing).",2010
"An Engineering Design Strategy for Reconfigurable Products That Support Poverty Alleviation","Reconfigurable products can adapt to new and changing customer needs. One potential, high-impact, area for product reconfiguration is in the design of income-generating products for poverty alleviation. Non-reconfigurable income-generating products such as manual irrigation pumps have helped millions of people sustainably escape poverty. However, millions of other impoverished people are unwilling to invest in these relatively costly products because of the high perceived and actual financial risk involved. As a result, these individuals do not benefit from such technologies. Alternatively, when income-generating products are designed to be reconfigurable, the window of affordability can be expanded to attract more individuals, while simultaneously making the product adaptable to the changing customer needs that accompany an increased income. The method provided in this paper significantly reduces the risks associated with purchasing income-generating products while simultaneously allowing the initial purchase to serve as a foundation for future increases in income. The method presented builds on principles of multiobjective optimization and Pareto optimality, by allowing the product to move from one location on the Pareto frontier to another through the addition of modules and reconfiguration. Elements of product family design are applied as each instantiation of the reconfigurable product is considered in the overall design optimization of the product. The design of a modular irrigation pump for developing nations demonstrates the methodology.",2010
"A Framework for Choice Modeling in Usage Context-Based Design","Usage Context-Based Design (UCBD) is an area of growing interest within the design community. A framework and a step-by-step procedure for implementing consumer choice modeling in UCBD are presented in this work. To implement the proposed approach, methods for common usage identification, data collection, linking performance with usage context, and choice model estimation are developed. For data collection, a method of try-it-out choice experiments is presented. This method is necessary to account for the different choices respondents make conditional on the given usage context, which allows us to examine the influence of product design, customer profile, usage context attributes, and their interactions, on the choice process. Methods of data analysis are used to understand the collected choice data, as well as to understand clusters of similar customers and similar usage contexts. The choice modeling framework, which considers the influence of usage context on both the product performance, choice set and the consumer preferences, is presented as the key element of a quantitative usage context-based design process. In this framework, product performance is modeled as a function of both the product design and the usage context. Additionally, usage context enters into an individual customer’s utility function directly to capture its influence on product preferences. The entire process is illustrated with a case study of the design of a jigsaw.",2010
"Product Family Deployment Through Optimal Resource Allocation Under Market System","A series of products, i.e. a product family is deployed for effectively and flexibly meeting with a variety of customer’s needs under a given product platform. Since such a deployment consumes various engineering resources and simultaneously brings profits gradually over the time sequence, when and how respective modules are designed and respective products are launched to the market must be rationally planed. Further, as a nature of product families, module commonalization accelerates the deployment but infuses some overheads on features and production cost. This paper investigates such a product family deployment problem under the optimal design viewpoint. After some general discussions, a mathematical model of dynamic design decisions is conditionally developed by integrating a combinatorial optimization technique for decision of module selection on commonalization and a market system model with discrete choice analysis and for describing the compromise among sequence of product rollout, arrangement of product lineup, required engineering resource, expected profit, etc. Then, the compromise among those factors is illustrated through the case study on a simplified deployment problem of circuit boards for digital television sets. Finally, an optimal planning approach for product family deployment and accompanied resource allocation is envisioned based on the developed model and findings from the case studies.",2010
"Strategic Product Design Decisions for Uncertain Market Systems Using an Agent Based Approach","Market players, such as competing manufacturing firms and retail channels, can significantly influence the demand and profit of a new product. Existing methods in design for market systems use game theoretic models that can maximize a focal manufacturing firm’s profit with respect to product design and price variables given the Nash equilibrium of the market system. However, in the design for uncertain market systems, there is seldom equilibrium with players having fixed strategies in a given time period. In this paper, we propose an agent based approach for design for market systems that accounts for learning behaviors of the market players under uncertainty. By learning behaviors we mean that market players gradually, over a time period, learn to play with better strategies based on action-reaction behaviors of other players. We model a market system with agents representing competing manufacturers and retailers who possess learning capabilities and are able to automatically react and make decisions on the product design and pricing. The proposed approach provides strategic design and pricing decisions for a focal manufacturer in response to anticipated reactions from market players in the short and long term horizons. Our example results show that the proposed agent based approach can produce competitive strategies for a focal firm over a time period when market players react only by setting prices compared to a game theoretic approach. Furthermore, it can yield profitable product design decisions and competitive strategies when competing firms react by changing design attributes in the short term — a case for which no previous method in design for market systems has been reported.",2010
"A Method for Supporting Service Design Based on Multiple Domain Knowledge","Recently, it has become increasingly important to provide customers with highly creative services to attain differentiation from competing firms. Fulfilling customer requirements with such creative service contents is an effective way to differentiate a firm’s services from those of its competitors. However, there have been few studies that directly support the creation of new service contents, e.g., service design support using information-processing technology for knowledge by the computer. It is valid for service providers to acquire creative service design solutions that fulfill customer requirements. In this paper, we propose a support method to fulfill customer requirements for a target service by using the functions of another type of service. This method supports the acquisition of new service design solutions on the basis of the similarity with the customer requirements or functions of different services. The proposed method is verified by applying it to an existing service case.",2010
"Parallel Biogeography-Based Optimization With GPU Acceleration for Nonlinear Optimization","This paper presents a massively parallel Biogeography-based Optimization – Pattern Search (BBO-PS) algorithm with graphics hardware acceleration on bound constrained optimization problems. The objective of this study was to determine the effectiveness of using Graphics Processing Units (GPU) as a hardware platform for BBO-PS. GPU, the common graphics hardware found in modern personal computers (PC), can be used for data-parallel computing in a desktop setting. In this research, the BBO was adapted in the data-parallel GPU computing platform featuring ‘Single Instruction – Multiple Thread’ (SIMT). The global optimal search of the BBO was enhanced by the classical local Pattern Search (PS) method. The hybrid BBO-PS method was implemented in the GPU environment, and compared to a similar implementation in the common computing environment with a Central Processing Unit (CPU). Computational results indicated that GPU-accelerated SIMT-BBO-PS method was orders of magnitude faster than the corresponding CPU implementation. The main contribution of this paper was the parallelization analysis and performance analysis of the hybrid BBO-PS with GPU acceleration. The research result was significant in that it demonstrated a very promising direction for high speed optimization with desktop parallel computing on a personal computer (PC).",2010
"Constraint Importance Mode Pursuing Sampling for Continuous Global Optimization","Many engineering design problems deal with global optimization of constrained black-box problems which is usually computation-intensive. Ref. [1] proposed a Mode-Pursuing Sampling (MPS) method for global optimization based on a sampling technique which systematically generates more sample points in the neighborhood of the function mode while statistically covering the entire problem domain. In this paper, we propose a novel and more efficient sampling technique which greatly enhances the performance of the MPS method, especially in the presence of ",2010
"Design Preference Elicitation, Derivative-Free Optimization and Support Vector Machine Search","In design preference elicitation, we seek to find individuals’ design preferences, usually through an interactive process that would need only a very small number of interactions. Such a process is akin to an optimization algorithm that operates with point values of an unknown function and converges in a small number of iterations. In this paper, we assume the existence of individual preference functions and show that the elicitation task can be translated into a derivative-free optimization (DFO) problem. Different from commonly-studied DFO formulations, we restrict the outputs to binary classes discriminating sample points with higher function values from those with lower values, to capture people’s natural way of expressing preferences through comparisons. To this end, we propose a heuristic search algorithm using support vector machines (SVM) that can locate near-optimal solutions with a limited number of iterations and a small sampling size. Early experiments with test functions show reliable performance when the function is not noisy. Further, SVM search appears promising in design preference elicitation when the dimensionality of the design variable domain is relatively high.",2010
"Protocol-Based Multi-Agent Systems: Examining the Effect of Diversity, Dynamism, and Cooperation in Heuristic Optimization Approaches","Many heuristic optimization approaches have been developed to combat the ever-increasing complexity of engineering problems. In general, these approaches can be classified based on the diversity of the search strategies used, the amount of change to those search strategies during the optimization process, and the level of cooperation between the strategies. A review of the literature indicates that approaches which are simultaneously very diverse, highly dynamic, and cooperative are rare but have immense potential for finding high quality final solutions. In this work, a taxonomy of heuristic optimization approaches is introduced and used to motivate a new approach, entitled Protocol-based Multi-Agent Systems. This approach is found to produce final solutions of much higher quality when its implementation includes the use of multiple search protocols, the adaptation of those protocols during the optimization, and the cooperation between the protocols than when these characteristics are absent.",2010
"A Hybrid Biomimetic Genetic Algorithm Using a Local Fuzzy Simplex Search","This paper presents a hybrid genetic algorithm that expands upon the previously successful approach of twinkling genetic algorithm (TGA) by incorporating a highly efficient local fuzzy-simplex search within the algorithm. The TGA was in principle a bio-mimetic algorithm that introduced a controlled deviation from a typical GA method, by not requiring that every genevariable of an offspring be the result of a crossover. Instead, twinkling allowed the genetic information of the randomly chosen gene locations to be directly passed on from one parent, which was shown to increase the likelihood of survival of a successful gene value within the offspring, rather than requiring it to be blended. The twinkling genetic algorithms proved highly effective at locating exact global optimum with a competitive rate of convergence for a wide variety of benchmark problems. In this work, it is proposed to couple the TGA with a fuzzy simplex local search to increase the rate of convergence of the algorithm. The proposed algorithm is tested using common mathematical and engineering design benchmark problems. Comparison of the results of this algorithm with earlier algorithms is presented.",2010
"Computation of the Usage Contexts Coverage of a Jigsaw With CSP Techniques","In the context of the Usage Context Based Design (UCBD) of a product-service, a taxonomy of variables is suggested to setup the link between the design parameters of a product-service and the part of a set of expected usages that may be covered. This paper implements a physics-based model to provide a performance prediction for each usage context that also depends on the user skill. The physics describing the behavior and consequently the performances of a jigsaw are established. Simulating numerically the usage coverage is non trivial for two reasons: the presence of circular references in physical relations and the need to efficiently propagate value sets or domains instead of accurate values. For these two reasons, we modeled the usage coverage issue as a Constraint Satisfaction Problem and we result in the expected service performances and a value of a covered usage indicator.",2010
"Visual Analysis of User Accommodation","This study presents a novel, quantitative tool for design decision-making for products designed for human variability. Accommodation, which describes the ability of a user to interact with a device or environment in a preferred way, is a key product performance metric. Methods that offer a better understanding of accommodation of broad user populations would allow for the design of products that are more cost-effective, safer, and/or lead to greater levels of customer satisfaction. Target user populations are often characterized by measures of anthropometry, or body dimensions. A methodology is proposed that uses a visual analysis method for understanding and exploring accommodation across the variability in anthropometry of a target user population. This is achieved by assessing binary accommodation of individuals using a “virtual fit” method and examining trends in binary accommodation across the range of anthropometric variability, referred to as the “anthropometry space”. Various factors influencing accommodation, such as user preference independent of anthropometry and the quality of a design, are also discussed and are an important contribution of the work. Two demonstration studies are presented that illustrate the methodology and provide opportunity for discussion of its impact. The first study investigates the simple univariate problem of dimensionally optimizing the seat height and range of adjustability of an exercise cycle. The second study investigates the more complex problem of optimally configuring the driver package of a commercial truck.",2010
"Considering Secular and Demographic Trends in Designing for Present and Future Populations","In products designed for human variability, the anthropometry (body measurements) of the target user population constitutes a primary source of variability that must be considered in the optimization of the spatial dimensions of the product. Accommodation, which describes the ability of a user to interact with a device or environment in their preferred manner, is a key measure of its performance. Other studies have considered various methods for accounting for the variability in anthropometry in a target user population to calculate estimated accommodation, but few have explicitly considered the effects of secular trends and demographic changes over time. This paper considers these changes in the context of a case study involving truck drivers and cab geometry. The truck driver populations are used to illustrate changes in body size and shape over a 30-year period and show how they affect user acceptability of designs. Changes in the gender split of the driver population are also considered, and are shown to have a significant effect on accommodation. The work demonstrates that secular trends and demographic changes over time significantly affect accommodation, but a well designed product will be more robust to these changes.",2010
"Augmented Reality Visualization of Automobile Air Conditioning Using Optical Tracking Tools","Virtual prototyping allows us to reduce the expensive production of real prototypes to a minimum and shorten vehicle development phases. Augmented Reality (AR) visualization is a demonstrative and intuitive tool in order to overlay physical prototypes with virtual content and thus comprehend complex relationships quickly. Further, AR technologies can intensify the collaboration between specialists with different expert knowledge and support common decision making during reviews meetings. However, the existing work processes, software tools and predefined regulations do not permit the use of AR tools in all automotive areas. In this work, a prototypical AR application, based on optical tracking tools, was set up in a real automotive environment and evaluated in terms of its applicability for CFD simulation data in the passenger compartment. The examination provides valuable information about environmental conditions, requirements from end users as well as the integration in existing work processes. The results are a basis for future improvements in order to offer a seamless and automated workflow in an early state of the development process and for maintenance.",2010
"Reverse Engineering for Spotting of Sheet Metal Forming Parts","The paper elucidates how to connect forming process simulation with innovative measurement- and analysis equipment thereby taking into account the machine influences. Reverse Engineering use 3D-Scanning data of sheet metal forming dies. Following this paradigm, the models simulation relies on are refined, and spotting of forming dies is subjected to a scientific analysis. That means, that with Reverse Engineering, “extended process engineering” is verified at the real spotting procedure, the comparison of simulation- and measuring results is used to evaluate how close the investigated models are to reality, extending the optimisation algorithms used for springback compensation to die spotting, the modification of the die topology will be carried out automatically thanks to new software functions.",2010
"2-D Path Planning for Direct Laser Deposition Process","The zigzag and offset path have been the two most popular path patterns for tool movement in machining process. Different from the traditional machining processes, the quality of parts produced by the metal deposition process is much more dependent upon the choice of deposition paths. Due to the nature of the metal deposition processes, various tool path patterns not only change the efficiency but also affect the deposition height, a critical quality for metal deposition process. This paper presents the research conducted on calculating zigzag pattern to improve efficiency by minimizing the idle path. The deposition height is highly dependent on the laser scanning speed. The paper also discussed the deposition offset pattern calculation to reduce the height variation by adjusting the tool-path to achieve a constant scanning speed. The results show the improvement on both efficiency and height.",2010
"A Multi-Axis Slicing Method for Direct Laser Deposition Process","With multi-axis capability, direct laser deposition process can produce a metal part without the usage of support structures. In order to fully utilize such a capability, the paper discusses a slicing method for multi-axis metal deposition process. Using the geometry information of adjacent layers, the slicing direction and layer thickness can be changed as needed. A hierarchy structure is designed to manage the topological information which is used to determine the slicing sequence. Its usage is studied to build overhang type structure. With such a character, some overhang features such as holes, can be deposited directly to save the required machining operation and material cost, which improves the efficiency of the metal deposition process. Combined with direct 3D layer deposition technique, the multi-axis slicing method is implemented.",2010
"Basic Study of Autologous-Bone-Replaceable Artificial Bone Fabrication With Porosity Distribution Using Electrolysis","Recently, the use of bioresorbable materials (e.g., β-tricalcium phosphate (β-TCP)) has enabled the development of autologous-bone-replaceable artificial bones that are degraded and resorbed, i.e., replaced with autologous bone, when placed inside the human body for a sufficiently long duration. Although such autologous-bone replaceability requires high porosity of the artificial bone to promote the ingression of blood vessels and cells, the high porosity reduces the mechanical strength, which leads to disadvantages such as possible fracture after bone substitution surgery. One solution to this problem is to optimally arrange low-porosity portions for mechanical strength and high-porosity portions for autologous-bone replaceability in solid artificial bones. Commercially available artificial bones typically have fixed shapes such as a rectangular parallelepiped or cylinder. The use of recent solid freeform fabrication technologies, however, has enabled solid artificial bones with various shapes to be customized for individual medical cases. In this paper, the authors propose a solid freeform fabrication method for autologous-bone-replaceable artificial bones with a porosity distribution. A β-TCP porous artificial bone can be fabricated by placing a slurry consisting of β-TCP powder, water, a peptization reagent and a frother in a mold, drying it to form a solid shape and then sintering it. This β-TCP slurry contains ammonium polyacrylate as the peptization reagent, which is an electrolyte, and ammonia, hydrogen and oxygen gases are produced from its electrolysis. The authors conceived the idea of controlling the foaming of the β-TCP slurry by electrolysis, and of designing and implementing a fabrication system consisting of a fine nozzle with a microscrew for extruding β-TCP slurry as a filament and electrodes for controlling the electrolysis of the slurry. Using this system, we can fabricate a solid shape by drawing two-dimensional sections with the slurry filament and stacking each section, and at the same time vary the porosity by controlling the electric current applied for the electrolysis of the slurry. Using the experimental system, three β-TCP porous samples (approximately 18mm × 18mm × 9mm) of high (71.8%), medium (59.5%) and low (54.6%) porosity are successfully fabricated by applying electric currents of 20mA, 10mA and 0mA, respectively. Then a β-TCP porous sample (approximately 40mm × 10mm × 10mm) with a gradient porosity distribution (from 72.3% to 56.1%) is successfully fabricated by varying the electric current from 0mA to 20mA in a continuous fabrication process. From these results, the authors confirm the efficacy and potential of the proposed approach.",2010
"Additive Manufacturing Based on Multiple Calibrated Projectors and Its Mask Image Planning","Additive manufacturing (AM) processes based on mask image projection such as ",2010
"The Automated Planning for Fixture Location Based on Process Requirement","This research carries out a systematic investigation on the automation of fixture location planning. A novel classification and representation method of constrained degrees of freedom (DOF) is proposed. Constrained DOFs are classified into four types: linear translation DOF, linear rotation DOF, planar translation DOF and planar rotation DOF, and they are represented in the form of vector. On the basis of the classification and representation, constrained DOFs are automatically obtained from process requirement. By analyzing datum constraint DOF capability and datum location ability, the operation rules of constrained DOF: union, decomposition and coordinate transformation, are established. The approach to the evaluation of datum location ability and the judgment of datum conflict is explored, and the solution of datum conflict and datum location problem is presented. Eventually, the rules of location point layout are formalized.",2010
"Robustness Analysis of Airfoil Performance","We demonstrate a technique to evaluate the aerodynamic robustness of a given blade profile which it is exposed to stochastic geometrical variation. The technique is based on random fields, with geometrical deviations continuously defined over the entire structure, with a prescribed statistical distribution function and a given correlation between these deviations. Control points are defined on the blade surface to model the blade geometry disturbances. At each control point a stochastic deviation is defined, which acts in the normal direction of the blade. By modeling disturbances in the normal direction instead of in the separate Cartesian directions, we automatically reduce the number of stochastic variables by a factor two. The perturbation variables are transformed via Karhunen-Loève eigenvalue decomposition, giving stochastically independent variables. The robustness is finally estimated by a Monte Carlo simulation, where computational fluid dynamic simulations are performed to evaluate the resulting change in blade performance for given geometrical perturbations.",2010
"Symmetry Plane Detection for 3D CAD Volumes","Symmetry properties of components have many applications during a product development process, including shape transformations for modification purposes, Finite Element Analysis (FEA), model retrieval, etc. This paper presents an algorithm to generate 3D model symmetry planes using the B-Rep model of CAD volumes. In the framework of CAD software, 3D models are described as B-Rep volume models. Design processes of volume models strongly rely on extrusion and revolution primitives from sketches containing essentially straight line segments and circular arcs. Hence, the boundary surfaces considered are planes, cylinders, cones, tori and spheres. The object boundary is effectively processed as an infinite set of points. Global symmetry properties of faces are derived to initiate the global symmetry planes of the object. To this end, the intersection curves between two adjacent faces are used to characterize possible global symmetry planes of the object. Then, the algorithm starts analyzing the symmetry properties of couple of faces. Subsequently, the candidate symmetry planes set up contains all the possible global symmetry planes. Finally, the symmetry properties of neighboring faces help determining robustly the global symmetry planes, whether there is a finite number or an infinite number.",2010
"Collision-Free Locating of Mobile Cranes in 3D Lifting System","Locating cranes is critical toward safely lifting in construction industry. In this paper, based on overlapping work envelopes, we present optimized solutions for collision-free locating of mobile cranes. Our solution first determines the feasible location areas of the cranes, which are discretized with regular grids. We then incorporate a collision detection method, through which the locations with potential collision are eliminated. With the objective of minimizing the weight sum about safety, a smart search is further performed to identify the most suitable locations. Our solution has been implemented in the intelligent computation module for 3D lifting system developed by our group. Its feasibility and effectiveness has also been demonstrated through a concrete case study.",2010
"A Fast Grid Deformation Algorithm for Aerodynamic Shape Optimization","A mesh movement algorithm suitable for aerodynamic design optimization problems is presented. It involves B-spline surface construction, projection and evaluation on B-spline faces for the surface mesh movement, as well as inverse-distance and 2D/3D TFI interpolations for the volume mesh deformation. The algorithm is fast and exhibits an excellent parallel efficiency. It is used to deform the surface and volume mesh of an ONERA-M6 wing undergoing several planform changes. The quality of the deformed mesh is preserved as long as the difference between the initial surface mesh and the B-spline surface model is small. A good agreement reported between the flow simulation results on the deformed mesh and those obtained on initial fixed mesh.",2010
"Fundamental Concepts for Collaboratively Obtaining Optimum Product Designs","Fundamental concepts for obtaining optimum product designs from higher view points are presented. These fundamental concepts are: (1) concepts that aim to achieve designs that are in harmony with natural and human environments; (2) collaborations based on mutuality, where the collaborating individuals are free from inhibiting hierarchies; and (3) methodologies that enable discovery of effective solutions by examining the deepest, most fundamental levels of design problems. Product designs that minimize stress on natural environments and maximize benefit to people are increasingly important, given limited natural resources and an increasing world population. The achievement of such product designs generally requires collaborative scenarios based on mutuality and equality of the participants, and the implementation of characteristics-based hierarchical optimization methodologies. Practical methodologies based on these fundamental concepts are discussed here and examples are provided.",2010
"Investigation of Design Tools as Complexity Management Techniques","Design tools which appear to manage complexity through their inherent behavior do not appear to have been developed specifically for complexity management. This research explores how complexity is managed within the design process through: the generation of complexity within the design process (sources), the techniques which were used to manage complexity (approaches), and the examination of design tools with respect to complexity. Mappings are developed between the sources, the approaches, and the tools with respect to phases of design. The mappings are propagated through these distinct, yet adjacent domains in order to study how the tools might be able to be used to manage complexity sources found in different stages of the design process. As expected, the highest value for each design tool is found in the stage of design in which the tool is traditionally been used. However, there are secondary ratings which suggest that design tools can be used in other stages of the design process to manage specific aspects of complexity.",2010
"Bayesian Network Classifiers for Set-Based Collaborative Design","Complex design problems are typically decomposed into smaller design problems that are solved by domain-specific experts who must then coordinate their solutions into a satisfactory system-wide solution. In set-based collaborative design, collaborating engineers coordinate themselves by communicating multiple design alternatives at each step of the design process. The goal in set-based collaborative design is to spend additional resources exploring multiple options in the early stages of the design process, in exchange for less iteration in the latter stages, when iterative rework tends to be most expensive. Several methods have been proposed for representing sets of designs, including intervals, surrogate models, fuzzy membership functions, and probability distributions. In this paper, we introduce the use of Bayesian networks for capturing sets of promising designs, thereby classifying the design space into satisfactory and unsatisfactory regions. The method is compared to intervals in terms of its capacity to accurately classify satisfactory design regions as a function of the number of available data points. A simplified, multilevel design problem for an unmanned aerial vehicle is presented as the motivating example.",2010
"Functional Decomposition of the Clustering Approach for Matrix-Based Structuring","In engineering design, matrices have been commonly used to capture dependency relationships for structure-related problems (e.g., product architecture, process workflow, and team organization). In this context, structuring is considered a group formation process that clusters the design entities and identifies the interactions among the formed groups. To support matrix-based design structuring, this paper proposes a clustering approach that has three phases in the working procedure. Firstly, the coupling analysis is used to assess the coupling strength of any two entities according to the application context. Secondly, the sorting analysis is used to organize the matrix’s rows and columns by bringing the highly coupled entities close to each other, thus yielding a sorted matrix. Thirdly, the partitioning analysis is applied to form a structured matrix that identifies the groups of entities and their interactions based on some structural criteria (e.g., number of groups, limits on group sizes, etc). The proposed clustering method has been applied to four engineering examples to demonstrate its flexibility and adaptability in tackling different design structuring problems.",2010
"Examining Interactions Between Solution Architecture and Designer Mistakes","When designing complex systems, it is often the case that a design process is subjected to a variety of unexpected inputs, interruptions, and changes. These disturbances can create unintended consequences including changes to the design process architecture, the planned design responsibilities, or the design objectives and requirements. In this paper a specific type of design disturbance, mistakes, is investigated. The impact of mistakes on the convergence time of a distributed multi-subsystem optimization problem is studied for several solution process architectures. A five subsystem case study is used to help understand the ability of certain architectures to handle the impact of the mistakes. These observations have led to the hypothesis that selecting distributed design architectures that minimize the number of iterations to propagate mistakes can significantly reduce their impact. It is also observed that design architectures that converge quickly tend to have these same error damping properties. Considering these observations when selecting distributed design architectures can passively reduce the impact of mistakes.",2010
"Support Vector Regression Modeling for Design and Analysis of Mechanical Snubbing","This paper discusses the application of Support Vector Regression (SVR) for modeling the non-linear and hysteretic behavior exhibited by mechanical snubbing systems. Though the discussion in this paper is limited to the application of SVR to snubbing in elastomeric isolators, the approach is generic and can be applied to other dynamic systems or to systems exhibiting hysteretic behavior. A theoretical model that represents the coupled dynamics of an isolation system with the corresponding snubbing system for a single degree-of-freedom system is proposed. The theoretical model is experimentally validated and is subsequently used to build a metamodel using SVR. The results of the metamodel are compared to the theoretical model for a simulation example and are found to be comparable, thereby reducing the computational time for the design and analysis of the snubbing system by orders of magnitude. The SVR based metamodel can, therefore, be used to substitute the computationally intense theoretical model for performing design iterations and design optimization of the snubbing system, significantly reducing model complexity as well as computational time during the design cycle.",2010
"A Simulation-Based Robust Concept Exploration Method","In early stages of the engineering design process it is necessary to explore the design space to find a feasible range or point that satisfies the design requirements. When robustness of the system is among the requirements, the Robust Concept Exploration Method (RCEM) can be used. In RCEM a metamodel such as a global response surface of the entire design space is used. Based on this surrogate model the robustness of the system is evaluated. In nonlinear or multimodal design spaces a very detailed metamodel such as a very high order response surface might be required to reflect accurately the characteristics of the model. For large design spaces this is computationally very expensive. In this paper, using the Probabilistic Collocation Method (PCM) for generating local response models at the points of interest, a Simulation-Based RCEM is proposed as a very efficient and flexible robust concept exploration method. We believe that using the PCM with other design exploration methods would be equally effective.",2010
"Making the Most Out of Surrogate Models: Tricks of the Trade","Design analysis and optimization based on high-fidelity computer experiments is commonly expensive. Surrogate modeling is often the tool of choice for reducing the computational burden. However, even after years of intensive research, surrogate modeling still involves a struggle to achieve maximum accuracy within limited resources. This work summarizes advanced and yet simple statistical tools that help. We focus on four techniques with increasing popularity in the design automation community: (i) screening and variable reduction in both the input and the output spaces, (ii) simultaneous use of multiple surrogates, (iii) sequential sampling and optimization, and (iv) conservative estimators.",2010
"Turning Black-Box Into White Functions","Modeling of h igh dimensional e xpensive b lack-box (HEB) functions is challenging. A recently developed method, radial basis function-based high dimensional model representation (RBF-HDMR), has been found promising. This work extends RBF-HDMR to enhance its modeling capability beyond the current second order form and “uncover” black-box functions so that not only a more accurate metamodel is obtained, but also key information of the function can be gained and thus the black-box function can be turned “white.” The key information that can be gained includes 1) functional form, 2) (non)linearity with respect to each variable, 3) variable correlations. The resultant model can be used for applications such as sensitivity analysis, visualization, and optimization. The RBF-HDMR exploration is based on identifying the existence of certain variable correlations through derived theorems. The adaptive process of exploration and modeling reveals the black-box functions till all significant variable correlations are found. The black-box functional form is then represented by a structure matrix that can manifest all orders of correlated behavior of variables. The proposed approach is tested with theoretical and practical examples. The test result demonstrates the effectiveness and efficiency of the proposed approach.",2010
"Improving Multi-Response Metamodels With Upper/Lower Bound Information Using Multi-Stage, Non-Stationary Covariance Functions","Metamodels have been proposed in the literature to reduce the time and resources devoted to design space exploration, to learn about design trade-offs, and to find the best solution to the design problem in the context of simulation-based design and optimization. In previous work in engineering design based on multiple performance criteria, we have proposed the use of Multi-response Bayesian Surrogate Models (MR-BSM) to model several response variables simultaneously, instead of modeling them independently. By doing so, it is expected that the correlation among the response variables can be used to achieve better models with smaller data sets. In this work, we extend the capabilities of MR-BSM by developing a multistage formulation with non-stationary covariance functions. This formulation for multi-response metamodeling in successive stages of experimental design, data acquisition and model fitting, enables the integration of different sources of information about system responses, with different levels of accuracy, into a single, global model of the system. The feasibility of the proposed formulation is demonstrated with an example in which two test functions are jointly approximated in two stages. In addition, we demonstrate the potential of the methodology to take advantage of a priori information, expressed as upper and lower bounds on the responses, to improve the accuracy of the metamodels. Results show that the use of bound information can result in order-of-magnitude improvements in metamodel accuracy.",2010
"A MINLP Model for Global Optimization of Plug-In Hybrid Vehicle Design and Allocation to Minimize Life Cycle Greenhouse Gas Emissions","Plug-in hybrid electric vehicles (PHEVs) have potential to reduce greenhouse gas (GHG) emissions in the U.S. light-duty vehicle fleet. GHG emissions from PHEVs and other vehicles depend on both vehicle design and driver behavior. We pose a twice-differentiable, factorable mixed-integer nonlinear programming model utilizing vehicle physics simulation, battery degradation data, and U.S. driving data to determine optimal vehicle design and allocation for minimizing lifecycle greenhouse gas (GHG) emissions. The resulting nonconvex optimization problem is solved using a convexification-based branch-and-reduce algorithm, which achieves global solutions. In contrast, a randomized multistart approach with local search algorithms finds global solutions in 59% of trials for the two-vehicle case and 18% of trials for the three-vehicle case. Results indicate that minimum GHG emissions is achieved with a mix of PHEVs sized for around 35 miles of electric travel. Larger battery packs allow longer travel on electric power, but additional battery production and weight result in higher GHG emissions, unless significant grid decarbonization is achieved. PHEVs offer a nearly 50% reduction in life cycle GHG emissions relative to equivalent conventional vehicles and about 5% improvement over ordinary hybrid electric vehicles. Optimal allocation of different vehicles to different drivers turns out to be of second order importance for minimizing net life cycle GHGs.",2010
"Hybrid Power/Energy Generation System Design Through Multistage Design Optimization Problem With Complementarity Constraints","The optimal design of hybrid power generation systems (HPGS) can significantly improve the economical and technical performance of power supply. However, the discrete-time simulation with logical disjunctions involved in HPGS design usually leads to a nonsmooth optimization model, to which well established techniques for smooth nonlinear optimization could not be directly applied. This paper proposes a multistage design optimization problem with complementarity constraints approach for HPGS design, which introduces a complementarity formulation of the nonsmooth logical disjunction, as well as a multistage decomposition framework, to ensure a fast local solution. A numerical study of a stand-alone hybrid photovoltaic (PV)/wind power generation system is presented to demonstrate the effectiveness of the proposed approach.",2010
"Design and Analysis of Hybrid Guideway Heating System for Morgantown Personal Rapid Transit","The Morgantown Personal Rapid Transit (M-PRT) system is a comfortable conveyance for travel in Morgantown, WV. One of its operating concerns is the increasing cost of heat to the guideway during winter. As the vehicles cannot run safely during snow, the system includes a guideway heating system to melt the ice from the guideway. To reduce the use of expensive natural gas, an interest has been expressed to define a hybrid heating system using an alternate fuel supply. Solid Oxide Fuel Cell (SOFC) was incorporated in the hybrid heating system. This hybrid heating system was designed, and then a detailed analysis was performed to ascertain the performance parameters like heat produced, thermal efficiency, cost of the system and the emissions involved. This high temperature fuel cell releases large amounts of usable heat in the form of exhaust gases. The exhaust gases are deprived of any undesired emissions that pollute the atmosphere. A USDOE EPSCoR WV State Implementation Award conducted by Advance Power Electricity Research Center (APERC) at West Virginia University provided support for conducting this research.",2010
"Design Optimization of Reverse Osmosis Water Desalination Systems via Genetic Algorithms","This paper explores the application of genetic algorithms (GA) for optimal design of reverse osmosis (RO) water desalination systems. While RO desalination is among the most cost and energy efficient methods for water desalination, optimal design of such systems is rarely an easy task. In these systems, salty water is made to flow at high pressure through vessels that contain semi-permeable membrane modules. The membranes can allow water to flow through, but prohibit the passage of salt ions. When the pressure is sufficiently high, water molecules will flow through the membranes leaving the salt ions behind and are collected in a fresh water stream. Typical system design variables for RO systems include the number and layout of the vessels and membrane modules, as well as the operating pressure and flow rate. This paper explores models for single and two-stage RO pressure vessel configurations. The number and layout of the vessels and membrane modules are regarded as discrete variables, while the operating pressures and flow rate are regarded as continuous variables. GA is applied to optimize the models for minimum overall cost of unit produced fresh water. Case studies are considered for four different water salinity concentration levels. In each of the studies, three different types of crossover are explored in the GA. While all the studied crossover types yielded satisfactory results, the crossover types that attempt to exploit design variable continuity performed slightly better, even for the discrete variables of this problem.",2010
"Studies on the Design of Reverse Osmosis Water Desalination Systems for Cost and Energy Efficiency","This paper explores optimal design of reverse osmosis (RO) systems for water desalination. In these systems, salty water flows at high pressure through vessels containing semi-permeable membrane modules. The membranes can allow water to flow through, but prohibit the passage of salt ions. When the pressure is sufficiently high, water molecules will flow through the membranes leaving the salt ions behind, and are collected in a fresh water stream. Typical system design variables include the number and layout of the vessels and membrane modules, as well as the operating pressure and flow rate. This paper presents models for single and two-stage pressure vessel configurations. The models are used to explore the various design scenarios in order to minimize the cost and energy required per unit volume of produced fresh water. Multi-objective genetic algorithm (GA) is used to generate the Pareto-optimal design scenarios for the systems. Case studies are considered for four different water salinity concentration levels. Results of the studies indicate that even though the energy required to drive the RO system is a major contributor to the cost of fresh water production, there exists a tradeoff between minimum energy and minimum cost. An additional parametric study on the unit cost of energy is performed in order to explore future trends. The parametric study demonstrates how an increase in the unit cost of energy may shift the minimum cost designs to shift to more energy-efficient design scenarios.",2010
"An Extended Pattern Search Approach to Wind Farm Layout Optimization","An extended pattern search approach is presented for optimizing the placement of wind turbines on a wind farm. The algorithm will develop a two-dimensional layout for a given number of turbines, employing an objective function that minimizes costs while maximizing the total power production of the farm. The farm cost is developed using an established simplified model that is a function of the number of turbines. The power development of the farm is estimated using an established simplified wake model, which accounts for the aerodynamic effects of turbine blades on downstream wind speed, to which the power output is directly proportional. The interaction of the turbulent wakes developed by turbines in close proximity largely determines the power capability of the farm. As pattern search algorithms are deterministic, multiple extensions are presented to aid escaping local optima by infusing stochastic characteristics into the algorithm. This stochasticity improves the algorithm’s performance, yielding better results than purely deterministic search methods. Three test cases are presented: a) constant, unidirectional wind, b) constant, multidirectional wind, and c) varying, multidirectional wind. Resulting layouts developed by this extended pattern search algorithm develop more power than previously explored algorithms with the same evaluation models and objective functions. In addition, the algorithm’s layouts motivate a heuristic that yields the best layouts found to date.",2010
"Design of an Extended-Range Electric Vehicle for the EcoCAR Challenge","The EcoCAR Challenge team at The Ohio State University has designed a range-extending electric vehicle capable of 40 miles all-electric range via a 22 kWh lithium-ion battery pack, with range extension and limited parallel operation supplied by a 1.8 L dedicated E85 engine. This vehicle is designed to drastically reduce fuel consumption, with an estimated fuel economy of 89 miles per gallon gasoline equivalent (mpgge), while meeting Tier II Bin 5 emissions standards. This paper documents the team’s control system development effort, starting with the vehicle architecture selection and specifying the powertrain configuration, explaining a detailed control system development process, summarizing the selected control hardware architecture at vehicle and component level, describing supervisory control algorithm design and implementation for fuel economy optimization and performance improvement, and concluding with the use of MIL and HIL techniques for system development and validation.",2010
"Plug-In Hybrid Electric Vehicle Battery Selection for Optimum Economic and Environmental Benefits Using Pareto Set Points and PSAT™","Plug-in hybrid electric vehicles (PHEVs) have the potential to reduce green house gases emissions and provide a promising alternative to conventional internal combustion engine vehicles. However, PHEVs have not been widely adopted in comparison to the conventional vehicles due to their high costs and short charging intervals. Since PHEVs rely on large storage batteries relative to the conventional vehicles, the characteristics and design issues associated with PHEV batteries play an important role in the potential adoption of PHEVs. Consumer acceptance and adoption of PHEVs mainly depends on fuel economy, operating cost, operation green house gas (GHG) emissions, power and performance, and safety among other characteristics. We compare the operational performance of PHEV20 (PHEV version sized for 20 miles of all electric range) based on fuel economy, operating cost, and greenhouse gas (GHG) emissions through Pareto set point identification approach for 15 different types of batteries, including lithium-ion, nickel metal hydride (NiMH), nickel zinc (NiZn), and lead acid batteries. It is found that two from 15 batteries dominate the rest. Among the two, a NiMH (type ess_nimh_90_72_ovonic) gives the highest fuel economy, and a lithium-ion (type ess_li_7_303) yields the lowest operating cost and GHG emissions. From comparing nine batteries that are either on or close to the Pareto frontier, one can see that lithium-ion and NiMH batteries offer better fuel economy than lead-acid batteries. Though lithium-ion batteries bear clear advantage on operating costs and GHG emissions, NiMH and lead-acid batteries show similar performances from these two aspects.",2010
"An Investigation of Sustainability, Preference, and Profitability in Design Optimization","Customer preferences for sustainable products are dependent upon the context in which the customer makes a purchase decision. This paper investigates a case study in which fifty-five percent of survey customers say they prefer recycled paper towels, but do not purchase them. These customers represent a profit opportunity for a firm. This paper explores the impact of investing capital in activating pro-environmental preferences on a firm’s profitability and greenhouse gas (GHG) emissions through a multi-objective optimization study. A product optimization is designed to include models of carbon dioxide emissions, manufacturing costs, customer preference, and technical performance. Because the optimization includes a tradeoff between recycled paper and performance, a model of customer preferences, and a market of competing products, the maximum GHG reduction occurs at less than 100% recycled paper. Also, the tradeoff between GHG reductions and profit is not dictated by the configuration of the product, but instead by its price. These results demonstrate the importance of including customer preferences with engineering performance in design optimization. Investment in the activation of pro-environmental preferences is high at all points on the Pareto optimal frontier, suggesting that further engineering design research into the activation of pro-environmental product preferences is warranted.",2010
"Response Surface Based Cost Model for Onshore Wind Farms Using Extended Radial Basis Functions","This paper develops a cost model for onshore wind farms in the U.S.. This model is then used to analyze the influence of different designs and economic parameters on the cost of a wind farm. A response surface based cost model is developed using Extended Radial Basis Functions (E-RBF). The E-RBF approach, a combination of radial and non-radial basis functions, can provide the designer with significant flexibility and freedom in the metamodeling process. The E-RBF based cost model is composed of three parts that can estimate (i) the installation cost, (ii) the annual Operation and Maintenance (O&M) cost, and (iii) the total annual cost of a wind farm. The input parameters for the E-RBF based cost model include the rotor diameter of a wind turbine, the number of wind turbines in a wind farm, the construction labor cost, the management labor cost and the technician labor cost. The accuracy of the model is favorably explored through comparison with pertinent real world data. It is found that the cost of a wind farm is appreciably sensitive to the rotor diameter and the number of wind turbines for a given desirable total power output.",2010
"Optimal Camera Path Planning for the Inspection of Printed Circuit Boards Using a Two Stepped Optimization Approach","Automated Optical Inspection (AOI) systems are rapidly replacing slow and tedious manual inspections of Printed Circuit Boards (PCBs). In an AOI system, a minicamera traverses the PCB in a pre-defined travel path, snapping shots of all the PCB components or nodes, at pre-defined locations. The images are then processed and information about the different nodes is extracted and compared against ideal standards stored in the AOI system. This way, a flawed board is detected. Minimizing both the number of images required to scan all the PCB nodes, and the path through which the camera must travel to achieve this, will minimize the image acquisition time and the traveling time, and thus the overall time of inspection. This consequently both reduces costs and increases production rate. This work breaks down this problem into two sub-problems: The first is a clustering problem; the second a travelling salesman sequencing problem. In the clustering problem, it is required to divide all the nodes of a PCB into the minimum number of clusters. The cluster size is constrained by the given dimensions of the camera’s scope or Field of Vision (FOV). These dimensions determine the dimension of the inspection windows. It is thus required to find the minimum number of inspection windows that will scan all the nodes of a PCB, and their locations. Genetic algorithms are applied in a two-step approach with special operators suited for the problem. A continuous Genetic Algorithm (GA) is applied to find the optimum inspection window locations that cover one node and as many other nodes as possible. A discrete GA is then applied to eliminate redundant inspection windows leaving the minimum number of windows that cover all nodes throughout the PCB. In the second sub-problem, an Ant Colony Optimization (ACO) method is used to find the optimum path between the selected inspection windows. The method proposed in this paper is compared against relevant published work, and it is shown to yield better results.",2010
"Constraint Management of Reduced Representation Variables in Decomposition-Based Design Optimization","In decomposition-based design optimization strategies, such as Analytical Target Cascading (ATC), it is sometimes necessary to use reduced dimensionality representations to approximate functions of large dimensionality whose values need to be exchanged among subproblems. The reduced representation variables may not be physically meaningful, and it can become challenging to constrain them properly and define the model validity region. For example, in coordination strategies like ATC, representing vector-valued coupling variables with improperly constrained reduced representation variables can lead to poor performance or convergence failure. This paper examines two approaches for constraining effectively the model validity region of reduced representation variables based on proper orthogonal decomposition: a penalty value-based heuristic and a support vector domain description. An ATC application on electric vehicle design helps to illustrate the concepts discussed.",2010
"Combined Plant and Controller Design Using Decomposition-Based Design Optimization and the Minimum Principle","An often cited motivation for using decomposition-based optimization methods to solve engineering system design problems is the ability to apply discipline-specific optimization techniques. For example, structural optimization methods have been employed within a more general system design optimization framework. We propose an extension of this principle to a new domain: control design. The simultaneous design of a physical system and its controller is addressed here using a decomposition-based approach. An optimization subproblem is defined for both the physical system (i.e., plant) design and the control system design. The plant subproblem is solved using a general optimization algorithm, while the controls subproblem is solved using a new approach based on optimal control theory. The optimal control solution, which is derived using the the Minimum Principle of Pontryagin (PMP), accounts for coupling between plant and controller design by managing additional variables and penalty terms required for system coordination. Augmented Lagrangian Coordination is used to solve the system design problem, and is demonstrated using a circuit design problem.",2010
"Optimizing the Unrestricted Placement of Turbines of Differing Rotor Diameters in a Wind Farm for Maximum Power Generation","This paper presents a new method (the Unrestricted Wind Farm Layout Optimization (UWFLO)) of arranging turbines in a wind farm to achieve maximum farm efficiency. The powers generated by individual turbines in a wind farm are dependent on each other, due to velocity deficits created by the wake effect. A standard analytical wake model has been used to account for the mutual influences of the turbines in a wind farm. A variable induction factor, dependent on the approaching wind velocity, estimates the velocity deficit across each turbine. Optimization is performed using a constrained Particle Swarm Optimization (PSO) algorithm. The model is validated against experimental data from a wind tunnel experiment on a scaled down wind farm. Reasonable agreement between the model and experimental results is obtained. A preliminary wind farm cost analysis is also performed to explore the effect of using turbines with different rotor diameters on the total power generation. The use of differing rotor diameters is observed to play an important role in improving the overall efficiency of a wind farm.",2010
"A Cutting Plane Method for Analytical Target Cascading With Augmented Lagrangian Coordination","Various decomposition and coordination methodologies for solving large-scale system design problems have been developed and studied during the past few decades. However, there is generally no guarantee that they will converge to the expected optimum design under general assumptions. Those with proven convergence often have restricted hypotheses or a prohibitive cost related to the required computational effort. Therefore there is still a need for improved, mathematically grounded, decomposition and coordination techniques that will achieve convergence while remaining robust, flexible and easy to implement. In recent years, classical Lagrangian and augmented Lagrangian methods have received renewed interest when applied to decomposed design problems. Some methods are implemented using a subgradient optimization algorithm whose performance is highly dependent on the type of dual update of the iterative process. This paper reports on the implementation of a cutting plane approach in conjunction with Lagrangian coordination and the comparison of its performance with other subgradient update methods. The method is demonstrated on design problems that are decomposable according to the analytic target cascading (ATC) scheme.",2010
"Reducible Uncertain Interval Design (RUID) by Kriging Meta-Model Assisted Multi-Objective Optimization","Optimization under uncertainty can be a difficult and computationally expensive problem driven by the need to consider the degrading effects of system variations. Sources of uncertainty that may be reducible in some fashion present a particular challenge because designers must determine how much uncertainty to accept in the final design. Many of the existing approaches for design under input uncertainty require potentially unavailable or unknown information about the uncertainty in a system’s input parameters; such as probability distributions, nominal values or uncertain intervals. These requirements may force designers into arbitrary or even erroneous assumptions about a system’s input uncertainty when attempting to estimate nominal values and/or uncertain intervals for example. These types of assumptions can be especially degrading during the early stages in a design process when limited system information is available. In an effort to address these challenges a new design approach is presented that can produce optimal solutions in the form of upper and lower bounds (which specify uncertain intervals) for all input parameters to a system that possess reducible uncertainty. These solutions provide minimal variation in system objectives for a maximum allowed level of input uncertainty in a multi-objective sense and furthermore guarantee as close to deterministic Pareto optimal performance as possible with respect to the uncertain parameters. The function calls required by this approach are dramatically reduced through the use of a kriging meta-model assisted multi-objective optimization technique performed in two stages. The capabilities of the approach are demonstrated through three example problems of varying complexity.",2010
"Multi-Objective Optimization in Industrial Robotic Cell Design","It has become a common practice to conduct simulation-based design of industrial robotic cells, where Mechatronic system model of an industrial robot is used to accurately predict robot performance characteristics like cycle time, critical component lifetime, and energy efficiency. However, current robot programming systems do not usually provide functionality for finding the optimal design of robotic cells. Robot cell designers therefore still face significant challenge to manually search in design space for achieving optimal robot cell design in consideration of productivity measured by the cycle time, lifetime, and energy efficiency. In addition, robot cell designers experience even more challenge to consider the trade-offs between cycle time and lifetime as well as cycle time and energy efficiency. In this work, utilization of multi-objective optimization to optimal design of the work cell of an industrial robot is investigated. Solution space and Pareto front are obtained and used to demonstrate the trade-offs between cycle-time and critical component lifetime as well as cycle-time and energy efficiency of an industrial robot. Two types of multi-objective optimization have been investigated and benchmarked using optimal design problem of robotic work cells: 1) single-objective optimization constructed using Weighted Compromise Programming (WCP) of multiple objectives and 2) Pareto front optimization using multi-objective generic algorithm (MOGA-II). Of the industrial robotics significance, a combined design optimization problem is investigated, where design space consisting of design variables defining robot task placement and robot drive-train are simultaneously searched. Optimization efficiency and interesting trade-offs have been explored and successful results demonstrated.",2010
"An Improved Kriging Assisted Multi-Objective Genetic Algorithm","Although Genetic Algorithms (GAs) and Multi-Objective Genetic Algorithms (MOGAs) have been widely used in engineering design optimization, the important challenge still faced by researchers in using these methods is their high computational cost due to the population-based nature of these methods. For these problems it is important to devise MOGAs that can significantly reduce the number of simulation calls compared to a conventional MOGA. We present an improved kriging assisted MOGA, called Circled Kriging MOGA (CK-MOGA), in which kriging metamodels are embedded within the computation procedure of a traditional MOGA. In the proposed approach, the decision as to whether the original simulation or its kriging metamodel should be used for evaluating an individual is based on a new objective switch criterion and an adaptive metamodeling technique. The effect of the possible estimated error from the metamodel is mitigated by applying the new switch criterion. Three numerical and engineering examples with different degrees of difficulty are used to illustrate applicability of the proposed approach. The results show that, on the average, CK-MOGA outperforms both a conventional MOGA and our developed Kriging MOGA in terms of the number of simulation calls.",2010
"Investigating the Significance of “One-to-Many” Mappings in Multiobjective Optimization","Significant research has focused on multiobjective design optimization and negotiating trade-offs between conflicting objectives. Many times, this research has referred to the possibility of attaining similar performance from multiple, unique design combinations. While such occurrences may allow for greater design freedom, their significance has yet to be quantified for trade-off decisions made in the design space (DS). In this paper, we computationally explore which regions of the performance space (PS) exhibit “one-to-many” mappings back to the DS, and examine the behavior and validity of the corresponding region associated with this mapping. Regions of interest in the PS and DS are identified and generated using indifference thresholds to effectively “discretize” both spaces. The properties analyzed in this work are a mapped region’s location in the PS and DS and the total hypervolume of the mappings. Our proposed approach is demonstrated on two different multiobjective engineering problems. The results indicate that one-to-many mappings occur in engineering design problems, and that while these mappings can result in significant design space freedom, they often result in notable performance sacrifice.",2010
"Ball Bearing Fatigue and Wear Life Optimization Using Elastohydrodynamic and Genetic Algorithm","The present work attempts to improve the performance of rolling element bearings through the increase of fatigue life and the reduction of bearing wear. The formulation is based on Elastohydrodynamic to maximize the realistically evaluated minimum film thickness without significant increase in viscous friction torque. Design vectors are reduced in the present study relative to previous studies as some variables are considered as dependent variables. Other design vectors are not considered as variables in the study due to machining accuracy. This paper presents a more viable method to solve the multi-objective optimization problem using genetic algorithms (GAs) to reduce the chances of getting trapped in local maximum or minimum. Using a utility function, optimal Pareto points are obtained by changing the weight coefficients. Specific weights can be used depending on the designer decision whether to increase fatigue and wear life, or decrease the power consumption.",2010
"Development of a Product Family Analysis Toolkit for Systematic Benchmarking","As global markets saturate and competition intensifies, many manufacturers are focusing on benchmarking families of products alongside individual products to gain valuable insight and strategic advantage over their competitors. Unfortunately, the advantages of benchmarking families of products are often undermined by the limited capability of current benchmarking tools to assist in this process. While methods have been proposed for product family analysis and benchmarking, a major problem is the way in which the component details are collected, and few of the methods have been integrated together. Benchmarking and product family analysis is also time-consuming and subject to human variability since the process is typically done manually without the aid of software. To address these problems, we introduce the Product Family Analysis Toolkit (PFA Toolkit), which combines several popular benchmarking tools to streamline and standardize the process of product family benchmarking. We describe the toolkit’s features and capabilities and then discuss its functionality and usability. The advantages of automating the product family benchmarking process are discussed along with future work.",2010
"User Interface Design for Interactive Product Family Analysis and Variants Derivation","Product family design (PFD) is one of the commonly adopted strategies of product realization in mass customization paradigm. Among the current product family modeling approaches, ontology based modeling has been identified as a promising approach. Previously, we have studied the feasibility of using a semantically annotated multi-facet product family ontology in performing product analysis and variant derivation in the PFD domain. However, the visualization aspects of the ontology are important to assist product designers and engineers to gain insights and benefit from the ever-increasing information from the ontology, e.g. dimension, assembly or configuration wise. From the previous literature, we observe that there are limited usage of visualization and interaction in PFD for tasks such as product analysis and variant derivation. The current hierarchy based representations are limited in displaying ontological relationships and tasks such as commonality analysis seldom make use of visualization to foster better understanding of component similarity. In this study, we report our efforts in assisting product family analysis and variant derivation through visualization and user interface (UI) which enables interactive PFD. Design considerations for our visualization and user interaction design are discussed. By using a multi-touch UI, we discuss on how our UI is able to enable users to better perform product analysis and variant derivation based on the aforementioned ontology in an interactive, intuitive and intelligent manner. We finally conclude this paper with some indications for future works.",2010
"Validating the Generational Variety Index (GVI) Through Product Family Optimization: A Preliminary Study","Effective product platforms must strike an optimal balance between commonality and variety. Increasing commonality can reduce costs by improving economies of scale while increasing variety can improve market performance, or in our robot family example, satisfy various robot missions. Two metrics that have been developed to help resolve this tradeoff are the Generational Variety Index (GVI) and the Product Family Penalty Function (PFPF). GVI provides a metric to measure the amount of product redesign that is required for subsequent product offerings, whereas PFPF measures the dissimilarity or lack of commonality between design (input) parameters during product family optimization. GVI is examined because it is the most widely used metric applicable during conceptual development to determine platform components. PFPF is used to validate GVI because of its ease of implement for parametric variety, as used in this case. This paper describes a product family trade study that has been performed using GVI for a robot product family and compares the results to those obtained by optimizing the same family using PFPF. This work provides a first attempt to validate the output of GVI by using a complementary set of results obtained from optimization. The results of this study indicate that while there are sometimes similarities between the results of GVI and optimization using PFPF, there is not necessarily a direct correlation between these two metrics. Moreover, the platform recommended by GVI is not necessarily the most performance-optimized platform, but it can help improve commonality. In the same regard, PFPF may miss certain opportunities for commonality. The benefits of integrating the two approaches are also discussed.",2010
"Universal Product Family Design Valuation in an Uncertain Market Environment","Strategic adaptability is essential in capitalizing on future investment opportunities and responding properly to market trends in an uncertain environment. Customized products or services are an important source of revenue for many companies, particularly those working with in a mass customization environment where customer satisfaction is of paramount important. In this paper, we extend methods from mass customization and product family design to create specific methods for universal product family design. The objective of this research is to propose a valuation financial model to facilitate universal design strategies that will maximize the expected profit under uncertain constrains. Real options analysis is applied to estimate the valuation of options related to introducing new modules as a platform in a universal product family. We use customers’ preferences based on performance utilities for universal design to reflect demand and demographic trends. To demonstrate implementation of the proposed model, we use a case study involving a family of light-duty trucks. We perform sensitivity analysis to investigate the behavior of the estimated option value against chaining system parameters.",2010
"Designing a Product Package Platform","An essential part of designing a successful product family is establishing a recognizable, familiar, product family identity. It is very often the case that consumers first identify products based on their physical embodiment. The Apple iPod, DeWalt power tools, and KitchenAid appliances are all examples of product families that have successfully branded themselves based on physical principles. While physical branding is often the first trait apparent to designers, there are some products that cannot be differentiated based on physical appearance. This is especially common for consumable products. For example, it is impossible to differentiate between diet Coke, Classic Coke, and Pepsi when each is poured into separate glasses. When differentiation is difficult to achieve from a product’s physical characteristics, the product’s package becomes a vital part of establishing branding and communicating membership to a product family while maintaining individual product identity. In this paper, product packaging is investigated with a focus on the graphic packaging components that identify product families. These components include: color, shape, typography, and imagery. Through the application of tools used in facilities layout planning, graph theory, social network theory, and display design theory an approach to determine an optimal arrangement of graphic components is achieved. This approach is validated using a web based survey that tracks user-package interactions across a range of commonly used cereal boxes.",2010
"System RBDO With Correlated Variables Using Probabilistic Re-Analysis and Local Metamodels","A simulation-based, system reliability-based design optimization (RBDO) method is presented that can handle problems with multiple failure regions and correlated random variables. Copulas are used to represent dependence between random variables. The method uses a Probabilistic Re-Analysis (PRRA) approach in conjunction with a sequential trust-region optimization approach and local metamodels covering each trust region. PRRA calculates very efficiently the system reliability of a design by performing a single Monte Carlo (MC) simulation per trust region. Although PRRA is based on MC simulation, it calculates “smooth” sensitivity derivatives, allowing the use of a gradient-based optimizer. The PRRA method is based on importance sampling. One requirement for providing accurate results is that the support of the sampling PDF must contain the support of the joint PDF of the input random variables. The trust-region optimization approach satisfies this requirement. Local metamodels are constructed sequentially for each trust region taking advantage of the potential overlap of the trust regions. The metamodels are used to determine the value of the indicator function in MC simulation. An example with correlated input random variables demonstrates the accuracy and efficiency of the proposed RBDO method.",2010
"A Simulation-Based Random Process Method for Time-Dependent Reliability of Dynamic Systems","Reliability is an important engineering requirement for consistently delivering acceptable product performance through time. As time progresses, a product may fail due to time-dependent operating conditions and material properties, and component degradation. The reliability degradation with time may significantly increase the lifecycle cost due to potential warranty costs, repairs and loss of market share. In this work, we consider the first-passage reliability, which accounts for the first time failure of non-repairable systems. Methods are available that provide an upper bound to the true reliability, but they may overestimate the true value considerably. This paper proposes a methodology to calculate the cumulative probability of failure (probability of first passage or upcrossing) of a dynamic system with random properties, driven by an ergodic input random process. Time series modeling is used to characterize the input random process based on data from a “short” time period (e.g. seconds) from only one sample function of the random process. Sample functions of the output random process are calculated for the same “short” time because it is usually impractical to perform the calculation for a “long” duration (e.g. hours). The proposed methodology calculates the time-dependent reliability, at a “long” time using an accurate “extrapolation” procedure of the failure rate. A representative example of a quarter car model subjected to a stochastic road excitation demonstrates the improved accuracy of the proposed method compared with available methods.",2010
"Multi-Objective Design and Tolerance Allocation for Single- and Multi-Level Systems","In this work we develop a method to perform simultaneous design and tolerance allocation for engineering problems with multiple objectives. Most studies in existing literature focus on either optimal design with constant tolerances or the optimal tolerance allocation for a given design setup. Simultaneously performing both design and tolerance allocation with multiple objectives for hierarchical systems increases problem dimensions and raises additional computational challenges. A design framework is proposed to obtain optimal design alternatives and to rank their performances when variations are present. An optimality influence range is developed to aid design alternatives selections with an influence signal-to-noise ratio that indicates the accordance of objective variations to the Pareto set and an influence area that quantifies the variations of a design. An additional tolerance design scheme is implemented to ensure that design alternatives meet the target tolerance regions. The proposed method is also extended to decomposed multi-level systems by integrating traditional sensitivity analysis for uncertainty propagation with analytical target cascading. This work enables decision-makers to select their best design alternatives on the Pareto set using three measures with different purposes. Examples demonstrate the effectiveness of the method on both single- and multi-level systems.",2010
"Modified Reduced Gradient With Realizations Sorting for Hard Equality Constraints in Reliability-Based Design Optimization","In this work, the presence of equality constraints in reliability-based design optimization (RBDO) problems is studied. Relaxation of soft equality constraints in RBDO and its challenges are briefly discussed while the main focus is on hard equalities that can not be violated even under uncertainty. Direct elimination of hard equalities to reduce problem dimensions is usually suggested; however, for nonlinear or black-box functions, variable elimination requires expensive root-finding processes or inverse functions that are generally unavailable. We extend the reduced gradient methods in deterministic optimization to handle hard equalities in RBDO. The efficiency and accuracy of the first and the second order predictions in reduced gradient methods are compared. Results show the first order prediction being more efficient when realizations of random variables are available. A gradient-weighted sorting with these random samples is proposed to further improve the solution efficiency of the reduced gradient method. Feasible design realizations subject to hard equality constraints are then available to be implemented with the state-of-the-art sampling techniques for RBDO problems. Numerical and engineering examples show the strength and simplicity of the proposed method.",2010
"Probabilistic Design Optimization of Frequency Dispersion for Rotating Blades","In this paper, a probabilistic design optimization method based on finite element method is proposed to calculate the variability of design parameters subject to a specified dispersion of natural frequencies of rotating blades. The element stiffness and mass matrices are derived using a two-stage finite element method and numerical integration. Based on the perturbation technology, the sensitivity of the frequencies, as well as relationship between the frequency dispersion and the coefficient of variability (CV) of the design parameters can be obtained. Such sensitivity information is then used to convert the probabilistic design optimization problem into a deterministic optimization problem. Two case studies are given to illustrate the proposed method. From the results, it is concluded that rotation of blade changes the sensitivity of CV to the design parameters considered, and using the proposed method can transform the probabilistic constraints to deterministic constraints.",2010
"Second-Order Reliability Method With First-Order Efficiency","The widely used First Order Reliability Method (FORM) is efficient, but may not be accurate for nonlinear limit-state functions. The Second Order Reliability Method (SORM) is more accurate but less efficient. To maintain both high accuracy and efficiency, we propose a new second order reliability analysis method with first order efficiency. The method first performs the FORM and identifies the Most Probable Point (MPP). Then the associated limit-state function is decomposed into additive univariate functions at the MPP. Each univariate function is further approximated as a quadratic function, which is created with the gradient information at the MPP and one more point near the MPP. The cumulant generating function of the approximated limit-state function is then available so that saddlepoint approximation can be easily applied for computing the probability of failure. The accuracy of the new method is comparable to that of the SORM, and its efficiency is in the same order of magnitude as the FORM.",2010
"A Joint Probability Approach to Multiobjective Optimization Under Uncertainty","In this paper, we present a new approach to solve optimization problems with multiple objectives under uncertainty. Optimality is considered in terms of the risk that the overall system performance, as defined by all of the multiple objectives exceeding their desired thresholds, remains acceptable. Unlike the existing state-of-the-art, where first-order moments of the system level objectives are used to ensure optimality, we employ a joint probability formulation in our research. The Pareto optimality criterion under uncertainty is defined in terms of joint probability, i.e., probability that all  system objectives are less than the desired thresholds. These thresholds can be viewed as the desired upper/lower bounds on the individual system objectives. The higher the joint probability, the more reliably the thresholds bound the system performance, hence the lower the overall system performance risk. However, a desirable high joint probability may necessitate undesirably high/low thresholds, and hence the tradeoff. In this context, the proposed method provides two decision-making capabilities: (1) Maximum probability design:  given a set of threshold values for system objectives, find the design that yields the maximum joint probability (2) Optimum threshold design:  Given a desired joint probability, find the set of thresholds that yield this probability. In this paper, optimization formulations are presented to solve the above two decision-making problems. A two-bar truss example and the conceptual design of a two-stage-to-orbit launch vehicle are presented to illustrate the proposed methods. The numerical results show that optimizing the mean values of the objectives individually does not necessarily guarantee the desired performance of all objectives jointly under uncertainty, which is of ultimate interest in multiobjective optimization.",2010
"Parameter Screening in Dynamic Computer Model Calibration Using Global Sensitivities","Sensitivity analysis and computer model calibration are generally treated as two separate topics. In sensitivity analysis one quantifies the effect of each input factor on outputs, whereas in calibration one finds the values of input factors that provide the best match to a set of field data. In this paper we show a connection between these two seemingly separate concepts, and illustrate it with an automotive industry application involving a Road Load Acquisition Data (RLDA) computer model. We use global sensitivity analysis for computer models with transient responses to screen out inactive input parameters and make the calibration algorithm numerically more stable. Because the computer model can be computationally intensive, we construct a fast statistical surrogate for the computer model with transient responses. This fast surrogate is used for both sensitivity analysis and RLDA computer model calibration.",2010
"Trending Mining for Predictive Product Design","The ",2010
"Validating Designs Through Sequential Simulation-Based Optimization","Computational simulation models support a rapid design process. Given model approximation and operating conditions uncertainty, designers must have confidence that the designs obtained using simulations will perform as expected. This paper presents a methodology for validating designs as they are generated during a simulation-based optimization process. Current practice focuses on validation of simulation models throughout the entire design space. In contrast, the proposed methodology requires validation only at design points generated during optimization. The goal of such validation is confidence in the resulting design rather than in the underlying simulation model. The proposed methodology is illustrated on a simple cantilever beam design subject to vibration.",2010
"Risk Management in Product Design: Current State, Conceptual Model and Future Research","Risk management is an important element of product design. It helps to minimize the project- and product-related risks such as project budget and schedule overrun, or missing product cost and quality targets. Risk management is especially important for complex, international product design projects that involve a high degree of novel technology. This paper reviews the literature on risk management in product design. It examines the newly released international standard ISO 31000 “Risk management — Principles and guidelines” and explores its applicability to product design. The new standard consists of the seven process steps communication and consultation; establishing the context; risk identification; risk analysis; risk evaluation; risk treatment; and monitoring and review. A literature review reveals, among other findings, that the general ISO 31000 process model seems applicable to risk management in product design; the literature addresses different process elements to varying degrees, but none fully according to ISO recommendations; and that the integration of product design risk management with risk management of other disciplines, or between project and portfolio level in product design, is not well developed.",2010
"Model-Based Method to Translate System Level Customer Need to Part Specification","Products are successful because they meet customer needs. However, many customer needs are not expressed in measurable terms. In addition, when such needs are achieved by a complex system made of hardware parts and software, decomposing customer needs to part-level specification is not a trivial task. This paper presents a model-based approach to address such problems. In the case study, the customer need was the noise and vibration level of an unconventional gasoline engine system when running at idle. The hardware component whose performance tolerance needed to be specified was a new type of fuel injectors. These new fuel injectors had higher piece-to-piece performance variations than the conventional fuel injectors. It was unclear whether such variation was acceptable for customer perceived powertrain quality. A virtual powertrain system simulation model was used to analytically evaluate the impact of the fuel injector performance variability. Monte Carlo simulation was carried out to assess the impact of injector variability. The results from the simulation were further refined using engine hardware testing. This study made recommendations for the acceptable level of hardware tolerance, which was different from what the supplier of the injectors had suggested.",2010
"Sampling-Based Stochastic Sensitivity Analysis Using Score Functions for RBDO Problems With Correlated Random Variables","This study presents a methodology for computing stochastic sensitivities with respect to the design variables, which are the mean values of the input correlated random variables. Assuming that an accurate surrogate model is available, the proposed method calculates the component reliability, system reliability, or statistical moments and their sensitivities by applying Monte Carlo simulation (MCS) to the accurate surrogate model. Since the surrogate model is used, the computational cost for the stochastic sensitivity analysis is negligible. The copula is used to model the joint distribution of the correlated input random variables, and the score function is used to derive the stochastic sensitivities of reliability or statistical moments for the correlated random variables. An important merit of the proposed method is that it does not require the gradients of performance functions, which are known to be erroneous when obtained from the surrogate model, or the transformation from X-space to U-space for reliability analysis. Since no transformation is required and the reliability or statistical moment is calculated in X-space, there is no approximation or restriction in calculating the sensitivities of the reliability or statistical moment. Numerical results indicate that the proposed method can estimate the sensitivities of the reliability or statistical moments very accurately, even when the input random variables are correlated.",2010
"Reliability-Based Design Optimization With Confidence Level for Non-Gaussian Distributions Using Bootstrap Method","For reliability-based design optimization (RBDO), generating an input statistical model with confidence level has been recently proposed to offset the inaccurate estimation of the input statistical model with Gaussian distributions. For this, the confidence intervals of mean and standard deviation are calculated using the Gaussian distributions of input random variables. However, if the input random variables are non-Gaussian, the use of the Gaussian distributions of input variables will provide inaccurate confidence intervals, and thus, yield undesirable confidence level of the reliability-based optimum design meeting the target reliability ",2010
"Multiscale Variability and Uncertainty Quantification Based on a Generalized Multiscale Markov Model","Variability is inherent randomness in systems, whereas uncertainty is due to lack of knowledge. In this paper, a generalized multiscale Markov (GMM) model is proposed to quantify variability and uncertainty simultaneously in multiscale system analysis. The GMM model is based on a new imprecise probability theory that has the form of generalized interval, which is a Kaucher or modal extension of classical set-based intervals to represent uncertainties. The properties of the new definitions of independence and Bayesian inference are studied. Based on a new Bayes’ rule with generalized intervals, three cross-scale validation approaches that incorporate variability and uncertainty propagation are also developed.",2010
"Updating Predictive Models: Calibration, Bias Correction and Identifiability","Model updating, which utilizes mathematical means to combine model simulations with physical observations for improving model predictions, has been viewed as an integral part of a model validation process. While calibration is often used to “tune” uncertain model parameters, bias-correction has been used to capture model inadequacy due to a lack of knowledge of the physics of a problem. While both sources of uncertainty co-exist, these two techniques are often implemented separately in model updating. This paper examines existing approaches to model updating and presents a modular Bayesian approach as a comprehensive framework that accounts for many sources of uncertainty in a typical model updating process and provides stochastic predictions for the purpose of design. In addition to the uncertainty in the computer model parameters and the computer model itself, this framework accounts for the experimental uncertainty and the uncertainty due to the lack of data in both computer simulations and physical experiments using the Gaussian process model. Several challenges are apparent in the implementation of the modular Bayesian approach. We argue that distinguishing between uncertain model parameters (calibration) and systematic inadequacies (bias correction) is often quite challenging due to an identifiability issue. We present several explanations and examples of this issue and bring up the needs of future research in distinguishing between the two sources of uncertainty.",2010
"A Hybrid Reliability Approach for Reliability-Based Design Optimization","Reliability-based Design Optimization problems have been solved by two well-known methods: Reliability Index Approach (RIA) and Performance Measure Approach (PMA). RIA generates first-order approximate probabilistic constraints using the measures of reliability indices. For infeasible design points, the traditional RIA method suffers from inaccurate evaluation of the reliability index. To overcome this problem, the Modified Reliability Index Approach (MRIA) has been proposed. The MRIA provides the accurate solution of the reliability index but also inherits some inefficiency characteristics from the Most Probable Failure Point (MPFP) search when nonlinear constraints are involved. In this paper, the benchmark examples have been utilized to examine the efficiency and stability of both PMA and MRIA. In our study, we found that the MRIA is capable of obtaining the correct optimal solutions regardless of the locations of design points but the PMA is much efficient in the inverse reliability analysis. To take advantages of the strengths of both methods, a Hybrid Reliability Approach (HRA) is proposed. The HRA uses a selection factor that can determine which method to use during optimization iterations. Numerical examples from the proposed method are presented and compared with the MRIA and the PMA.",2010
"Enabling Integrated Material and Product Design Under Uncertainty Through Stochastic Constitutive Relations","This paper presents a computational framework that mathematically propagates material microstructure uncertainties to coarser system resolutions for use in multiscale design frameworks. The computational framework uses a homogenized stochastic constitutive relation that links microstructure uncertainty with stochastic material properties. The stochastic constitutive relation formulated in this work serves as the critical link between the material and product domains in integrated material and product design. Ubiquitous fine resolution uncertainty sources influencing prediction of material properties based on their structures are categorized, and stochastic cell averaging is achieved by two advanced uncertainty quantification methods: random process polynomial chaos expansion and statistical copula functions. Both methods confront the mathematical difficulty in randomizing constitutive law parameters by capturing the marked correlation among them often seen in complex materials, thus the results proffer a more accurate probabilistic estimation of constitutive material behavior. The method put forth in this research, though quite general, is applied to a plastic, high strength steel alloy for demonstration.",2010
"Tolerance Allocation of Assemblies Using Fuzzy Comprehensive Evaluation and Decision Support Processes","Advancements in manufacturing technology significantly impact the design process. The ability to manufacture assembly components with specific tolerances has increased the need for tolerance allocation. This research proposes a framework that overcomes the drawbacks of the traditional tolerance control methods, and reduces subjectivity by using fuzzy set theory and decision support processes. The combination of fuzzy comprehensive evaluation and conjoint analysis facilitate the reduction of subjectivity in the tolerance control process. The application of the framework is demonstrated with two practical engineering problems. Tolerances are allocated for a clutch assembly and an O-ring seal in an accumulator.",2010
"Effective Random Field Characterization Considering Statistical Dependence for Probability Analysis and Design","The Proper Orthogonal Decomposition (POD) method has been employed to extract the important signatures of the random field presented in an engineering product or process. Our preliminary study found that coefficients of the signatures are statistically uncorrelated but may be dependent. In general, the statistical dependence of the coefficients is ignored in the random field characterization for probability analysis and design. This paper thus proposes an effective approach to characterize the random field for probability analysis and design while accounting for the statistical dependence among the coefficients. The proposed approach is composed of two technical contributions. The first contribution is to develop a generic approximation scheme of random field as a function of the most important field signatures while preserving prescribed approximation accuracy. The coefficients of the signatures can be modeled as random field variables and their statistical properties are identified using the Chi-Square goodness-of-fit test. Second, the Rosenblatt transformation is employed to transform the statistically dependent random field variables into statistically independent random field variables. There exist so many transformation sequences when the number of random field variables becomes large. It was found that an improper selection of a transformation sequence may introduce high nonlinearity into system responses, which causes inaccuracy in probability analysis and design. Hence, a novel procedure is proposed for determining an optimal transformation sequence that introduces the least degree of nonlinearity to the system response after the Rosenblatt transformation. The proposed random field characterization can be integrated with one of the advanced probability analysis methods, such as the Eigenvector Dimension Reduction (EDR) method, Polynomial Chaos Expansion (PCE) method, etc. Three structural examples including a Micro-Electro-Mechanical Systems (MEMS) bistable mechanism are used to demonstrate the effectiveness of the proposed approach. The results show that the statistical dependence in random field characterization cannot be neglected for probability analysis and design. Moreover, it is shown that the proposed random field approach is very accurate and efficient.",2010
"Validation of Computational Fluid Structure Interaction Models for Shape Optimization Under Blast Impact","A first order structural optimization problem is examined to evaluate the effects of structural geometry on blast energy transfer in a fully coupled fluid structure interaction problem. The fidelity of the fluid structure interaction simulation is shown to yield significant insights into the blast mitigation problem not captured in similar empirically based blast models. An emphasis is placed on the accuracy of simulating such fluid structure interactions and its implications on designing continuum level structures. Higher order design methodologies and algorithms are discussed for the application of such fully coupled simulations on vehicle level optimization problems.",2010
"A Modified Continuous Reactive Tabu Search for Damage Detection in Beams","The use of vibration-based techniques in damage identification has recently received considerable attention in many engineering disciplines. While various damage indicators have been proposed in the literature, those relying only on changes in the natural frequencies are quite appealing since these quantities can conveniently be acquired. The identification of damage involves an optimization step where response of a continuously updated finite elements model (FEM) is compared with the response of the experimental measurements and error between both responses is minimized. In this paper it is shown that such error function is highly multi-modal and that the same response can be obtained by more than one damage scenario. In order to find these optima a hybrid optimization approach is developed which utilizes two components; namely. Modified Continuous Reactive Tabu Search (MCRTS) and Real Coded Genetic Algorithms. MCRTS, the primary component, is a meta-heuristic capable of finding several optima in a multi-modal search space, which suites the nature of the problem at hand. GAs, the secondary component, although a global optimizer, is used as a local optimizer that is fired in promising regions of the search space as identified by the major component (MCRTS). It is used in favor of direct search methods to account for the presence of minor local optima. In order to test the algorithm, several beams are manufactured and crack damages are induced using wire-cutting, and the natural frequencies are tested experimentally. Such beams have two locations that can give the same response. The developed algorithm managed to find the two sought-for optima consistently in several runs. This proves the merit of this approach as being capable of handling the problem at hand.",2010
"An Efficient Tradeoff Approach for Topology Optimization With Manufacturing Constraints","In this paper an efficient approach for generating tradeoff curves when performing topology optimization with manufacturing constraints is presented. By minimizing a new stiffness-volume ratio, or in-fact a new compliance-volume product, the tradeoff curve is generated by changing a new design parameter. The volume appearing in the objective is raised to the power of this new design parameter. In such manner different conceptual designs can be generated. By adopting a nested approach, the problem is easily solved by a simple numerical scheme. This is a nice feature of the approach which makes the numerical performance most efficient and robust. This feature makes it also easy to include manufacturing constraints by simply updating the move limits such that these constraints are satisfied. The design parametrization is done by the SIMP-model and patterns of checker-boards are prevented by adopting Sigmund’s filter. The efficiency of the approach is demonstrated by presenting tradeoff curves for both 2D- and 3D-problems.",2010
"Optimum Design of an Anchoring System for Percutaneous Mitral Valve Repair","In a novel procedure for percutaneous mitral valve repair, inter-related hook-shaped anchors are inserted around the annulus to replace the surgeon’s suturing in open-heart ring annuloplasty. To properly attach to the tissue, the anchors should withstand large deformation applied during the delivery process and recover their original shape when released into the heart tissue. To this end, stress concentration is avoided along the anchors, which are fabricated of a super-elastic material, by means of shape optimization. Shape optimization consists in finding the smoothest anchor mid-curve possible, which minimizes the von Mises stresses applied during the delivery process. An optimization algorithm aimed at minimizing the weighted rms value of the curvature is introduced. A geometrically optimum shape is obtained by equally weighting the curvature values. Further reduction in the stress values is possible by weighting the curvature values along the anchor in an iterative procedure that yields a structurally optimum anchor. The weights at each iteration are defined proportional to the stress distribution along the anchor obtained in the previous iteration.",2010
"Tracing Pareto-Optimal Frontiers in Topology Optimization","In ",2010
"Topology Optimization of Piezoelectric Energy Harvesting Devices Subjected to Stochastic Excitation","Converting ambient vibration energy into electrical energy using piezoelectric energy harvester has attracted much interest in the past decades. In this paper, topology optimization is applied to design the optimal layout of the piezoelectric energy harvesting devices. The objective function is defined as to maximize the energy harvesting performance over a range of ambient vibration frequencies. Pseudo excitation method (PEM) is applied to analyze structural stationary random responses. Sensitivity analysis is derived by the adjoint method. Numerical examples are presented to demonstrate the validity of the proposed approach.",2010
"Fastener Pattern Optimization of an Eccentrically Loaded Multi-Fastener Connection","This paper presents the use of a genetic algorithm in conjunction with geometric nonlinear finite element analysis to optimize the fastener pattern and lug location in an eccentrically loaded multi-fastener connection. No frictional resistance to shear was included in the model, as the connection transmitted shear loads into four dowel fasteners through bearing-type contact without fastener preload. With the goal of reducing the maximum von Mises stress in the connection to improve fatigue life, the location of the lug hole and four fastener holes were optimized to achieve 55% less maximum stress than a similar optimization using the traditional instantaneous center of rotation method. Since the maximum stress concentration was located at the edge of a fastener hole where fatigue cracks could be a concern, reduction of this quantity lowers the probability of crack growth for both bearing-type and slip-resistant connections. It was also found that the location of the maximum von Mises stress concentration jumped from the fastener region to the lug as the applied force angle was decreased below 45 degrees, thus the fastener pattern could not be optimized for lower angles.",2010
"The Mathematical Model of a Procedure for Percutaneous Annuloplasty","Existing mathematical models of the mitral valve allow the simulation of ring open-heart annuloplasty procedures intended to reduce the lumen of the valve. Using these models, only a posteriori effects can be predicted. With the advent of novel percutaneous annuloplasty approaches, there is a need to describe a priori effects; in particular, this paper focuses on a technique which consists of sequentially installing interconnected anchors around the mitral annulus, whose lumen is reduced by the tightening of the tethered wire. We develop here a static mathematical model of the mitral annulus that takes into account the mechanical response of its tissue and the surrounding muscular tissue. A number of roughly coplanar points corresponding to anchor positions, at about equal distantes, are identified on the annulus. Each of these points is then attached to a linearly elastic spring of a given stiffness, The spring-end is connected to a fixed pinned support, the other end supporting the wire, that forms a loop. With this model we estimate the anchor-points position vectors after lumen reduction and the wire tension that is needed to reduce the perimeter of the polygon defined by the anchor points to a given value, which, for each patient, is related to the desired lumen. This formulation leads to the minimization of the potential energy of the mechanical system over the position vectors of the anchor points after tightening, which are the design variables. These are found by solving the first-order normality conditions of the equality-constrained optimization problem. Preliminary experimental data obtained on cadaveric swine hearts validate the model: it can be used to predict, for a given perimeter size reduction, the wire tension as well as the anchor position after repair.",2010
"Using Level Set Method in Order to Design Structures Against Buckling","The level set approach has been used as a powerful tool in designing structures with a proper safety margin against stability and buckling issues. In this article a closed form equation for critical buckling load of any arbitrary topology has been proposed and employed in Level Set formulation in order to maximize it. Results show that the Level Set Method is straight forward and easy to implement, with fewer limitations overall in the topology optimization of engineering structures.",2010
"An Optimization Model for Parabolic-Trough Solar Power Generation Systems","This paper explores the design optimization of parabolic-trough solar power generation systems. In these systems, solar radiation is focused onto receiver tubes in which a thermal carrier fluid is circulated. The collected thermal energy is then used to generate steam that powers a steam turbine to drive an electric generator. An optimization model is constructed that aims to minimize the cost of electric energy produced. In this model, the optimization is concerned with decision variables that affect: i) the solar field and ii) thermal storage. The steam turbine and generator are not part of the optimization model, as they are assumed to use the same off-shelf components that are used in fossil-fuel based power plants. It is understood that decisions concerning the solar field both affect and are affected by the design of the solar collector assemblies (SCAs), which are the support structures that hold the focusing mirrors. Design of the SCAs is a structural optimization problem that aims to minimize the cost of the structure while satisfying dimensional and loading constraints. Genetic algorithm (GA) is used for the optimization of the parabolic trough system model. For every candidate design examined by GA for the solar field and thermal storage, the most suitable structural design of the SCAs is obtained from solving the sub-problem of structural optimization. This “nested” optimization model is made possible by pre-analyzing a large range of SCA designs and recording them as a lookup database. The developed optimization model of the parabolic trough systems allows for parametric studies on how certain incentives, government policies and key technological developments may affect the system design decisions.",2010
