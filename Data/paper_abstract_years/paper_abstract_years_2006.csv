title,Abstract,year
"Pattern-Based Reasoning in Critical Parameter Management (CPM) for Rapid Redesign","Critical Parameter Management (CPM) is an emerging area in engineering design owing to the motivation from Design for Six Sigma. Introduced in this paper is the exploration of CPM to support rapid redesign of ",2006
"Topological Synthesis of Shell Structures","Currently available tools for classical topology optimization of structures have proven valuable in conceptual design. These tools may provide design direction very early in the design cycle. However, the results subsequently need to be interpreted and translated by an engineer into a consistent CAD-model. This research focuses on the topological design synthesis of shell structures, which is being carried out using a design language approach. The aim of this approach is to automatically generate, modify and optimize an abstract representation of the design. This representation is automatically translatable into a CAD-model and will thus lead to an optimization process that offers a valid structural CAD-model as result. Design languages serve in this context as a computable abstraction of design representation and synthesis by use of rule-based information processing mechanisms. These rules (also called design patterns), are applied to generate and modify the topology of the design representation. Design patterns contain the engineers know-how and best-practice. The computerized execution of design patterns in a design compiler yields a powerful topology modification tool. As prototype application, the synthesis of shell structures is presented in this work. The automation mechanisms and the information flow through design synthesis, model generation, design analysis and evaluation are outlined. A discussion on the future application of design patterns for knowledge-based structural optimization is derived from the shown examples.",2006
"Growth Design Modeling","As an essential part of design, the conceptual design needs computer assistance from its initial design stage of product development. However computer-aided conceptual design is limited by the current function-to-form mapping approaches. A new growth design process model is proposed in this paper in which geometrical solutions grow gradually from scratch to their complete configurations. Two theoretical design principles, Decomposition & Reconstitution (D&R) principle and Cell Division principle, are briefly introduced to provide better understanding of Growth Design model. The concept of Conceptual Structure is used to support the transition process from conceptual design to detailed design in the growth design process. Finally, an application case is introduced to show the effects of the growth design model.",2006
"Fuzzy Preference Evaluation for Hierarchical Co-Evolutionary Design Concept Generation","Conceptual design is important but complex. Its success heavily depends on a designer’s individual experience and intuition. Design support tools are in need to assist designers to improve design quality and efficiency. However, to date there are few computational tools that are mature enough to provide effective assistance for design concept generation. One of the major reasons is that design information is inherently incomplete and subjective at the early stage of design. No effective evaluation methods have been devised to assess the connectivity between means (i.e., sub-solutions or function carriers) although it has an impact effect on system performance. In this paper, we propose a fuzzy reference model for conceptual design evaluation as part of our hierarchical co-evolutionary design concept generation based on function-means connectivity. An example of designing a simple mechanical transporter is presented to demonstrate the proposed approach.",2006
"Application of a Genetic Algorithm to Concept Variant Selection","This paper outlines a framework for applying a genetic algorithm to the selection of component variants between the conceptual and detailed design stages of product development. A genetic algorithm (GA) is defined for the problem and an example is presented that demonstrates its application and usefulness. Functional modeling techniques are used to formulate the design problem and generate the chromosomes that are evaluated with the algorithm. In the presented example, suitable GA parameters and the break-even point where the GA surpassed an enumerated search of the same solution space were found. Recommend uses of the GA along with limitations of the method and future work are presented as well.",2006
"A Hybrid Method for Parametric Conceptual Vehicle Design Using Legacy Components and Parametric Surfaces","Early conceptual design is one of the most important stages in vehicle product development. At this stage, vehicle design information is limited. In many cases, historical or benchmark vehicle data are used as surrogates in decision making. Some of the legacy components may be reused in the new vehicle design in order to reduce development and manufacturing costs. Nowadays, parametric modeling methods have been developed and employed for conceptual vehicle design. However, how to quickly reuse the previous design and legacy vehicle data in parametric design is still a challenge. A hybrid method for parametric conceptual vehicle design is presented in this paper, which uses both legacy components and parametric surfaces. This method allows easy reuse of historical data together with new parametric surfaces for early vehicle design. It enables mix and match of legacy components with newly designed parametric surfaces in representing a new design. This method provides a systematic way to enforce the commonality and reusability in a vehicle design.",2006
"Exploiting Shape Similarity in Engineering Analysis","Engineering analysis methods, such as the finite element method, are employed extensively to optimize complex engineering designs, but their success in conceptual product development is rather limited since numerous designs must be analyzed to cover the design space, and unfortunately, modern analysis methods can be tedious and time consuming in such scenarios. We propose here a novel analysis methodology for conceptual design wherein, given the simulation results and performance of one of the designs, one predicts upper and lower bounds on the performance of geometrically similar designs. The methodology rests on sound mathematical principles such as adjoint theory of boundary value problems, and is partly motivated by recent work on shape similarity exploitation in manufacturing wherein the cost of manufacturing a new part is estimated by retrieving the manufacturing costs of geometrically similar parts.",2006
"Issues of Similarity in Engineering Design","A useful way to generate solutions to engineering design problems is to compare the solutions of design problems similar to the one at hand and validate the solutions to satisfy the new design requirements. This process involves evaluating the similarity between the design problem at hand and the various design problems in the repository. The purpose of this paper is to investigate the meaning and the use of similarity in engineering design. Various similarity theories in literature have been explored. Previous applications of these theories are limited to the retrieval of similar computer-aided design models and process plans. This paper extends the applications of these theories to the various stages of the design process.",2006
"Enabling Local Risk Assessment to Support Global Collaboration in a Distributed Environment","Risk is a crucial criterion for decision making among multiple stakeholders negotiating for an agreement in a distributed environment. The challenge here is that risk may have different meanings and implications to different stakeholders, and this creates considerable barriers to effective negotiation and coordination in collaborative design. Our goal is to 1) capture the heterogeneous risk information at intra- and inter- stakeholder levels, 2) represent them using a uniform structure based on a function-failure relationship, and 3) enable the negotiation of the risk information among the multiple stakeholders through this uniform structure. Though a significant number of existing methods for risk analysis and management have been developed, these methods mainly focus on the local domain of a certain single stakeholder, and few have considered the possible influence and variations related to global aspects that is important for negotiation among multiple, distributed stakeholders. This work develops intra-level risk property tables to capture and represent the various risk evaluations from individual members in a single stakeholder; and then inter-level risk property tables are formed based on the synthesis of the various intra-level risk properties into a group representation for the single stakeholder, which is directly used in global negotiation and coordination with other stakeholders. An adjustable approach is used in our work to enable the adjustability of the intra- and inter- level risk evaluations via negotiation. An example problem from a NSF/NEES-sponsored research collaborative network is used to demonstrate the use of this method. The preliminary results show that this method has potential in enabling local risk assessment to support global negotiation and coordination in a distributed, collaborative environment.",2006
"Enhanced Convergence in Distributed Design Processes","The focus of this paper is on studying the convergence properties of the solution process of decentralized or distributed subsystems, where each subsystem has its own design problem, including objective(s), constraints, and design variables. The challenging aspect of this type of problem comes in the coupling of the subsystems, which create complex research and implementation challenges in modeling and solving these types of problems. We focus on the dynamics of these distributed design problems and attempt to further the understanding of the fundamental mechanics behind these processes in order to support the decisions being made by a network of decision makers. In this work, the domain of attraction, or region where convergence to a stable equilibrium point is guaranteed, of a decentralized design process is studied. Two approaches based on concepts from nonlinear control theory are presented: the first determines the domain of attraction for a specified Lyapunov function and the second optimizes for a Lyapunov function which maximizes the domain of attraction. The two techniques are illustrated on a benchmark pressure vessel design problem.",2006
"Investigating Different Approaches for Front-Loading Problem Solving in Product Development: Evidence From Engine Engineering","Front-loading problem solving is a strategy that seeks to improve product development performance by shifting the identification and solving of design problems to the earlier phases of product development process. Front-loading reduces the development time by speeding up problem solving process and eliminating the total number of problems solved in a project. Furthermore, it supports intelligent decision making through loading problem solving tasks with required pieces of knowledge. In this work, front-loading is studied from a conceptual view point. Also different approaches for front-loading are investigated and classified. To support the discussions with practical examples, front-loading in studied in the context of engine engineering.",2006
"Case Study Instrument Development for Studying Collaborative Design","This paper outlines a research instrument developed to analyze the relationship between communication modes, leadership styles, and team composition. The instrument is a survey that captures this information from collaborative design team members. This information can be correlated with team success, and the productive characteristics can be encouraged in future groups. The survey was distributed and analyzed in small numbers, and first round recommendations and student feedback are gathered. The developed instrument gathers background information on the student, group, and project. The project itself was defined in four stages: Problem Definition, Concept Generation, Concept Selection, and Concept Expansion. The students were asked questions about the team leadership style based on the Vroom-Yetton Model. The students were then asked how often they used various communication modes (verbal, textual, and graphical) when communicating Peer-to-Peer, Peer-to-Group, and Group as a Whole. These questions were repeated for each of the design stages. The instrument was structured and refined in order to analyze the behavior of undergraduate design students. It is intended to provide researchers and educators a tool to evaluate and critique collaborative behavior in order to streamline the design process. However, the survey was formulated broadly enough to be used in an industrial setting with small changes in the format to accommodate more experienced designers. The analysis of the original distribution revealed that the survey should be broken down into four parts corresponding to the four design stages and administered longitudinally.",2006
"Topology and Classical Shape Optimization of a Lower Control Arm: A Case Study","This paper addresses the design of a lower control arm of a sport utility vehicle. Four worst load cases were identified and for each of them the forces and moments at each of the three ends of the arm were obtained as input. There was also some space constraint within which the arm had to be designed. A finite-element based methodology is presented here to design a lower control arm. First Topology optimization is applied to get an overall approximate shape within the design space. This process is similar to finding out a load path for the structure to withstand the loads. This shape is then sized to withstand all the loads using classical shape optimization techniques. Shape vectors, which are the most important inputs for shape optimization, are generated for the different regions of the control arm. This case study illustrates the complete process of topology and shape optimizations to get a final design of the lower control arm.",2006
"Improved Head Restraint Design for Safety and Compliance","The National Highway Traffic Safety Administration (NHTSA) recently revised Federal Motor Vehicle Safety Standard (FMVSS) 202, which governs head restraints. The new standard, known as FMVSS 202a, establishes for the first time in the U.S. a requirement for the fore-aft position of the head restraint. The fore-aft distance between the head restraint and headform representing a midsize male occupant must not exceed 55 mm when measured with the seat back angle set to 25 degrees. The goal of the rule change is to reduce the incidence of whiplash-associated disorders caused by rear impacts. Moving the head restraint closer to the head prior to impact decreases the amount of relative motion between the occupants’ heads and torsos and is believed to decrease the risk of soft-tissue neck injury. As manufacturers phase in seats that meet the new criterion, some vehicle models are producing complaints from drivers that the head restraint causes discomfort by interfering with their preferred head position, forcing them to select a more reclined seat back angle than they would prefer. To address this issue, an analysis of driver head locations relative to the seat was conducted using a new optimization-based framework for vehicle interior optimization. The approach uses simulations with thousands of virtual occupants to quantity distributions of postural variables of interest. In this case, the analysis showed that smaller-stature occupants are disproportionately likely to experience head-position interference from a head restraint that is rigidly affixed to the seat back. Using an analysis approach that considers both postural and anthropometric variability, design guidelines for the kinematics of an articulated head restraint are proposed. Such a restraint would provide optimal head restraint positioning across occupant sizes while minimizing interference.",2006
"An Efficient Algorithm for Vehicle Crashworthiness Design Via Crash Mode Matching","This paper presents an efficient algorithm for developing vehicle structures for crashworthiness, based on the analyses of ",2006
"Crashworthiness Design Using a Hybrid Cellular Automaton Algorithm","Crashworthiness design is an evolving discipline that combines vehicle crash simulation and design synthesis. The goal is to increase passenger safety subject to manufacturing cost constraints. The crashworthiness design process requires the modeling of the complex interactions involved in a crash event. Current approaches utilize a parameterized optimization approach that requires response surface approximations of the design space. This is due to the expensive nature of numerical crash simulations and the high nonlinearity and noisiness in the design space. These methodologies usually require a significant design effort to determine an initial design. In this research, a non-gradient approach to topology optimization is developed for crashworthiness design. The methodology utilizes the cellular automata paradigm to generate a concept design.",2006
"Preliminary Results on Generating Assembly Sequences for Shape Display","Recently there has been much interest in developing shape display systems, i.e. systems that can generate, based on a virtual representation such as a CAD file, a physical shape that a user can touch. The approach considered here is to automatically assemble a shape from simple, identical building blocks using a robot. This paper describes preliminary research results on generating assembly sequences for this purpose, using a rule-based approach in order to deal with the huge search space due to the large number of components. While many of the details of the rule-based approach are discussed elsewhere, this paper focuses on analyzing and comparing the use of two different methods, the assembly method and the disassembly method in the z-direction, for assembly sequence generation.",2006
"Process Planning Support System Considering Product Quality","This paper proposes a new process planning method to achieve intended product quality. The design for the product quality is achieved by total designing and planning of production process considering both good factors and bad factors that affect product quality in the production process. The representation model of quality related production knowledge that is directly used in the computational design method is proposed. The discrete quality related behavior of product in process is modeled. Input information of the proposed design method consists following information: 1) a product structure; 2) feasible operations of the production process; 3) possible quality related causalities in the production process. The design method calculates the space of quality state automatically by this method. Final output information from this method is a good production process (good operations and good sequence) that achieves the intended quality state of final product. In order to develop this production process planning method for keeping the intended quality, this paper proposes a computational representation model of the quality related information in production process. Based on the production process model, a computational recommendation algorithm of production process that can achieve the intended product quality state is developed. The representation, sharing and reuse of production knowledge and know-how are realized based on the proposed production process model. A prototype system for planning a production process without failures of final product is implemented for the purpose of examination and validation for proposed method. A practical design example of a production process planning for one component of an auto-circuit breaker is demonstrated. The methodology proposed in this research addresses: (1) how the product quality in production process should be modeled; (2) how to design good operations and a good process in production that can achieve the intended product quality; and (3) how to apply the proposed method to a practical planning problem of an actual production process.",2006
"Optimum Disassembly Process With Genetic Algorithms for a Compressor","This study presents an optimum disassembly process with genetic algorithms. The disassembly sequence matrix including the interference matrix, the special connection relation and the contact matrix of a product are used to study the disassembly process. The genetic algorithms are utilized for the multi-objective function formed by the instability function, tool change function, disassembly direction function and inconvenient function due to the gravity to obtain the optimum disassembly process. The disassembly processes of a compressor are investigated and improved, and a redesigned compressor is recommended.",2006
"Multiobjective Optimization for Integrated Tolerance Allocation and Fixture Layout Design in Multistation Assembly","Cost and product quality are significant attributes in manufacturing processes, such as multistation assembly. We use multiobjective optimization for integrated tolerance allocation and fixture layout design to address their interaction and to quantify tradeoffs among cost, product quality, and assembly process robustness. Design decisions relate to product tolerances, assembly process tolerances, and fixture locating positions. A nested optimization strategy is adopted, and the proposed methodology is demonstrated using a vehicle side frame assembly example. The obtained results provide evidence for the existence of tradeoffs, based on which we can identify critical quality and budget requirements.",2006
"Visibility for Determining the Rotational Axis CNC Rapid Prototyping and Rapid Manufacturing","This paper outlines the requirements for employing visibility for CNC Rapid Prototyping (RP) and Rapid Manufacturing (RM), e.g., necessary conditions for CNC RP. Visibility, as used here, is with respect to line-of-sight access for the tool to the workpiece for machining access. This method forms the basis for the selection of an axis of rotation for which the workpiece would be completely visible and a step in the automation sequence for this process. A review of prior visibility work in the area of machining is included and specifically those related to CNC RP. Additionally, there is a discussion of a new software implementation of visibility and its methodology for CNC RP. The goal of this implementation is to develop a procedure that will reduce the set of all potential workpiece rotational axes to a subset that will provide full visibility. Results are presented for the software tool along with potential improvements for the future.",2006
"Stereolithography Mock-Up With Internal Camera for Automatic Usability Data Acquisition and Functional Simulation","In this paper, we propose a new type of mock-up consisting of a transparent solid object, fabricated by stereolithography, with a small video camera embedded inside. By processing the video image of the object surface taken from the inside in real time, operation data of the mock-up (e.g., when interfaces such as buttons and dials are operated by the users) and usage (e.g., which hand and fingers are used to operate the buttons and dials of the mock-up) data can be obtained. Consequently, a simple stereolithography model without sensors or circuits can be a functional mock-up. By analyzing these automatically obtained usage data, operation data and the result of functional simulation, some usability data will be obtained or extracted. Designers can use such data to improve the usability of the product that the mock-up imitates. The feasibility and validity of the concept is confirmed and reported on the basis of three experiments: a basic experiment using a simple mock-up consisting of a button and a dial, and two detailed experiments on usability data acquisition and the simulation of a Japanese-character-input interface using stereolithography mock-ups with a small fish-eye video camera inside.",2006
"Determine Mesh Orientation by Voxel-Based Principal Component Analysis","In this paper we propose a new method to determine the part orientation of a 3D mesh based on Principal Component Analysis (PCA). Although the idea and practice of using PCA to determine part orientation is not new, it is not without practical issues. A major drawback of PCA, when it comes to dealing with meshes comprised of nodes and elements, is that the results are ",2006
"Introducing Hierarchical Particle Swarm Optimization to Optimal Part Orientation in Fused Deposition Modeling","This article presents hierarchical particle swarm optimization algorithm to determine optimal part orientation for the complex parts produced by fused deposition modeling. Owing to the importance of efficient prototyping during product development phase, it is necessary to address critical issues involved in fused deposition modeling. The best part orientation is explored by taking into account volumetric error, build cost and orientational efficiency in the form of an aggregate objective. This paper exploits the search efficiency exhibited by hierarchical particle swarm optimization (HPSO), an optimization algorithm working on the basis of swarm intelligence, with the aim to resolve underlying optimal part orientation (OPO) problem. Further, in order to establish efficacy of HPSO a comparative study has been performed with a genetic algorithm based search. The results indicate outperforming behavior of HPSO and thus it is claimed to be a viable and efficient alternative to OPO problem.",2006
"High Accuracy Laser Scanned View Registration Method for Reverse Engineering Using a CMM Generated CAD Model","Complete CAD modelling by 3D digitizers comprises of full coverage scanning of the part from a number of views in the Reverse Engineering process. Different scanned views are required to be registered into a cohesive coordinate system to generate a practical CAD model. In this paper a high accuracy registration method is introduced in which the preregistered views, obtained by a conventional registration method are used as a base for fine registration. This is based on the relationships of the scanned views in the Laser Scanned model and the scanned features created by the Coordinate Measurement Machine. This method is most suitable for Reverse Engineering of precision parts with accurate and standard machined geometric features and complex but less accurate casting features. Experimental results of the method application on two components are presented.",2006
"Using Tolerance-Maps to Generate Frequency Distributions of Clearance for Pin-Hole Assemblies","A new mathematical model for representing the geometric variations of lines is extended to include probabilistic representations of 1-D clearance which arise from multidimensional variations of an axis, a hole and a pin-hole assembly. The model is compatible with the ASME/ANSI/ISO Standards for geometric tolerances. Central to the new model is a Tolerance-Map (T-Map), a hypothetical volume of points that models the 3-D variations in location and orientation for a segment of a line (the axis), which can arise from tolerances on size, position, orientation, and form. Here it is extended to model the increase in yield that occurs when maximum material condition (MMC) is specified. The frequency distribution of 1-D clearance is decomposed into manufacturing bias, i.e. toward certain regions of a Tolerance-Map, and into a geometric bias that can be computed from the geometry of multidimensional T-Maps. Although the probabilistic representation in this paper is focused on geometric bias and manufacturing bias is presumed to be uniform, the method is robust enough to include manufacturing bias in the future. Geometric bias alone shows a greater likelihood of small clearances than large clearances between an assembled pin and hole.",2006
"Semantic Tolerance Modeling","A significant amount of research efforts has been given to explore the mathematical basis for 3D dimensional and geometric tolerance representation, analysis, and synthesis. However, engineering semantics is not maintained in these mathematic models. It is hard to interpret calculated numerical results in a meaningful way. In this paper, a new semantic tolerance modeling scheme based on modal interval is introduced to improve interpretability of tolerance modeling. With logical quantifiers, semantic relations between tolerance specifications and implications of tolerance stacking are embedded in the mathematic model. The model captures the semantics of physical property difference between rigid and flexible materials as well as tolerancing intents such as sequence of specification, measurement, and assembly. Compared to traditional methods, the semantic tolerancing allows us to estimate true variation ranges such that feasible and complete solutions can be obtained.",2006
"Geometric Construction of Piecewise Line-Symmetric Spherical Motions Using Quaternion Biarcs","This paper employs quaternion biarcs to interpolate a set of orientations with angular velocity constraints. The resulting quaternion curve represents a piecewise spherical line-symmetric rational motion with C1  continuity. Since a quaternion arc corresponds to the motion of the planet gear in a special spherical epicyclic gear train, each segment of the quaternion biarcs can be realized with such an epicyclic gear train. Quaternion biarcs may be used to approximate B-spline quaternion curves that represent rational spherical motions that have applications in robot path planning, CAD/CAM, mechanism design, and computer graphics.",2006
"Aeronautical Planar Flank Milling Automation: Computing the G-Zones","Flank milling process is commonly applied in the aeronautical industry. It consists of manufacturing mechanical parts using the side of a machinning tool. This process is relevant to be less time consuming as it delivers better surface quality. However, flank milling can only be applied on ruled surfaces. In this article, we cover flank milling application on planar surfaces, a particular ruled surface type. In recent works we presented how to extract planar surfaces milling directions by using expertise provided through our industrial application. We take this study further, where we propose a validation for the proposed milling directions. This validation requires at first a translation of the problem from 3D to 2D. Then, by applying several proposed algorithms we extract for each direction its L-Zone. An L-Zone is the term we use to identify the unmachined part area using a particular milling direction. By intersecting the different L-Zones we obtain the G-Zone that consists of the total unmachined area. Computing the G-Zone for each planar surface indicates the ability of this surface to be flank milled. The proposed study is part of an effort to automate process planning of aeronautical parts. Automating this particular trade can result in a critical reduction of time, effort and costs in aeronautical industries, mainly due to having small production batch.",2006
"Aerodynamic Shape Optimization and Knowledge Mining of Centrifugal Fans Using Simulated Annealing Coupled With a Neural Network","An aerodynamic shape optimization method suitable for “inexpensive” centrifugal impellers and diffusers has been developed. The shapes are parameterized using non-uniform rational B-spline curves with special attention being paid to the blade’s edge profiles. A hybrid algorithm combining simulated annealing and a neural network is employed for collaborative optimization. The simulated annealing and neural network take turns in controlling the optimization processes, not only for maximizing the efficiency of global exploration, but also for minimizing the risks of automation failures or of reaching an incorrect optimum. A statistical analysis was also conducted using the neural network to extract design knowledge. By applying the proposed method to a centrifugal impeller and diffuser design problem, we obtained innovative shapes for the leading edge of the impeller and the trailing edge of the diffuser. Important design parameters related to the new shapes were identified through the design space analysis.",2006
"Optimal Tolerance Allocation of Automotive Pneumatic Control Valves Based on Product and Process Simulations","This paper discusses a computational method for optimally allocating dimensional tolerances for an automotive pneumatic control valve. Due to the large production volume, costly tight tolerances should be allocated only to the dimensions that have high influence to the quality. Given a parametric geometry of a valve, the problem is posed as a multi-objective optimization with respect to product quality and production cost. The product quality is defined as 1) the deviation from the nominal valve design in the linearity of valve stroke and fluidic force, and 2) the difference in fluidic force with and without cavitation. These quality measures are estimated by using Monte Carlo simulation on a Radial-Basis Function Network (RBFN) trained with computational fluid dynamics (CFD) simulation of the valve operation. The production cost is estimated by the tolerance-cost relationship obtained from the discrete event simulations of valve production process. A multi-objective genetic algorithm is utilized to generate Pareto optimal tolerance allocations with respect to these objectives, and alternative tolerance allocations are proposed considering the trade-offs among multiple objectives.",2006
"Optimization of Chemical Vapor Deposition Process","Chemical Vapor Deposition (CVD) process is simulated and optimized for the deposition of a thin film of silicon from silane. The key focus is on the rate of deposition and on the quality of the thin film produced. The intended application dictates the level of quality need for the film. Proper control of the governing transport processes results in large area film thickness and composition uniformity. A vertical impinging CVD reactor is considered. The goal is to optimize the CVD system. The effect of important design parameters and operating conditions are studied using numerical simulations. Then Compromise Response Surface Method (CRSM) is used to model the process over a range of susceptor temperature and inlet velocity of the reaction gases. The resulting response surface is used to optimize the CVD system.",2006
"Data Mining and Fuzzy Clustering to Support Product Family Design","In mass customization, data mining can be used to extract valid, previously unknown, and easily interpretable information from large product databases in order to improve and optimize engineering design and manufacturing process decisions. A product family is a group of related products based on a product platform, facilitating mass customization by providing a variety of products for different market segments cost-effectively. In this paper, we propose a method for identifying a platform along with variant and unique modules in a product family using data mining techniques. Association rule mining is applied to develop rules related to design knowledge based on product function, which can be clustered by their similarity based on functional features. Fuzzy c-means clustering is used to determine initial clusters that represent modules. The clustering result identifies the platform and its modules by a platform level membership function and classification. We apply the proposed method to determine a new platform using a case study involving a power tool family.",2006
"Knowledge Model for Managing Product Variety and Its Reflective Design Process","Recent manufacturers have been utilizing product families to diversify and enhance the product performance by simultaneously designing multiple products under commonalization and standardization. Design information of product architecture and family is inevitably more complicated and numerous than that of a single product. Thus, more sophisticated computer-based support system is required for product architecture and family design. This paper proposes a knowledge model for a computer-based system to support reflective process of designing product architecture and product family. This research focuses on three problems which should be overcome when product family are modeled in the computer system; design repository without data redundancy and incorrectness, knowledge acquisition without forcing the additional effort on the designer, and integration of prescriptive models to support early stages of the design process. An ontology that is a foundation of a knowledge model is defined to resolve these problems. An example of designing an air conditioner product family is shown to demonstrate the capability of the system.",2006
"Design Information Support Tool (DIST): Its Development and Effectiveness Investigation","A number of software tools exist to assist the designer during the design process. These include tools for solid modeling of components, programs for simulating complex systems, for generating machining code and so on. However, a closer examination reveals that most of these tools are of use in the later stages in the design process. Even though design activity analyses showed the initial phases of design to have the maximum impact on the successful design of a product, few tools exist to support the needs of the designer during these critical stages. This paper documents the development of a design support tool (DIST) based on an analysis of a collaborative design activity. Then, results of an experiment are shared, which was designed to investigate DIST’s effectiveness during conceptual design by novice designers.",2006
"Automating Redesign of Sheet-Metal Parts in Automotive Industry Using KBE and CBR","Automating redesign is an approach for engineering designers to prevent design related manufacturability problems in early product development and thus reduce costly design iterations. A vast amount of work exists, with most research findings seemingly staying within the research community rather than finding its way into use in industrial settings where research issues have often evolved from the concerned applied research. The aim of this paper is to present an approach with industrial implementation potential regarding automating redesign of sheet-metal components in early product development to avoid manufacturing problems due to design flaws and non-optimal designs. Geometry, generated by a knowledge-based engineering (KBE) system, gives input to the case-based reasoning (CBR) governed manufacturing planning. If geometry is found non-manufacturable or enhancement of already manufacturable geometry is possible, the CBR system will suggest redesign actions to resolve the problem. CBR extends the capabilities of the rule-based KBE-system by enabling plan-based evaluation. The approach has the potential for industrial implementation, since KBE is often closely coupled to an industrial CAD-system, hence enabling technology is at the industry. Also, combining KBE and CBR reduces the coding effort compared to coding the whole design support with CBR, as feature recognition is simplified by means of KBE. A case study of development of sheet-metal manufactured parts at a Swedish automotive industry partner presents the method in use. As it is shown that redesign can be automated for sheet-metal parts there is a potential for reducing costly design and manufacturing iterations.",2006
"Verve: A General Purpose Open Source Reinforcement Learning Toolkit","Intelligent agents are becoming increasingly important in our society in applications as diverse as house cleaning robots, computer-controlled opponents in video games, unmanned aerial combat vehicles, entertainment robots, and autonomous explorers in outer space. However, the broader adoption of intelligent agents is often hindered by their limited adaptability to new tasks; when conditions change slightly, agents may quickly become confused. Additionally, a substantial engineering effort is required to design an agent for each new task. This paper presents an adaptable, general purpose intelligent agent toolkit based on reinforcement learning (RL), an approach with strong mathematical foundations and intriguing biological implications. RL algorithms are powerful because of their generality: agents simply receive a scalar reward value representing success or failure, which greatly simplifies the agent design process. Furthermore, these algorithms can be combined with other techniques (e.g., planning from a learned internal model) to improve learning efficiency. The design and implementation of an open source RL toolkit is presented here as a step towards the goal of general purpose agents. Experimental results show learning performance on several tasks, including two physical control problems.",2006
"Augmenting Tools for Reverse Engineering Methods","Reverse engineering has gained importance over the past few years due to an intense competitive market aiding in the survivability of a company. This paper examines the reverse engineering process and what, how, and why it can assist in making a better design. Two well known reverse engineering methodologies are explored, the first by Otto and Wood and the second by Ingle. Each methodology is compared and contrasted according to the protocols and tools used. Among some of the reverse engineering tools detailed and illustrated are: Black box, Fishbone, Function Structure, Bill of Material, Exploded CAD models, Morphological Matrix, Subtract and Operate Procedure (SOP), House of Quality matrix, and FMEA. Even though both methodologies have highly valued tools, some of the areas in reverse engineering need additional robust tooling. This paper presents new and expanded tooling to augment the existing methods in hopes of furthering the understanding of the product, and process. Tools like Reverse Failure Mode and Effects Analysis (RFMEA), Connectivity graphs, and inter-relation matrix increase the design efficiency, quality, and the understanding of the reverse engineering process. These tools have been employed in two industry projects and one demonstrative purpose for a Design for Manufacture Class. In both of these scenarios, industry and academic, the users found that the augmented tools were useful in capturing and revealing information not previously realized.",2006
"Qualitative and Quantitative Sequential Sampling","This paper introduces a method for sequentially determining experiments in a “design of experiments” where optimization and user knowledge are used to guide the efficient choice of sample points. Typical approaches to the design of experiments involves determining the sample points all at once prior to any experimentation, or sequentially based on the results of previous sample points. This method combines information from multiple fidelity sources including actual physical experiment, computer simulation models of the product, first principals involved in design and designer’s qualitative intuitions about the design. Both quantitative and qualitative information from different sources are merged together to arrive at new sampling strategy. This is accomplished by introducing the concept of confidence, C, which is represented as a field that is a function of the decision variables, x, and the performance parameter, f. The advantages of the approach are demonstrated using different example cases.",2006
"On Adaptive Sampling for Single and Multi-Response Bayesian Surrogate Models","In order to reduce the time and resources devoted to design-space exploration during simulation-based design and optimization, the use of surrogate models, or metamodels, has been proposed in the literature. Key to the success of metamodeling efforts are the experimental design techniques used to generate the combinations of input variables at which the computer experiments are conducted. Several adaptive sampling techniques have been proposed to tailor the experimental designs to the specific application at hand, using the already-acquired data to guide further exploration of the input space, instead of using a fixed sampling scheme defined a priori. Though mixed results have been reported, it has been argued that adaptive sampling techniques can be more efficient, yielding better surrogate models with less sampling points. In this paper, we address the problem of adaptive sampling for single and multi-response metamodels, with a focus on Multi-stage Multi-response Bayesian Surrogate Models (MMBSM). We compare distance-optimal latin hypercube sampling, an entropy-based criterion and the maximum cross-validation variance criterion, originally proposed for one-dimensional output spaces and implemented in this paper for multi-dimensional output spaces. Our results indicate that, both for single and multi-response surrogate models, the entropy-based adaptive sampling approach leads to models that are more robust to the initial experimental design and at least as accurate (or better) when compared with other sampling techniques using the same number of sampling points.",2006
"A Kriging Metamodel Assisted Multi-Objective Genetic Algorithm for Design Optimization","The high computational cost of population based optimization methods, such as multi-objective genetic algorithms, has been preventing applications of these methods to realistic engineering design problems. The main challenge is to devise methods that can significantly reduce the number of computationally intensive simulation (objective/constraint functions) calls. We present a new multi-objective design optimization approach in that kriging-based metamodeling is embedded within a multi-objective genetic algorithm. The approach is called Kriging assisted Multi-Objective Genetic Algorithm, or K-MOGA. The key difference between K-MOGA and a conventional MOGA is that in K-MOGA some of the design points or individuals are evaluated by kriging metamodels, which are computationally inexpensive, instead of the simulation. The decision as to whether the simulation or their kriging metamodels to be used for evaluating an individual is based on checking a simple condition. That is, it is determined whether by using the kriging metamodels for an individual the non-dominated set in the current generation is changed. If this set is changed, then the simulation is used for evaluating the individual; otherwise, the corresponding kriging metamodels are used. Seven numerical and engineering examples with different degrees of difficulty are used to illustrate applicability of the proposed K-MOGA. The results show that on the average, K-MOGA converges to the Pareto frontier with about 50% fewer number of simulation calls compared to a conventional MOGA.",2006
"Review of Metamodeling Techniques in Support of Engineering Design Optimization","Computation-intensive design problems are becoming increasingly common in manufacturing industries. The computation burden is often caused by expensive analysis and simulation processes in order to reach a comparable level of accuracy as physical testing data. To address such a challenge, approximation or metamodeling techniques are often used. Metamodeling techniques have been developed from many different disciplines including statistics, mathematics, computer science, and various engineering disciplines. These metamodels are initially developed as “surrogates” of the expensive simulation process in order to improve the overall computation efficiency. They are then found to be a valuable tool to support a wide scope of activities in modern engineering design, especially design optimization. This work reviews the state-of-the-art metamodel-based techniques from a practitioner’s perspective according to the role of metamodeling in supporting design optimization, including model approximation, design space exploration, problem formulation, and solving various types of optimization problems. Challenges and future development of metamodeling in support of engineering design is also analyzed and discussed.",2006
"An Improved Genetic Algorithm for the Optimization of Composite Structures","This paper describes a new approach for reducing the number of the fitness and constraint function evaluations required by a genetic algorithm (GA) for optimization problems with mixed continuous and discrete design variables. The proposed modification improves the efficiency of the memory constructed in terms of the continuous variables. The work presents the algorithmic implementation of the proposed memory scheme and demonstrates the efficiency of the proposed multivariate approximation procedure for the weight optimization of a segmented open cross section composite beam subjected to axial tension load. Results are generated to demonstrate the advantages of the proposed improvements to a standard genetic algorithm.",2006
"Surrogate Model Updating Using Clustering in a Genetic Algorithm Setup","This paper addresses the critical issue of fidelity in simulation-based design optimization using preference-based surrogate models. Specifically, it presents an integrated clustering-based updating procedure in a genetic algorithm setup to iteratively improve the efficacy of Kriging models. A potential drawback of using preference-based surrogate models in simulation based design is that the surrogates may misrepresent the true optima if the model building schemes fail to capture the critical points of interest with enough fidelity or clarity. This work addresses this vulnerability and presents an efficient clustering-technique integrated surrogate model updating procedure that can capture the buried, transient, yet inherent data pattern in the evolution progression of design candidates within a genetic algorithm setup, and screen out distinct optimal points for subsequent sequential model validation and updating. The results show that the successful finding of the true optimal design through cost-effective surrogate-based optimization depends not only on the selection of sampling schemes such as sample rate and distribution in the initial surrogate model build-up, but also on an efficient and reliable updating procedure that can prevent suboptimal decisions.",2006
"Exploration in the Preliminary Mechanical Design of Trade-Offs Between Automotive Architectural Constraints and Aggregate Noise Performances","This paper deals with the preliminary design of large complex mechanical products. It gives a new way to lead tradeoffs between architectural constraints and mechanical performances. While topology optimization and structural optimization have been widely developed in the last two decades [1], they are still not adapted to take into account the problem of volume allocation in the preliminary design of large and complex mechanical systems as automotive vehicles and aircrafts. New approximate criteria to assess the compliance of architectural constraints and important mechanical performances like contributions to noise are proposed. These criteria are assembled within a sole performance metamodel that embeds most of decision issues usually discussed in the preliminary design stage and which allows to encompass the strict traditional volume allocation process. Through the use of Pareto techniques, we show that it is convenient and relevant to explore the design space by allowing great variation of the design morphology. The trade-offs are made possible since the stage of volume allocation and automotive architects can be helped by tools for leading quantitative negotiations in preliminary design.",2006
"Heterogeneous Design Optimization From the Microstructure","Designers have for some time had powerful tools that allow optimization of geometric design using, for example, parametric design techniques. However, these optimization strategies utilize an incomplete model in the design process. An assumption often made throughout the design process is that the material properties are homogeneous. However, it is recognized that engineered materials may have significant non-homogeneity, which often manifests itself in an undesirable manner (e.g. in the heat affected zone). In an ideal world, the non-homogeneity could be used to improve a design. A designer would have full control over the heterogeneous design of a product, being enabled to optimize both the material properties and geometry concurrently. Recent developments in microstructure sensitive design (MSD) have now made the problem of optimizing over both geometry and material structure accessible. The inverse problem of designing material to achieve desired properties is being tackled. The latest techniques allow a designer to search the property closures of materials for the optimum theoretical microstructures. Furthermore, work has started in terms of linking the space of theoretical microstructures with manufacturing techniques that are currently available for manipulation of the microstructure of polycrystalline materials. This paper demonstrates how the latest available MSD tools may be applied to the design process. A curved rod is optimized to maintain certain critical physical properties, while meeting certain geometrical criteria. The result is a local reduction in radius of 20% without significant reduction in properties. While the manufacturability of this particular case study is not specifically addressed, the potential future directions for such design tools are discussed and some practicalities in terms of bridging the gap between theoretical microstructures and available materials are reviewed.",2006
"Parametric Structural Shape and Topology Optimization Method With Radial Basis Functions and Level-Set Method","In this paper, a parametric structural shape and topology optimization method is presented. To solving structure optimization problems, the level-set method has become a powerful design tool and been widely used in many fields. Combined with the Radial Basis Functions (RBF), which is a popular tool in function approximation, the method of level-set can be represented in a parametric way with a set of advantages comparing with the conventional discrete means. Some numerical examples are presented to illustrate its advantages.",2006
"FEA-Based Design of Heterogeneous Objects","Finite Element Analysis (FEA) is an important step for the design of structures or components formed by heterogeneous objects such as multi-materials, Functionally Graded Materials (FGMs), etc. The main objective of the FEA-based design of heterogeneous objects is to simultaneously optimize both geometry and material distribution over the design domain (e.g., Homogenization Design Method). However, the accuracy of the FEA-based design wholly depends on the quality of the finite element models. Therefore, there exists an increasing need for generating finite element models adaptive to both geometric complexity and material distribution. This paper introduces a method for FEA-based design of heterogeneous objects. At the design stage, a heterogeneous solid model is first created by referring to the libraries of primary materials and composition functions that are already available in the field of material science. The heterogeneous solid model is then discretized into an object model onto which appropriate material properties are mapped. Discretization converts continuous material variations inside an object into stepwise variations. Next, the object model is adaptively meshed and converted into a finite element model. The meshing algorithm first creates nodes on the iso-material curves (or surfaces) of heterogeneous solid models. Triangular (or tetrahedral) meshes are then generated inside each iso-material region formed by iso-material curves (or surfaces). FEA using commercial software is finally performed to estimate stress levels. This FEA-based design cycle is repeated until a satisfactory solution is obtained. If the design objective is satisfactory, the object model is fed to the fabrication system where a process planning is performed to create instructions for LM machines. An example (FGM pressure vessel) is shown to illustrate the entire FEA-based design cycle.",2006
"Material Property Identification and Sensitivity Analysis Using Indentation and FEM","Mechanical properties of materials in small-scale applications, such as thin coatings, are often different from those of bulk materials due to the difference in the manufacturing process. Indentation has been a convenient tool to study the mechanical properties in such applications. In this paper, a numerical technique is proposed that can identify the mechanical properties by minimizing the difference between the results from indentation experiments and those from finite element analysis. First, two response surfaces are constructed for loading and unloading curves from the indentation experiment of a gold film on the silicon substrate. Unessential coefficients of the response surface are then removed based on the test statistics. Different from the traditional methods of identification, the tip geometry of the indenter is included because its uncertainty significantly affects the results. In order to validate the accuracy and stability of the method, the sensitivity of the identified material properties with respect to each coefficient is analyzed.",2006
"Topology Optimization of Micromachined Structures With Surface Micromachining Manufacturing Constraints","Topology optimization methods have been shown to have extensive application in the design of microsystems. However, their utility in practical situations is restricted to predominantly planar configurations due to the limitations of most microfabrication techniques in realizing structures with arbitrary topologies in the direction perpendicular to the substrate. This study addresses the problem of synthesizing optimal topologies in the out-of-plane direction while obeying the constraints imposed by surface micromachining. A new formulation that achieves this by defining a design space that implicitly obeys the manufacturing constraints with a continuous design parameterization is presented in this paper. This is in contrast to including manufacturing cost in the objective function or constraints. The resulting solutions of the new formulation obtained with gradient-based optimization directly provide the photolithographic mask layouts. Two examples that illustrate the approach for the case of stiff structures are included.",2006
"Multifunctional Topology Design of Cellular Material Structures","Prismatic cellular or honeycomb materials exhibit favorable properties for multifunctional applications such as ultra-light load bearing combined with active cooling. Since these properties are strongly dependent on the underlying cellular structure, design methods are needed for tailoring cellular topologies with customized multifunctional properties that may be unattainable with standard cell designs. Topology optimization methods are available for synthesizing the form of a cellular structure—including the size, shape, and connectivity of cell walls and the number, shape, and arrangement of cell openings—rather than specifying these features ",2006
"Designing Design Processes for Integrated Materials and Products Realization: A Multifunctional Energetic Structural Material Example","We present an approach for the integrated design of materials, products, and design processes. The approach is based on the use of reusable interaction patterns to model design processes, and the consideration of design process decisions using the value of information metrics. The approach is presented using a multifunctional energetic structural materials (MESM) design example. The design objectives in the example include sufficient strength and energy release capabilities. The design is carried out by using simulation models at different scales that model different aspects of the system. Preliminary results from the application of the approach to the MESM design problem are discussed. In this paper, we show that the integrated design of materials and products can be carried out more efficiently by considering the design of design processes.",2006
"Exploring the Advantages of Materials Design in a Product Design Process","In this paper, we explore the benefits of materials design in a product design process. We also compare the methods of material selection and materials design by demonstrating two examples—the design of a cantilever beam for minimum weight and the design of a fan blade for minimum weight. The design of the cantilever beam is carried out using Ashby’s material selection method as well as a proposed method for materials design. The design of the fan blade and its material is completed using computational tools. Our goal in this paper is to demonstrate the benefits of materials design over material selection methods and to illustrate the flexibility inherent in materials design processes. We are more interested in revealing the possibilities of materials design, rather than the specific results from the example problems. The investigation of materials design presented in this paper moves us one step closer towards the realization of a systematic, inductive method for the concurrent design of products and materials.",2006
"Free-Form Feature Representation and Comparison","Knowledge Management systems have been applied to several areas in the manufacturing industry. These applications have been predominately text based, however, much of manufacturing’s information is related to geometric features. In this paper we describe a representation and content-based search method such that given a geometric specification, the knowledge of previous designs having similar feature requirements is returned. The technique uses curvature based shape index segmentation to construct an attributed graph-like descriptor for 2.5D free-form surface features, such as those found on stamped components. Efficient edit distance methods are applied for search. Experiments on a database of freeform features indicate feasible runtimes with favourable results.",2006
"Parametric Solid Modeling","Parametric modeling systems are fundamentally changing the design process practiced in the industry today. Practically all commercial CAD systems combine established solid modeling techniques with constraint solving and heuristic algorithms to create, edit and manipulate solid models, while enforcing the requirement that every such solid model must maintain the validity of the prescribed geometric constraints. However, a number of fundamental (open) problems limit the functionality and performance of these parametric modeling systems. For example, the allowable parametric changes are ",2006
"Poisson Based Reuse of Freeform Features","Due to the complexity, reuse of freeform features represented by B-spline is still an open issue. Based on Poisson equation, a novel approach to the reuse of freeform features represented by uniform rational B-spline (URBS) is proposed in this paper. In order to effectively support reuse, a new representation of freeform features is put forwarded, which is based on a new property about the odd URBS and consists of the principal control points, geometry context and basic surface of the feature. Based on the new representation, reuse of freeform features is achieved by updating the principal control points on the pasted area of the target surface using Poisson equation. The approach is implemented and some examples are given.",2006
"Parametric and Topological Control in Shape Optimization","We propose a novel approach to shape optimization that combines and retains the advantages of the earlier optimization techniques. The shapes in the design space are represented implicitly as level sets of a higher-dimensional function that is constructed using B-splines (to allow free-form deformations), and parameterized primitives combined with R-functions (to support desired parametric changes). Our approach to shape design and optimization offers great flexibility because it provides explicit parametric control of geometry and topology within a large space of freeform shapes. The resulting method is also general in that it subsumes most other types of shape optimization as special cases. We describe an implementation of the proposed technique with attractive numerical properties. The effectiveness of the method is demonstrated by several numerical examples.",2006
"Topology Synthesis and Classification of Low-Mobility Parallel Mechanisms","Topology synthesis of low-mobility parallel mechanisms is an important direction of mechanism research. At present, various systematic methods for topology synthesis have been proposed, and large numbers of new mechanisms that satisfy the motion requirements have been synthesized through step-by-step deducing. However, some fundamental problems are ignored. In this paper, some significant synthesis methods are compared from the aspects of the description of output character, limb structure synthesis and geometrical relationship between limbs. The commonly existent problems are analyzed, including the strict description of the output character of the moving platform and the instantaneous mechanisms in the process of topology synthesis. The limitation of existent methods is also indicated. Moreover, a classifying method for low-mobility parallel mechanisms from the viewpoint of topology synthesis is proposed. This classification has guiding effect for synthesis, analysis and application of parallel mechanisms.",2006
"Coupling Degree of Mechanism and Its Application","The kinematic and dynamic analysis of an spatial multi-loop mechanism especially parallel mechanism is significant but always complex. Based on the topological structure of mechanisms, this paper proposes the concept of coupling degree of mechanism systematically, and applies it to the criterion of basic kinematic chains(BKCs) and other problems. The relation between topology, kinematics and dynamics of parallel mechanisms is established, and then it is achieved to quantitatively describe the analysis complexity of a parallel mechanism and to obtain its simplest solving path, according to its topological structure. The preliminary method for unified modeling of the topology, kinematics and dynamics of parallel mechanisms is proposed, using BKC as the basic analysis unit. Some suggestions for optimization and selective preference of parallel mechanisms are also presented.",2006
"A Survey of Various Encoding Schemes and Associated Placement Algorithms Applied to Packing and Layout Problems","In this paper, we investigate the current state-of-the-art in packing algorithms. The focus of this survey is on the different types of encoding schemes and associated placement techniques used to represent the layout of a set of objects. The encoding schemes are investigated with respect to their suitability to different types of packing problems, specific scenarios where a given representation may outperform others and their limitations. The different types of placement algorithms that can be used with a given encoding are described. Some common desirable characteristics that an encoding scheme should follow are also discussed. Finally a qualitative comparison of the various encoding schemes is presented to help in selecting a specific representation based on a set of criteria.",2006
"Vehicle Component Layout With Shape Morphing: An Initial Study","This work focuses on incorporating component shape design into a vehicle configuration design or layout process. A concurrent design process consisting of performing layout design and simultaneous shape morphing of some select components is adopted to replace the traditional sequential design approach. The objective is to improve design efficiency and reduce design cost. Two important issues in the packing optimization with shape morphing problem are identified and studied: the morphing and the optimization. A parameterization-based morphing method and a mesh-based morphing method are implemented, and their advantages and disadvantages are discussed. To efficiently solve this complex problem, it is proposed to decompose it into a bi-level formulation: system level and component level. At the system level, the given functional objectives of the layout design problem are optimized with respect to component positions and orientations. At the component level, the shape of select components is morphed to minimize the overlap with other objects and the enclosure. By iterating between these two levels, the original problem is solved. This bi-level approach is intended to overcome the complexity of performing the placement simultaneously with the shape morphing.",2006
"Simultaneous Geometry Optimization and Material Selection for Truss Structures","In this work, we explore simultaneous design and material selection by posing it as an optimization problem. The underlying principles for our approach are Ashby’s material selection procedure and structural optimization. For the simplicity and ease of initial implementation of the general procedure, truss structures under static load are considered in this work in view of maximum stiffness, minimum weight/cost and safety against failure. Along the lines of Ashby’s material indices, a new design index is derived for trusses. This helps in choosing the most suitable material for any design of a truss. Using this, both the design space and material database are searched simultaneously using optimization algorithms. The important feature of our approach is that the formulated optimization problem is continuous even though the material selection is an inherently discrete problem.",2006
"A Two-Stage Design Method for Compliant Mechanisms Having Specified Non-Linear Output Paths","Compliant mechanisms generated by traditional topology optimization methods have linear output response, and it is difficult for traditional methods to implement mechanisms having non-linear output responses, such as nonlinear deformation or path. To design a compliant mechanism having a specified nonlinear output path, a two-stage design method based on topology and shape optimization is constructed here. In the first stage, topology optimization generates an initial and conceptual compliant mechanism based on ordinary design conditions, with “additional” constraints that are used to control the output path at the second stage. In the second stage, an initial model for the shape optimization is created, based on the result of the topology optimization, and the additional constraints are replaced by spring elements. The shape optimization is then executed, to generate a detailed shape of the compliant mechanism having the desired output path. In this stage, parameters that represent the outer shape of the compliant mechanism and the properties of spring elements are used as design variables in the shape optimization. In addition to configuration of the specified output path, executing the shape optimization after the topology optimization also makes it possible to consider the stress concentration and large displacement effects. This is an advantage offered by the proposed method, since it is difficult for traditional methods to consider these aspects, due to inherent limitations of topology optimization.",2006
"A Multicriteria System-Based Method for Simulation-Driven Design Synthesis","This paper presents an algorithmic, physics-based, design synthesis method aimed at facilitating synthesis through automated generation of a range of feasible and optimally directed design alternatives. The method assists designers in the exploration of performance limits and trade-offs for synthesis tasks as well as reducing design time through rapid, computational generation. The method introduced combines a multicriteria generate-and-test search algorithm, called Burst, with a Connected Node System (CNS) design representation and provides automatic links to multiphysics simulation for quantitative evaluation of design performance throughout the synthesis process. The CNS-Burst method is applied to two benchmark synthesis tasks in the domain of MEMS to validate the method. The solutions generated meet the modeled design requirements and the variety of designs generated offers designers the possibility of selecting devices according to their preferences among performance trade-offs. The potential for extension to larger, more complex MEMS design synthesis and optimization tasks is discussed.",2006
"Design and Verification of a New Computer Controlled Seating Buck","Appraising vehicle package design concepts using seating bucks — physical prototypes representing vehicle package, is an integral part of the vehicle package design process. Building such bucks is costly and may impose substantial burden on the vehicle design cycle time. Further, static seating bucks lack the flexibility to accommodate design iterations during the gradual progression of a vehicle program. A “Computer controlled seating buck”, as described in this paper, is a quick and inexpensive alternative to the traditional seating bucks with the desired degree of fidelity. It is particularly useful to perform package and ergonomic studies in the early stages of a vehicle program, long before the data is available to build a traditional seating buck. Such a seating buck has been developed to accommodate Ford vehicle package design needs. This paper presents the functional requirements, the high level conceptual design of how these requirements are realized, and the methods to verify, improve and sustain the dimensional accuracy and capability of the new computer controlled seating buck.",2006
"An Efficient Method for Detecting Packaging Supporting Space","This paper presents an efficient method to generate packaging space that can be used to create molded foam. This method first reduces the number of facets presenting the product and then detects heights of supporting lines from an evenly spaced mesh grid on the bottom space of a pre-defined bounding box. The height of each supporting line is further relaxed and smoothed to form the final supporting surface. Based on the supporting surface, an STL format is created so it can be fabricated using RP machines. Three examples are presented to demonstrate this method.",2006
"Topology Optimization Considering Gravitational and Centrifugal Forces","In this paper, topology optimization problems with two types of body force are considered: gravitational force and centrifugal force. For structural design under both external and gravitational forces, a total mean compliance formulation is used to produce the stiffest structure. For rotational structural design with high angular velocity, one additional design criteria, kinetic energy, is included in the formulation. Sensitivity analyses of the total mean compliance and kinetic energy are derived. Finally, design examples are presented and compared to show the effects of body forces on the optimized results.",2006
"BB-ATC: Analytical Target Cascading Using Branch and Bound for Mixed-Integer Nonlinear Programming","The analytical target cascading (ATC) methodology for optimizing hierarchical systems has demonstrated convergence properties for continuous, convex formulations. However, many practical problems involve both continuous and discrete design variables, resulting in mixed integer nonlinear programming (MINLP) formulations. While current ATC methods have been used to solve such MINLP formulations in practice, convergence properties have yet to be formally addressed, and optimality is uncertain. This paper describes properties of ATC for working with MINLP formulations and poses a solution method applying branch and bound as an outer loop to the ATC hierarchy in order to generate optimal solutions. The approach is practical for large hierarchically decomposed problems with relatively few discrete variables.",2006
"Applying the Mahalanobis-Taguchi System to Vehicle Handling","The Mahalanobis Taguchi System (MTS) is a diagnosis and forecasting method for multivariate data. Mahalanobis Distance (MD) is a measure based on correlations between the variables and different patterns that can be identified and analyzed with respect to a base or reference group. The MTS is of interest because of its reported accuracy in forecasting from small, correlated data sets. This is the type of data that is encountered with consumer vehicle ratings. MTS enables a reduction in dimensionality and the ability to develop a scale based on MD values. MTS identifies a set of useful variables from the complete data set with equivalent correlation and considerably less time and data. This paper presents the application of the MTS, its applicability in identifying a reduced set of useful variables in multidimensional systems.",2006
"Model-Based Decomposition Using Non-Binary Dependency Analysis and Heuristic Partitioning Analysis","The two-phase method for model-based decomposition (Chen et al. 2005a) has two major functional components: dependency analysis and partitioning analysis. The functions of these two components are enhanced and generalized in this paper in order to improve the method’s capability. On the one hand, the non-binary dependency analysis is developed such that the two-phase method can handle both binary and non-binary dependency information of the model. The essence of this development is to properly select a resemblance coefficient for the quantification of couplings among the model’s elements. On the other hand, as the past version of partitioning analysis takes the enumerative approach to search decomposition solutions, the heuristic partitioning analysis is developed as an alterative to search a reasonably good solution in a shorter time. The working principle of the heuristic approach is to analyze the coupling structure of the model such that the ",2006
"Product Optimization Incorporating Discrete Design Variables Based on Decomposition of Performance Characteristics","In order to obtain superior design solutions, the largest possible number of design alternatives, often expressed as discrete design variables, should first of all be considered, and the best design solution should then be selected from this wide set of alternative designs. Also, product designs should be initiated from the earliest possible stages, such as the conceptual and fundamental design stages, when discrete rather than continuous design variables have primacy. Although the use of discrete design variables is fundamentally important, this has implications in terms of computational demands and the accuracy of the optimized solution. This paper proposes an optimization method for product designs incorporating discrete design variables, in which hierarchical product optimization methodologies are constructed based on decomposition of characteristics and/or extraction of simpler characteristics. The optimizations are started at the lowest levels of the hierarchical optimization structure, and proceed to the higher levels. The discrete design variables are efficiently selected and optimized as smaller sub-optimization problems at the lowest hierarchical levels, while the optimum solutions for the entire problem are obtained by conventional mathematical programming methods. Practical optimization procedures for machine product optimization problems having several types of discrete design variables are constructed, and some applied examples demonstrate their effectiveness.",2006
"Development of Distributed Computing Framework for Parallel Multi-Disciplinary Optimization","This paper discusses the detailed design and development of a web based parallel multi-disciplinary optimization (PMDO) framework in the distributed computing environment. This system consists of the ",2006
"Ontologies for Supporting Engineering Design Optimization","This paper presents an optimization ontology and its implementation into a prototype computational knowledge base tool dubbed ONTOP  (ONT ology for OP timization). Salient features of ONTOP include a knowledge base which incorporates both standardized optimization terminology, formal method definitions, and often unrecorded optimization details, such as any idealizations and assumptions that may be made when creating an optimization model, as well as the model developer’s rationale and justification behind these idealizations and assumptions. ONTOP  was developed using Protégé, a Java-based, free open-source ontology development environment created by Stanford University. Two engineering design optimization case studies are presented. The first case study consists of the optimization of a structural beam element and demonstrates ONTOP ’s ability to address the variations in an optimal solution that may arise when different techniques and approaches are used. A second case study, a more complex design problem which deals with the optimization of an impeller of a pediatric left ventricular heart assist device, demonstrates the wealth of knowledge ONTOP is able to capture. Together, these test beds help illustrate the potential value of an ontology in representing application-specific knowledge while facilitating both the sharing and exchanging of this knowledge in engineering design optimization.",2006
"Two-Stage System of Systems Model by Linking System Design With Resource Allocation","Decomposition-based product design optimization under system of systems paradigm is linked with resource (i.e., product) allocation. A two-stage, system of systems approach to linking resource allocation (e.g., vehicle routing problem (VRP)) and system design optimization (e.g., vehicle design problem (VDP)) is presented. The problem inherently contains discrete variables from VRP, thus a practical formulation is presented to overcome convergence difficulty associated with shared discrete variables in a decomposed setting. Two examples, composed of four and eight air routes respectively with the introduction of a new aircraft to the existing fleet, are presented to demonstrate the effectiveness of the proposed approach. A new type of aircraft is designed and allocated to the currently existing VRP to meet the demand, while the direct operating cost of an airline is minimized.",2006
"Optimization of a PEM Fuel Cell System for Low-Speed Hybrid Electric Vehicles","Design optimization is performed by presenting a systematic method to obtain the optimal operating conditions of a Proton Exchange Membrane (PEM) fuel cell system targeted towards a vehicular application. The fuel cell stack model is a modified version of the semi-empirical model introduced by researchers at the Royal Military College of Canada and one that is widely used by industry. Empirical data obtained from tests of PEM fuel cell stacks are used to determine the empirical parameters of the fuel cell performance model. Based on this stack model, a fuel cell system model is built in MATLAB. Included in the system model are heat transfer and gas flow considerations and the associated Balance of Plant (BOP) components. The modified ADVISOR vehicle simulation tool is used to integrate the New York City Cycle (NYCC) drive cycle and vehicle model to determine the power requirements and hence the load cycle of the fuel cell system for a low-speed fuel cell hybrid electric vehicle (LSFCHEV). The optimization of the powerplant of this vehicle type is unique. The vehicle model has been developed in the work to describe the characteristics and performance of an electric scooter, a simple low-speed vehicle (LSV). The net output power and system exergetic efficiency of the system are maximized for various system operating conditions using the weighted objective function based on the load cycle requirement. The method is based on the coupling of the fuel cell system model with three optimization algorithms (a) sequential quadratic programming (SQP); (b) simulated annealing (SA); and (c) genetic algorithm (GA). The results of the optimization provide useful information that will be used in future study on control algorithms for LSFCHEVs. This study facilitates research on more complex fuel cell system modeling and optimization, and provides a basis for experimentation to verify the fuel cell system model.",2006
"A Multidisciplinary and Multiobjective System Analysis and Optimization Methodology for Embedding Integrated Systems Health Management (ISHM) Into NASA’s Complex Systems","NASA’s future space exploration systems will include a highly complex Integrated Systems Health Management (ISHM) capability, which can detect, predict, isolate and respond to system and component failures in order to improve safety and maintainability. An ISHM system, as a whole, consists of several subsystems that monitor different components of a space mission. Due to the complex and multidisciplinary nature of designing ISHM, there seems to be a lack of formal methodologies to design an optimal (or near-optimal) ISHM for a given system of systems. In this research, we propose a new methodology to design and optimize ISHM as a distributed system with multiple interacting disciplines as well as multiple conflicting design objectives (i.e. Figures Of Merit or FOMs). This specialized multidisciplinary design approach can be used to optimize the effectiveness of ISHM systems for future NASA missions. We assume a hierarchical design protocol, where each subsystem communicates with other subsystems only in a top-down tree structure. At the top level, the overall performance of the mission consists of system-level variables, parameters, objectives, and constraints that are shared throughout the system and by all subsystems. Each subsystem will then comprise of these shared values in addition to those values that are specific to subsystems. As a specific case study, we take the example of designing an ISHM capability for X-34 reusable launch vehicle in two levels. The proposed approach, referred to as ISHM Multidisciplinary and Multiobjective System Analysis & Optimization (or ISHM MMSA&O), has a hierarchical structure to pass up or down shared values between the two levels with system-level and subsystem-level optimization routines.",2006
"Flexible Product Platforms: Framework and Case Study","Customization and market uncertainty require increased functional and physical bandwidth in product platforms. This paper presents a platform design process in response to such future uncertainty. The process consists of seven iterative steps and is applied to an automotive body-in-white (BIW) where 10 out of 21 components are identified as potential candidates for embedding flexibility. The method shows how to systematically pinpoint and value flexible elements in platforms. This allows increased product family profit despite uncertain variant demand and specification changes. We show how embedding flexibility suppresses change propagation and lowers switch costs, despite an increase of 34% in initial investment for equipment and tooling. Monte Carlo simulation results for 12 future scenarios reveal the value of embedding flexibility.",2006
"Platform-Based Design and Development: Current Trends and Needs in Industry","Many companies constantly struggle to find cost-effective solutions to satisfy the diverse demands of their customers. In this paper, we report on two recent industry-focused conferences that emphasized platform design, development, and deployment as a means to increase variety, shorten lead-times, and reduce development and production costs. The first conference, ",2006
"Two Methodologies for Identifying Product Platform Elements Within an Existing Set of Products","As companies are pressured to decrease product development costs concurrently with increasing product variety, the need to develop products based upon common components and platforms is growing. Determining why a platform worked, or alternatively why it did not, is an important step in the successful implementation of product families and product platforms in any industry. Unfortunately, published literature on platform identification and product family analysis using product dissection and reverse engineering methods is surprisingly sparse. This paper introduces two platform identification methodologies that use different combinations of tools that can be readily applied based on information obtained directly from product dissection. The first methodology uses only the Bills-of-Materials and Design Structure Matrices while the second utilizes function diagrams, Function-Component Matrices, Product-Vector Matrices, and Design Structure Matrices to perform a more in-depth analysis of the set of products. Both methodologies are used to identify the platform elements in a set of five single-use cameras available in the market. The proposed methodologies identify the film advance and shutter actuation platform elements of the cameras, which include seven distinct components. The results are discussed in detail along with limitations of these two methodologies.",2006
"A Comprehensive Metric for Evaluating Component Commonality in a Product Family","The competitiveness in today’s market forces many companies to rethink the way they design (and redesign) products. Instead of developing one product at a time, many manufacturing companies are developing families of products to provide enough variety for the marketplace while keeping costs relatively low. Although the benefits of commonality are widely known, many companies are still not taking full advantage of it when developing new products or redesigning existing ones. One reason is the lack of appropriate methods and useful metrics to assess a product family based on commonality and diversity. Although many component-based commonality metrics have been proposed in the literature, they do not (1) help resolve the tradeoff between commonality and diversity in a product family and (2) capture enough information to be completely useful during product family design and redesign. In this paper, we propose the Comprehensive Metric for Commonality (CMC) to evaluate the design of a product family on a 0–1 scale based on the components in each product, their size, geometry, material, manufacturing process, assembly, costs, and the allowed diversity in a family. To demonstrate the usefulness of this metric for product family benchmarking and redesign, the CMC is compared to six other component-based commonality indices. A CMC-based method is also proposed and applied to a family of staplers to (1) assess the level of commonality in the product family and (2) give recommendations for redesigning the product family.",2006
"Towards a Suite of Problems for Comparison of Product Platform Design Methods: A Proposed Classification","This paper presents the status of an ongoing project to develop a comprehensive suite of test problems suitable for comparing methods for scale-based product platform design. Despite a growing body of work in the area, there is no adequate set of testbed example problems for product platform design and benchmarking. A lack of consensus as to exactly what scale-based platform design entails has also hampered comparison of methods. In order to make a comprehensive test suite, we first need to define what different capabilities of platform design methods should be tested. To further this end, a classification scheme for example problems for scale-based platform design is presented. This simple taxonomy classifies example problems on the basis of two criteria: selection of platform architecture and incorporation of market demand. A brief review of examples from the literature shows that the existing examples are useful to test only a few of the capabilities of platform design methods. A new extension of an existing example, the design of a family of universal electric motors, is presented to test capabilities not covered by the existing set. This extended example is the first in our suite of examples.",2006
"Selection-Integrated Optimization (SIO) Methodology for Optimal Design of Adaptive Systems","Many engineering systems are required to operate under changing operating conditions. A special class of systems called adaptive systems have been proposed in the literature to achieve high performance under changing environments. Adaptive systems acquire this powerful feature by allowing their design configuration to change with operating conditions. In the optimization of the adaptive systems, designers are often required to select (i) adaptive and (ii) non-adaptive (or fixed) design variables of the design configuration. Generally, the selection of these variables, and the optimization of adaptive systems are performed sequentially, thus leaving a likelihood of a sub-optimal design. In this paper, we propose the Selection-Integrated Optimization (SIO) methodology that integrates the two key processes: (1) the selection of the adaptive and fixed design variables, and (2) the optimization of the adaptive system, thereby leading to an optimum design. A major challenge to integrating these two key processes is the selection of the number of fixed and adaptive design variables, which is discrete in nature. We propose the Variable-Segregating Mapping-Function (VSMF) that overcomes this roadblock by progressively approximating the discreteness in the design variable selection process. This simple yet effective approach allows the SIO methodology to integrate the selection and optimization processes, and help avoid one significant source of sub-optimality from typical optimization formulations. The SIO methodology finds its applications in a variety of other engineering fields as well, such as product family optimization. However, in this paper, we limit the scope of our discussion to adaptive system optimization. The effectiveness of the SIO methodology is demonstrated by optimally designing a new air-conditioning system called Active Building Envelope (ABE) System.",2006
"A Method to Improve Platform Leveraging in a Market Segmentation Grid for an Existing Product Line","This paper describes a method for improving commonality in a highly customized low volume product line whose members were originally developed one-at-a-time to meet specific customer requirements. Rather than focusing on redesign of the entire product line, which can often be cost prohibitive, the method is part of a strategy to redesign a limited set of component parts that have the highest potential for cost savings. The method involves a four-step process: (1) determine an optimal component solution for each member artifact of an existing market segment grid, (2) test the feasibility of using each optimal component as a platform for the other artifacts, (3) formulate an optimization problem around the feasibility statistics whose solution is a product platform portfolio, and (4) solve the optimization problem for the minimum number of platforms that can adequately span the existing market segment grid. The proposed method is applied to an example involving the redesign of actuator mounting yokes for an existing set of valves that are used in nuclear power plants. The method shows promise for determining a product platform mix that maximizes commonality yet meets performance requirements.",2006
"An Enhanced Change Modes and Effects Analysis (CMEA) Tool for Measuring Product Flexibility With Applications to Consumer Products","Contemporary product designers seek to create products that are not only robust for the current marketplace but also flexible for future changes, adaptations, and evolutions. This type of product flexibility is distinctive from mass customization, product architecture of singular products, and product families. The intent is to design products that intrinsically enable future changes even though such changes may not be known or planned in the current product offering. To accommodate product flexibility of this type, research advancements are needed in terms of fundamental design principles and evaluation methods for predicting and improving the flexibility of a product. This paper presents advancements in both areas. We first present the systematic enhancement of a flexibility assessment tool referred to as CMEA, Change Modes and Effects Analysis. CMEA provides the basic ability to assess the flexibility of a product, with analogous features to the well-known Failure Modes and Effects Analysis. Our enhancements extend the method to provide for intuitive and more repeatable measures of flexibility. We then use the enhanced CMEA to investigate a variety of consumer products with the goal of inductively deriving product flexibility principles. Concrete applications are shown for these principles from the domain of power yard tools, such as hedge trimmers, weed trimmers, and leaf blowers. Also, the applications are used to demonstrate the value of the CMEA enhancements.",2006
"Determination of Ranged Sets of Design Specifications by Incorporating Heterogeneous Design Capability Information","Setting design specifications (targets) is a critical task in the early stages of a design process. Flexible targets can accommodate uncertainty and changes in design by postponing design commitments and preserving design freedom. In this work, a new and efficient method for obtaining a ranged set of design specifications that meets the overall design goal while incorporating heterogeneous design capability information is developed. Our proposed method involves two important aspects. First, a quantization algorithm based on rough set theory is used to decompose a design attribute space into subregions based on how well they meet the overall design goal. Second, a new design flexibility measure is used as a metric to select the most desired “target region” based on both the size of the region and the design capability information retrieved from potential design concepts. Our approach captures heterogeneous design capability information in the design attribute space and enhances the ability to adapt to evolving design knowledge as well as unexpected changes. The proposed method is much more efficient than conventional optimization algorithms for solving such problems. The proposed method is demonstrated by a numerical example and the design of a domestic blender.",2006
"Assessing and Improving Commonality and Diversity Within a Product Family","At a time when product differentiation is a major indicator of success in the global market, each company is looking to offer competitive and highly differentiated products. This differentiation issue is restricted by the design of platform-based products that share modules and/or components. It is not easy to differentiate products in a market that is often overwhelmed by numerous options. A platform-based approach can be risky because competition in the global market can become an internal competition among similar products within the family if there is not enough differentiation in the family. Thus, the goal for the product platform is to share elements for common functions and to differentiate each product in the family by satisfying different targeted needs. To assess commonality in the family, numerous indices have been proposed in the literature. Nevertheless, existing indices focus on commonality and reflect an increase in value when commonality increases but do not positively reflect an increase in the value as a result of diversity; hence, the ",2006
"Assessing and Increasing Product and Family Differentiation in the Market","To help guarantee profit and stability in today’s global market, companies must focus on the differentiation of their products. Successfully differentiated products will attract customers, generate revenue and benefit the brand image, whereas a banal product can lose money and leave a bad impression in the market. Many large companies have recently lost significant market share in part due to poor product differentiation. This paper introduces four indices to assess this differentiation at two levels—family and market—based on product function and function attributes. At the family level, the Product Differentiation Index (PDI) assesses the differentiation between a product and other products in the rest of the family and also the differentiation within the family. At the market level, the Family Differentiation Index (FDI), Family Coverage Index (FCI), and Family Un-coverage Index (FUI) assess the differentiation, the coverage, and the un-coverage of a family with another, and/or with the rest of the market, respectively. These indices help designers and marketers evaluate the positioning of their products and support product family planning. A case study involving two competitive single-use camera families is presented.",2006
"Module Interface Representation","Modular design issues are receiving increased attention by companies interested in reducing costs from carrying large numbers of components while at the same time increasing product quality and providing customers with greater product variety. Existing research has mainly focused on optimizing product platforms and product offerings, with little attention being given to the interfaces between modules. This research presents an investigation into how module interfaces are best represented in a CAD/PDM environment. The representation decisions are identified and advantages and limitations for each option are presented. Representation decisions revolve around issues such as the use of higher abstraction models, the use of ports, and referencing interface components in interface definitions. We conclude that higher abstraction models are necessary, ports should be represented explicitly, and interface hardware should not be included directly with interfaces. The research considers a large number of components from representative products offered by a home appliance manufacturer.",2006
"Product Platform Problem Taxonomy: Classification and Identification of Benchmark Problems","Many companies are using product platform concepts to gain economies of scale and to identify new market opportunities. Though the area of product platforming continues to be actively investigated by both industry and academia, there is no comprehensive classification scheme that can provide a clear picture of the existing problems and possible future research directions. Hence, in the present paper, we introduce a broad taxonomy that classifies product platform problems based on the product development stages. This can serve as a basis to: (1) Extract and categorize problems from research literature; (2) Identify potential extensions and/or new problems that have not been addressed in the literature; and (3) Identify existing problem sets and/or develop new problem sets for benchmarking purposes. We introduce a Conditions and Assumptions Code (CAC) scheme and use it in the identification of benchmark problems as well as in analyzing two classes of evaluation methods adopted for the platform problems: metrics-based and optimization-based. Thus, we have not only categorized existing problems but also identified possible future research problems in each of the categories. This categorization serves as a navigation tool to understand the progress made in this field so far and to identify new research directions.",2006
"Adapted Concept Generation and Computational Techniques for the Application of a Transformer Design Theory",NULL,2006
"2-D Deposition Pattern and Strategy Study on Rapid Manufacturing","Different from the traditional machining processes, the quality of parts produced by the metal deposition process is much more dependent upon the choice of deposition paths. Due to the nature of the metal deposition processes, various tool path patterns will result in different shapes in the metal deposition process with about the same input geometry. This paper presents the research conducted on the effect of various scanning patterns and strategies for the deposition results. Triangle and rectangle patterns are selected as basic 2-D “cells” to plan the scanning path. Several criteria, like minimum angle, minimum length of edge, etc. are defined to categorize the different “cell” shapes. Based on deposition results, the suitable patterns are determined for each type. The previously defined patterns are applied for each cell in order to achieve the optimal quality. The experiment has demonstrated that the pattern and strategy selection has improved the deposition quality significantly.",2006
"Ruled Layer Generation Between Two Freeform Curves by Normal and Distance Matching","A new curve matching method is proposed to generate non-self-intersecting and non-twisted ruled layers for application in diverse fields such as layered manufacturing, offsetting and multi-axis CNC machining. The method establishes point-to-point correspondence represented by a set of ruling lines between two directrices of the ruled surface. The directrices are given as non-self-intersecting, closed, at least C1 continuous, planar, B-spline curves. To match the points on the directrices, a heuristic optimization method developed with the objective is to maximize the sum of the inner products of the unit normals at the end points of the ruling lines and minimize the sum of the lengths of connecting ruling lines. The generated ruling lines can be used as cutter location data for multi-axis NC machining of ruled surfaces. Moreover, by subdividing the ruling lines into equal number of segments, one can construct a series of intermediate piecewise linear curves that represent the metamorphosis between the directrices. Implementation and examples are also presented.",2006
"Spiral-Like Path Planning Without Gap for Material Deposition Processes","Contour-parallel path, also called offset path, is a commonly used deposition strategy exploited by many deposition processes. This offset path has been studied vastly due to the potential of its applications. The majority of contour-parallel path research is found in robotics, CNC machining, and metal deposition. Even though curve offsetting has been extensively studied, there is virtually no algorithm that can produce a complete connected deposition path and can fill arbitrary shape cross sections entirely without gaps. This gap problem is similar to the uncut-region problem in CNC pocket machining. There are only few investigations on this uncut-region or gap problem even though the problem has long been recognized. A new strategy to divide regions and to plan the spiral-like deposition paths without gap based on contour-parallel paths is discussed in this paper. To prove the correctness and the usefulness of the proposed method, simulations and experiments are also discussed.",2006
"Fast Layered Manufacturing Support Volume Computation on GPUs","We present a GPU-accelerated algorithm for computing a fast approximation of the volume of supports required for layered manufacturing in a given build direction, one criterion often used to choose a direction that requires less time and material. In a sequence of rendering passes that project the part in the given build direction, we use depth peeling to identify faces bounding supports. We exploit programmable graphics hardware to compute the total height of all supports at each projected pixel location, scale the values by pixel area, and finally sum over all pixels to find the total volume of supports. For sample parts tested, our algorithm achieves over 99% accuracy and running times ranging from .2 seconds, for a part with 1,252 facets and depth complexity 2, to 1.86 seconds, for a part with 419,798 facets and depth complexity 9.",2006
"A Sensitivity Analysis of Shot Peening Parameters","Shot peening process is largely used for surface treatment of metallic components with the aim of increasing surface toughness and extending fatigue life. The fatigue strength of the component can be improved by inducing compressive residual stress in the surface and subsurface layers by the shot peening process. Numerical simulation of the shot peening process is an important tool that is used to aid in understanding the effects of the process parameters on intended goal of attaining the optimum residual stress profile and maximum process gain. In this paper an elasto-plastic finite element model is used for the shot peening process. The process parameters that are varied in this analysis are: the shot diameter, shot speed, number of shots at a given time (coverage) and target material. The analysis is carried out for two different materials, namely, steel and aluminum. An Explicit finite element code (ABAQUS) is used to perform this task. These parameters have different effects on the resulting residual profile and the results of the study showed that by adjusting these parameters, the most effective residual stress profile could be obtained.",2006
"Reliability-Based Design Using Saddlepoint Approximation","Reliability-based design optimization is much more computationally expensive than deterministic design optimization. To alleviate the computational demand, the First Order Reliability Method (FORM) is usually used in reliability-based design. Since FORM requires a nonlinear transformation from non-normal random variables to normal random variables, the nonlinearity of a constraint function may increase. As a result, the transformation may lead to a large error in reliability calculation. In order to improve accuracy, a new reliability-based design method with Saddlepoint Approximation is proposed in this work. The strategy of sequential optimization and reliability assessment is employed where the reliability analysis is decoupled from deterministic optimization. The accurate First Order Saddlepoint method is used for reliability analysis in the original random space without any transformation, and the chance of increasing nonlinearity of a constraint function is therefore eliminated. The overall reliability-based design is conducted in a sequence of cycles of deterministic optimization and reliability analysis. In each cycle, the percentile value of the constraint function corresponding to the required reliability is calculated with the Saddlepoint Approximation at the optimal point of the deterministic optimization. Then the reliability analysis results are used to formulate a new deterministic optimization model for the next cycle. The solution process converges within a few cycles. The demonstrative examples show that the proposed method is more accurate and efficient than the reliability-based design with FORM.",2006
"Uncertainty Analysis With Probability and Evidence Theories","Both aleatory and epistemic uncertainties exist in engineering applications. Aleatory uncertainty (objective or stochastic uncertainty) describes the inherent variation associated with a physical system or environment. Epistemic uncertainty, on the other hand, is derived from some level of ignorance or incomplete information about a physical system or environment. Aleatory uncertainty associated with parameters is usually modeled by probability theory and has been widely researched and applied by industry, academia, and government. The study of epistemic uncertainty in engineering has recently started. The feasibility of the unified uncertainty analysis that deals with both types of uncertainties is investigated in this paper. The input parameters with aleatory uncertainty are modeled with probability distributions by probability theory, and the input parameters with epistemic uncertainty are modeled with basic probability assignment by evidence theory. The effect of the mixture of both aleatory and epistemic uncertainties on the model output is modeled with belief and plausibility measures (or the lower and upper probability bounds). It is shown that the calculation of belief measure or plausibility measure can be converted to the calculation of the minimum or maximum probability of failure over each of the mutually exclusive subsets of the input parameters with epistemic uncertainty. A First Order Reliability Method (FORM) based algorithm is proposed to conduct the unified uncertainty analysis. Two examples are given for the demonstration. Future research directions are derived from the discussions in this paper.",2006
"Engineering Product Design Optimization for Retail Channel Acceptance","Significant recent research has focused on the marriage of consumer preferences and engineering design in order to improve profitability. The extant literature has neglected the effects of channel markets which are increasingly prevalent. At the crux of the issue is the fact that channel dominating retailers, like Wal-Mart, have the ability to unilaterally control manufacturer production decisions as gatekeepers to the consumer or market. In this paper, we propose a new methodology that accounts for this power asymmetry. A chance constrained framework is used to model retailer acceptance of possible engineering designs and accounts for the important effect on the profitability of the retailer’s assortment through a latent class estimation of demand from conjoint surveys. Our approach allows the manufacturer to optimize a product design for profitability while reliably ensuring that the product will make it to market by making the retailer more profitable with the addition of the new product. As a demonstrative example, we apply the proposed approach for product design selection in the case of an angle grinder. For this example, we analyze the market and are able to improve expected manufacturer profitability while simultaneously presenting the decision maker with tradeoffs between slotting allowances, market share, and risk of retailer acceptance.",2006
"A Comparison of Probability Bounds Analysis and Sensitivity Analysis in Environmentally Benign Design and Manufacture","There is growing acceptance in the design community that two types of uncertainty exist: inherent variability and uncertainty that results from a lack of knowledge, which variously is referred to as imprecision, incertitude, irreducible uncertainty, and epistemic uncertainty. There is much less agreement on the appropriate means for representing and computing with these types of uncertainty. Probability bounds analysis (PBA) is a method that represents uncertainty using upper and lower cumulative probability distributions. These structures, called probability boxes or just p-boxes, capture both variability and imprecision. PBA includes algorithms for efficiently computing with these structures under certain conditions. This paper explores the advantages and limitations of PBA in comparison to traditional decision analysis with sensitivity analysis in the context of environmentally benign design and manufacture. The example of the selection of an oil filter involves multiple objectives and multiple uncertain parameters. These parameters are known with varying levels of uncertainty, and different assumptions about the dependencies between variables are made. As such, the example problem provides a rich context for exploring the applicability of PBA and sensitivity analysis to making engineering decisions under uncertainty. The results reveal specific advantages and limitations of both methods. The appropriate choice of an analysis depends on the exact decision scenario.",2006
"A Sequential Algorithm for Possibility-Based Design Optimization","Deterministic optimal designs that are obtained without taking into account uncertainty/variation are usually unreliable. Although reliability-based design optimization accounts for variation, it assumes that statistical information is available in the form of fully defined probabilistic distributions. This is not true for a variety of engineering problems where uncertainty is usually given in terms of interval ranges. In this case, interval analysis or possibility theory can be used instead of probability theory. This paper shows how possibility theory can be used in design and presents a computationally efficient sequential optimization algorithm. After, the fundamentals of possibility theory and fuzzy measures are described, a double-loop, possibility-based design optimization algorithm is presented where all design constraints are expressed possibilistically. The algorithm handles problems with only uncertain or a combination of random and uncertain design variables and parameters. In order to reduce the high computational cost, a sequential algorithm for possibility-based design optimization is presented. It consists of a sequence of cycles composed of a deterministic design optimization followed by a set of worst-case reliability evaluation loops. Two examples demonstrate the accuracy and efficiency of the proposed sequential algorithm.",2006
"Numerical Methods for Propagating Imprecise Uncertainty","Since engineering design requires decision making under uncertainty, the degree to which good decisions can be made depends upon the degree to which the decision maker has expressive and accurate representations of his or her uncertain beliefs. Whereas traditional decision analysis uses precise probability distributions to represent uncertain beliefs, recent research has examined the effects of relaxing this assumption of precision. A specific example of this is the theory of imprecise probability. Imprecise probabilities are more expressive than precise probabilities, but they are also more computationally expensive to propagate through mathematical models. The probability box (p-box) is an alternative representation that is both more expressive than precise probabilities, and less computationally expensive than general imprecise probabilities. In this paper, we introduce a method for propagating p-boxes through black box models. Based on two example models, a new method, called p-box convolution sampling (PCS), is compared with three other p-box propagation methods. It is found that, although PCS is less expensive than the alternatives, it is still relatively expensive and therefore only justifiable when the expected benefits are large. Several directions for further improving the efficiency of PCS are discussed.",2006
"A Single-Loop Approach for System Reliability-Based Design Optimization","An efficient single-loop approach for series system reliability-based design optimization problems is presented in this paper. The approach enables the optimizer to apportion the system reliability among the failure modes in an optimal way by increasing the reliability of those failure modes whose reliability can be increased at low cost. Furthermore, it identifies the critical failure modes that contribute the most to the overall system reliability. A previously reported methodology uses a sequential optimization and reliability approach. It also uses a linear extrapolation to determine the coordinates of the most probable points of the failure modes as the design changes. As a result, the approach can be slow and may not converge if the location of the most probable failure point changes significantly. This paper proposes an alternative system RBDO approach that overcomes the above difficulties by using a single-loop approach where the searches for the optimum design and for the most probable failure points proceed simultaneously. An easy to implement active set strategy is used. The maximum allowable failure probabilities of the failure modes are considered as design variables. The efficiency and robustness of the method is demonstrated on three design examples involving a beam, an internal combustion engine and a vehicle side impact. The results are compared with deterministic optimization and the conventional component RBDO formulation.",2006
"Robust Design of Compressor Blades Against Manufacturing Variations","The aim of this paper is to develop and illustrate an efficient methodology to design blades with robust aerodynamic performance in the presence of manufacturing uncertainties. A novel geometry parametrization technique is developed to represent manufacturing variations due to tolerancing. A Gaussian Stochastic Process Model is trained using DOE techniques in conjunction with a high fidelity CFD solver. Bayesian Monte Carlo Simulation is then employed to obtain the statistics of the performance at each design point. A multiobjective optimizer is used to search the design space for robust designs. The multiobjective formulation allows explicit trade-off between the mean and variance of the performance. A design, selected from the robust design set is compared with a deterministic optimal design. The results demonstrate an effective method to obtain compressor blade designs which have reduced sensitivity to manufacturing variations with significant savings in computational effort.",2006
"The Risk in Early Design (RED) Method: Likelihood and Consequence Formulations","This study focuses specifically on the relationship between function and risk in early design by presenting a mathematical mapping from product function to likelihood and consequence risk assessments that can be used in the conceptual design phase. An investigation of a spacecraft orientation subsystem is used to demonstrate the proposed mappings. The risk assessment presented in this paper is a tool that will aid designers by identifying risks as well as reducing the subjectivity of the likelihood and consequence value from a risk element, provide four key risk element properties (design parameter, failure mode, likelihood, and consequence) for numerous risk elements with a simple calculation, and provide a means for inexperienced designers to effectively address risk in the conceptual design phase. The investigation demonstrates that the method presented in this paper is a useful tool for preliminary identification and assessment of product risks.",2006
"Optimal Design With Non-Normally Distributed Random Parameters, Conditional Probability, and Joint Constraint Reliabilities: A Case Study in Vehicle Emissions Regulations to Achieve Ambient Air Quality Standards","Making appropriate environmental policy decisions requires considering various sources of uncertainty. An air pollution example is formulated as a design optimization problem with probabilistic constraints, also referred to as reliability-based design optimization (RBDO). Environmental applications with a large number of constraints and significant model complexity present special challenges. In this paper an efficient active set strategy is integrated with a reliability contour surface approach to solve probabilistic problems with non-normal variable probability distributions. Discrete random parameters, which result in Bayesian probability, are also present and they are incorporated using delta function approximations. Joint constraint reliability that considers satisfying all regulatory constraints is also discussed. A demonstration example of setting the optimal vehicle speed limit while maintaining high reliability for CO and NOx standards of a residential area near two highway systems is included.",2006
"Simulation Model Refinement for Decision Making Via a Value-of-Information Based Metric","Since no simulation model is perfect, any simulation model for modeling a system’s physical behavior can be refined further. Hence, the question faced by a designer is — ",2006
"A Bayesian Approach to Reliability-Based Optimization With Incomplete Information","In engineering design, information regarding the uncertain variables or parameters is usually in the form of finite samples. Existing methods in optimal design under uncertainty cannot handle this form of incomplete information; they have to either discard some valuable information or postulate existence of additional information. In this article, we present a reliability-based optimization method that is applicable when information of the uncertain variables or parameters is in the form of both finite samples and probability distributions. The method adopts a Bayesian Binomial inference technique to estimate reliability, and uses this estimate to maximize the confidence that the design will meet or exceed a target reliability. The method produces a set of Pareto trade-off designs instead of a single design, reflecting the levels of confidence about a design’s reliability given certain incomplete information. As a demonstration, we apply the method to design an optimal piston-ring/cylinder-liner assembly under surface roughness uncertainty.",2006
"Modeling Implications in Simulation-Based Design of Stents","Variations associated with stenting systems, artery properties, and doctor skills necessitate a better understanding of coronary artery stents so as to facilitate the design of stents that are customized to individual patients. This paper presents the development of an integrated computer simulation-based design approach using engineering finite element analysis (FEA) models for capturing stent knowledge, utility theory-based decision models for representing the design preferences, and statistics-based surrogate models for improving process efficiency. Two focuses of the paper are: 1) understanding the significance of engineering analysis and surrogate models in the simulation-based design of medical devices; 2) investigating the modeling implications in the context of stent design. The study reveals that the advanced nonlinear FEA software with analysis capacities on large deformation and contact interaction has offered a platform to execute high fidelity simulations, yet the selection of appropriate analysis models is still subject to the tradeoff between cost of analysis and accuracy of solution; the cost-prohibitive simulations necessitate the employment of surrogate models in subsequent multi-objective design optimization. A detailed comparison between regression models and Kriging models suggests the importance of sampling schemes in successfully implementing Kriging methods.",2006
"Understanding the Effects of Model Uncertainty in Robust Design With Computer Experiments","The use of metamodels in simulation-based robust design introduces a new source of uncertainty that we term ",2006
"A Function-Based Methodology for Analyzing Critical Events","The objective of this research was to develop a function-based method for analyzing the critical sequences of events that must occur for complex space missions to be successful. The resulting methodology, the Function-based Analysis of Critical Events, or FACE, uses functional and event models to identify the changes in functionality of a system as it transitions between critical mission events. Two examples are presented that detail the application of FACE to prior mission failures including the loss of the Columbia orbiter and the failure of the Mars Polar Lander probe. The result of the research is a methodology that allows designers to not only reduce the occurrence of such failures but also analyze the specific functional causes of the failures when they do occur.",2006
"Some Metrics and a Bayesian Procedure for Validating Predictive Models in Engineering Design","Even though model-based simulations are widely used in engineering design, it remains a challenge to validate models and assess the risks and uncertainties associated with the use of predictive models for design decision making. In most of the existing work, model validation is viewed as verifying the model accuracy, measured by the agreement between computational and experimental results. However, from the design perspective, a good model is considered as the one that can provide the discrimination (good resolution) between design candidates. In this work, a Bayesian approach is presented to assess the uncertainty in model prediction by combining data from both physical experiments and the computer model. Based on the uncertainty quantification of model prediction, some design-oriented model validation metrics are further developed to guide designers for achieving high confidence of using predictive models in making a specific design decision. We demonstrate that the Bayesian approach provides a flexible framework for drawing inferences for predictions in the intended but may be untested design domain, where design settings of physical experiments and the computer model may or may not overlap. The implications of the proposed validation metrics are studied, and their potential roles in a model validation procedure are highlighted.",2006
"Multi-Disciplinary Conceptual Modeling and Optimization With Uncertainty Effects Consideration","A multi-disciplinary modeling and design optimization formulation for uncertainty effects consideration is presented in this paper. The optimization approach considers minimization of uncertainty factors related to the overall system performance while satisfying target requirements specified in the form of constraints. The design problem is decomposed into two analysis systems; performance design and uncertainty effects analysis. Each design system can be partitioned into several subsystems according to the different functions they perform. Performance evaluation is considered by minimizing the variations between specified expected values of performance functions and their target values. Uncertainty effects analysis is defined by minimizing the ratio of the maximum variations caused by uncertainty factors over the expected function values. The entire problem has been treated as a multi-disciplinary design optimization (MDO) for maximum robustness and performance achievement. An electromechanical system is used as an example to demonstrate this optimization methodology.",2006
"An Inverse Analysis Method for Design Optimization With Both Statistical and Fuzzy Uncertainties","Structural analysis and design optimization have recently been extended to consider various uncertainties. If the statistical data for the uncertainties are sufficient to construct the input distribution function, the uncertainties can be treated as random variables and RBDO is used; otherwise, the uncertainties can be treated as fuzzy variables and PBDO is used. However, many structural design problems include both uncertainties with sufficient data and uncertainties with insufficient data. For these problems, RBDO will yield an unreliable design since the distribution functions of uncertainties are not believable. On the other hand, treating the random variables as fuzzy variables and invoking PBDO may yield too conservative design with a higher optimum cost. This paper proposes a new design formulation using the performance measure approach (PMA). For the inverse analysis, this paper proposes a new most probable/possible point (MPPP) search method called maximal failure search (MFS), which is an integration of the enhanced hybrid mean value method (HMV+) and maximal possibility search (MPS) method. Some mathematical and physical examples are used to demonstrate the proposed inverse analysis method and design formulation.",2006
"Alternative Methods for Reliability-Based Robust Design Optimization Including Dimension Reduction Method","The objective of reliability-based robust design optimization (RBRDO) is to minimize the product quality loss function subject to probabilistic constraints. Since the quality loss function is usually expressed in terms of the first two statistical moments, mean and variance, many methods have been proposed to accurately and efficiently estimate the moments. Among the methods, the univariate dimension reduction method (DRM), performance moment integration (PMI), and percentile difference method (PDM) are recently proposed methods. In this paper, estimation of statistical moments and their sensitivities are carried out using DRM and compared with results obtained using PMI and PDM. In addition, PMI and DRM are also compared in terms of how accurately and efficiently they estimate the statistical moments and their sensitivities of a performance function. In this comparison, PDM is excluded since PDM could not even accurately estimate the statistical moments of the performance function. Also, robust design optimization using DRM is developed and then compared with the results of RBRDO using PMI and PDM. Several numerical examples are used for the two comparisons. The comparisons show that DRM is efficient when the number of design variables is small and PMI is efficient when the number of design variables is relatively large. For the inverse reliability analysis of reliability-based design, the enriched performance measure approach (PMA+) is used.",2006
"INSIDES: A New Design and Simulation Platform for Virtual Prototyping in Automotive and Aerospace Industry","Human Machine Interactions Systems (HMI) are decisive for acceptance and safety of new cockpits in the automotive as well as in the aerospace industries. A new design and simulation platform called INSIDES will be presented where virtual cockpit prototypes are being built based on 3D CAD geometry e.g. from CATIA and integrated with logical interaction data derived from UML specifications. This new development platform enables the continuous validation and check of new interaction concepts by involving usability engineers in the very early stage of the development cycle. Since the simulation work is being done in the context of the entire aircraft cockpit / car interior with all instruments, control commands as well as displays devices a better validation of HMI systems can be achieved.",2006
"Development of Evaluation System for Style Design Using Mixed Reality Technology","Currently industrial design is mainly done by CAD and mock-up is created to evaluate the design. This process is repeated from the rough sketches to the final detailed mock-up until the designer is satisfied. In this process, creating the mock-up, especially detailed mock-up, is quite costly. Hence, there is a need to improve the efficiency of mock-up fabrication. One of the strategies for realizing this is to use virtual models (VM) instead of mock-up. VM is a model which is created on computer by 3D computer graphics, and it allows realistic graphical modeling which can be modified easily, reduces time considerably, and enables dynamic viewing of models from any angle and orientation. Despite this, mock-ups are still required because the process of design evaluation by touching physical models (PM) is still important to designers. To resolve this disadvantage of VM, this paper proposes a new evaluation system for industrial design. With this system, VM and the rapid prototyping mock-up are overlapped in virtual space to produce a tangible VM. This new type of VM functions just like a detailed mock-up but can be created much faster and cheaper. This system employs the concept of Augmented Virtuality (AV), which is mainly based on virtual space and real objects are added to reinforce the virtual space. In this case, haptic information from the rapid prototyping mock-up is added to optical information from the VM. When estimating the 3D position of a real object, optical information is used considerably more than haptic information. Therefore, if there are only a few positional or geometrical differences between the rapid prototyping mock-up and the VM, the differences can be offset by the (incorporated into the) optical information. For this reason, if the designer needs to modify the product shape, only the VM needs to be modified, allowing old mock-ups to be used repeatedly. This means that designers can evaluate a variety of product designs with only one mock-up, thus reducing both time and the costs for creating a mock-up. The operator wears a data glove on his/her hand to construct a virtual hand in the virtual space. With this virtual hand, the operator can also evaluate the user interface (UI) of the product by means of pushing buttons or watching display on the VM. This paper also provides a new method for overlapping the virtual space and real space.",2006
"A Graphical Modeling Environment for Configuring Modular Product Families and Platforms","A significant amount of research has established that product platform planning is effective for the development of multiple products and management strategy for companies in today’s market. However, there are still significant challenges in planning and the realization of product families and platforms. This is particularly true for determining family and platform architectures—imperative assets in companies in order to pursue competitive advantages. It is a challenging task because individual customization of products generally competes with the goal of maximizing platform commonality. To address this challenge, this paper introduces a graphical computer-based modeling environment to support product design teams in configuring modular product families. In the modeling environment, a product family can be decomposed into its products, modules, and functions. Also, interfaces among the product components can be elaborated by defining the relationship types (fundamental, redundant, and operational). Further elaboration can be achieved by defining an appropriate set of module drivers from four different perspectives: financial, customer, design processes, and organizational culture/IT. These features facilitate modeling of a product family at multiple levels of abstraction as capturing design drivers, reasoning and goals. The application of the modeling environment is illustrated with a family of coffee-makers. It is demonstrated how the proposed modeling method offers a comprehensive representation and understanding of product family planning by integrating multiple perspectives on modular architecting. Moreover, a matrix-based analysis option is provided for design teams to view the relationships between the technical functions and the forms, and the design goals and the customer requirements in a preliminary manner.",2006
"Visual Representations as an Aid to Concept Generation","This paper describes our initial efforts to develop a 3D visualization tool that is part of an overall effort to create a Concept Generator, an automated conceptual design tool, to aid a designer during the early stages of the design process. The use of CAD software has diversified into various disciplines that have made use of simulation and software modeling tools for reasons that range from improving accuracy in design, reduction in lead times, and simple visualizations. The impacts of CAD software have been beneficial in industry and education. Described in this paper is the use of low-memory VRML models to represent components. These low memory models have been created to achieve several goals that complement the overall objectives of the concept generator. One key goal is that the concept generator be accessible via the web, thus the need for low-memory and low-data models. Additionally, as the concept generator is intended for usage during early conceptual design, the 3D visualization tool allows the creation of models upon which basic manipulations can be performed so that a designer can get an initial feel of the structure that his product is going to take. Our research has enabled us to create a basic visualization tool which, while similar in nature to most other CAD software tools, is unique, in that it represents the link, as a visual interface, between a formulated concept and the designer. The paper presents the research problem, an overview of the architecture of the software tool and some preliminary results on visual representations as an aid to concept generation.",2006
