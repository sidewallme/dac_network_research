title,Abstract,year
"Overlapped Grouping Genetic Algorithm for Optimization of Reels Cutting Planning Problems","The optimization of Reel-Cutting Planning Problem “RCPP” is concerned with finding the best selection of a strategic reel set and the corresponding tactical cutting lengths “sheets set”, from a wide feasible space, to be used in producing a set of blanks. The problem is classified as a weighted multi objectives two-staged guillotine two-dimensional Cutting Stock Problem “CSP” of Dyckoff’s type 2/V/D/M. This paper presents a tailored metaheuristic solution using grouping genetic algorithms (GGAs). Overlapped Chromosome representation is newly developed for optimization. Developed also an especial database to create the cutting patterns of resulting solution. The objective in this application is to minimize the total cost, which includes the total material cost required to achieve a lot size, the cost of selecting reels set size, and the cost of cutting lengths from these reels “sheet set size”. An industrial case study is considered. The attained optimum cutting plan provides an overall improvement of 4.3% over the current professional cutting plan. The reduction in the reel set size is from five to an optimum of three reels. The developed procedure also provides reduction of trim loss of 4% over the current plan. The developed approach also proved to be faster than the currently used techniques.",2007
"A Perspective of Hierarchical Layout Design Optimization for Highly Packaged Equipments","This paper discusses a perspective of hierarchical layout design optimization for highly packaged equipments and demonstrates an implementation of an optimization algorithm with a simplified case study. First, the Pareto optimality of subsystem-level shape design against the optimality of system-level shape design is extracted through two-level hierarchical formulation of layout design problems. Then, a computational design algorithm is developed for a class of two-dimensional layout design problems of rectangles, some of which are the results of similar problems defined in its sub-levels. The algorithm represents the layout topology with sequence-pair and the shape of each module or component with the aspect ratio, and optimizes them with genetic algorithms. The Pareto optimality of sub-levels is handled with the functionality of multi-objective optimization of genetic algorithms, in which a set of Pareto are simultaneously generated. Top-level and sub-level layout problems are coordinated through exchange of preferable ranges of shapes and layout. A case study is explored under the developed algorithm. The promises and limitations of the proposed framework is briefly discussed for defining the future works.",2007
"A Genetic Algorithm Based Procedure for Extracting Optimal Solutions From a Morphological Chart","A novel approach using a genetic algorithm is presented for extracting globally satisfycing (Pareto optimal) solutions from a morphological chart where the evaluation and combination of “means to sub-functions” is modeled as a combinatorial multi-objective optimization problem. A fast and robust genetic algorithm is developed to solve the resulting optimization problem. Customized crossover and mutation operators specifically tailored to solve the combinatorial optimization problem are discussed. A proof-of-concept simulation on a practical design problem is presented. The described genetic algorithm incorporates features to prevent redundant evaluation of identical solutions and a method for handling of the compatibility matrix (feasible/infeasible combinations) and addressing desirable/undesirable combinations. The proposed approach is limited by its reliance on the quantifiable metrics for evaluating the objectives and the existence of a mathematical representation of the combined solutions. The optimization framework is designed to be a scalable and flexible procedure which can be easily modified to accommodate a wide variety of design methods that are based on the morphological chart.",2007
"A Variable Fidelity Model Management Framework for Multiscale Computational Design of Continuous Fiber SiC-Si3N4 Ceramic Composites","Research applications involving design tool development for multiple phase material design are at an early stage of development. The computational requirements of advanced numerical tools for simulating material behavior such as the finite element method (FEM) and the molecular dynamics method (MD) can prohibit direct integration of these tools in a design optimization procedure where multiple iterations are required. The complexity of multiphase material behavior at multiple scales restricts the development of a comprehensive meta-model that can be used to replace the multiscale analysis. One, therefore, requires a design approach that can incorporate multiple simulations (multi-physics) of varying fidelity such as FEM and MD in an iterative model management framework that can significantly reduce design cycle times. In this research a material design tool based on a variable fidelity model management framework is presented. In the variable fidelity material design tool, complex “high fidelity” FEM analyses are performed only to guide the analytic “low-fidelity” model toward the optimal material design. The tool is applied to obtain the optimal distribution of a second phase, consisting of silicon carbide (SiC) fibers, in a silicon-nitride (Si3 N4 ) matrix to obtain continuous fiber SiC-Si3 N4  ceramic composites (CFCCs) with optimal fracture toughness. Using the variable fidelity material design tool in application to one test problem, a reduction in design cycle time around 80 percent is achieved as compared to using a conventional design optimization approach that exclusively calls the high fidelity FEM.",2007
"Topology Optimization for Synthesizing Extrusion Based Nonlinear Transient Designs","Concept designs synthesized using conventional topology optimization methods are typically not easily manufacturaed, in that multiple finishing processes are required to construct the component. A manufacturing technique that requires only minimal effort is extrusion. Extrusion is a manufacturing process used to create objects of a fixed cross-sectional profile. Extrusion often minimizes the need for secondary machining, although not necessarily of the same dimensional accuracy as machined parts. The result of using this process is lower costs for the manufacture of the final product. In this paper, a non-gradient hybrid cellular automaton (HCA) algorithm is developed to synthesize constant cross section structures that are subjected to nonlinear transient loading. Examples are presented to demonstrate the efficiency of the proposed methodology in synthesizing these structures. The methodology is first demonstrated for elastic-static modeling. The novelty of the proposed method is the ability to generate constant cross section topologies for plastic-dynamic problems since the issue of complex gradients can be avoided using the HCA method.",2007
"An Intelligent and Efficient Tree Search Algorithm for Computer-Aided Component Selection","Component selection in engineering design is a process in which an assembly of pre-defined component types is given and a choice of specific components is desired that satisfies a set of design requirements and constraints. Although algorithmic approaches to component selection have been researched for specific mechanical engineering problems such as bearing selection, a generalized technique has not been developed. This paper proposes a universal tool to automate the process of component selection by incorporating a tree search. Our technique evaluates the worth of candidate solutions in terms of the customer needs satisfaction and the compatibility between interconnected components. The tree search technique used in this research is not only quick and efficient but also guarantees an optimal solution.",2007
"A Graph Grammar Approach to Generate Neural Network Topologies","Neural networks are increasingly becoming a useful and popular choice for process modeling. The success of neural networks in effectively modeling a certain problem depends on the topology of the neural network. Generating topologies manually relies on previous neural network experience and is tedious and difficult. Hence there is a rising need for a method that generates neural network topologies for different problems automatically. Current methods such as growing, pruning and using genetic algorithms for this task are very complicated and do not explore all the possible topologies. This paper presents a novel method of automatically generating neural networks using a graph grammar. The approach involves representing the neural network as a graph and defining graph transformation rules to generate the topologies. The approach is simple, efficient and has the ability to create topologies of varying complexity. Two example problems are presented to demonstrate the power of our approach.",2007
"A Learning Algorithm for Optimal Internal Combustion Engine Calibration in Real Time","Advanced internal combustion engine technologies have increased the number of accessible variables of an engine and our ability to control them. The optimal values of these variables are designated during engine calibration by means of a static correlation between the controllable variables and the corresponding steady-state engine operating points. While the engine is running, these correlations are being interpolated to provide values of the controllable variables for each operating point. These values are controlled by the electronic control unit to achieve desirable engine performance, for example in fuel economy, pollutant emissions, and engine acceleration. The state-of-the-art engine calibration cannot guarantee continuously optimal engine operation for the entire operating domain, especially in transient cases encountered in driving styles of different drivers. This paper presents the theoretical basis and algorithmic implementation for allowing the engine to learn the optimal set values of accessible variables in real time while running a vehicle. Through this new approach, the engine progressively perceives the driver’s driving style and eventually learns to operate in a manner that optimizes specified performance indices. The effectiveness of the approach is demonstrated through simulation of a spark ignition engine, which learns to optimize fuel economy with respect to spark ignition timing, while it is running a vehicle.",2007
"A Simplified Systematic Method of Acquiring Design Specifications From Customer Requirements","Faithfully obtaining design specifications from customer requirements is essential for successful designs. The natural lingual, inexact, incomplete and vague attributes of customer requirements make it very difficult to map customer requirements to design specifications. In general design process, the design specifications are determined by designers based on their experience and intuition, and often a certain target value is set for a specification. However, it is on one hand very difficult, on the other hand unreasonable, so a suitable limit range rather than a certain value is preferred at the beginning of design, especially at the concept design process. In this paper, a simplified systematic approach of transforming customer requirements to design specifications is proposed. First, a two-stepped clustering approach for grouping customer requirements and design specifications based on HOQ matrix is presented, by which the mapping is limited to within each group. To further simplify the inference mapping rules of customer requirements and design specifications, the minimal condition inference mapping rules for each design specification are extracted based on rough set theory. In the end, a suitable value range is determined for a specification by applying the fuzzy rule matrix.",2007
"Towards a Reference Ontology for Functional Knowledge Interoperability","Functionality is one of the key aspects of artifact models for design. A function of a device, however, can be captured in different ways in different domains or by different model-authors. Much research on functions has been conducted in the areas of engineering design, functional representation and philosophy, although there are several definitions and notions of functions. We view conceptualization of function is multiplicative in nature: different functions can be captured simultaneously from an objective behavior of an artifact under different teleological contexts of users/designers, or from different viewpoints (perspectives) of a model-author. Such differences become problematic for sharing functional knowledge among engineers. In this article, we attempt to clarify the differences of such perspectives for capturing functions on the basis of the ontological engineering. On the basis of a generalized model of the standard input-output model in the well-known systematic design methodology, we show descriptive categorization of some upper-level types (classes) of functions with references to some definitions of functions in the literature. Such upper-level ontological categories of functions are intended to be used as a reference ontology for functional knowledge interoperability. One of the two usages here is to convert functional models between different functional taxonomies. A functional term in a taxonomy is (ideally) categorized into a generic type defined in the reference ontology. It is widely recognized in the literature that such an upper-level ontology helps automatic “mapping discovery” which is to find similarities between two ontologies and determine which concepts represent similar notion. The reference ontology of function might have such an effect. Another usage of the reference ontology is to integrate fault knowledge into functional knowledge and automatic transformation of FMEA sheets. The designer can describe an integrated model of both functional knowledge and fault knowledge. Based on ontology mappings, automatic transformations of FMEA sheets can be realized. In this article, we discuss the detail of the definitions of the upper-level categories of functions ontologically. Then, we give an overview of usages and effects of the upper-level categories as a reference ontology for functional knowledge interoperability.",2007
"Fuzzy Analytical Hierarchy Process-Based Assembly Unit Partition for Complex Products","Assembly planning is one of the NP complete problems, which is even harder to solve for complex products. Furthermore, development of complex products has been becoming a collaborative activity among different enterprises at the same or different sites. Collaboration among designers is considered as an effective strategy to tackle the difficulty of assembly planning. According to the strategy, the task of assembly planning of complex products is decomposed into several simpler subtasks related to portions of the whole product, and the subtasks are then assigned to the designers in collaboration. In this paper, assembly unit partition is addressed, which means the decomposition of a complex product into smaller assembly units. Assembly unit partition is more complex than subassembly extraction (or identification) because more assembly constraints are taken into consideration. Firstly, different and necessary assembly constraints are analyzed in detail and their related evaluation indices are given for assembly unit partition. Then, the assembly unit model and the decision model of assembly unit partition are proposed. A method of assembly unit partition based on the decision model and Fuzzy Analytical Hierarchy Process (FAHP) method is clarified. The valid assembly units can be determined by analyzing the decision values of assembly relations between two parts and the given conditions. Finally, the validity and effectiveness of the method is verified with an example.",2007
"PDM Implementation in Indian Industries: An Impact Study on Technology Adoption","Shorter product life cycles, growing product complexity and the need for a large number of product variants have made Product Data Management (PDM) increasingly important for many manufacturing industries. Since, many industries in India either have implemented PDM or are considering implementation, it is the right time to study the real impact of PDM in Indian industries. This impact study was made by the responses obtained through a survey questionnaire. The main objective of the survey is to study the impact of PDM implementation on productivity. The variables of productivity measurement in terms of Key Performance Areas have been identified using Performance Objective-Productivity model. The questionnaire has been designed and administered and the responses received have been analysed using SPSS software. General descriptive statistics including mean and standard deviation along with paired t-test and Pearson’s correlation studies have been employed for analysing the importance of the measures of productivity. A user driven PDM evaluation framework and methodology based on the two stage Quality Function Deployment (QFD) technique has been developed to select the most suitable PDM product for a specific industry. A simple model to measure the Return on Investment for PDM implemented industries based on Port and Mackrell’s [24] work has been developed. Thus, methodologies have been developed and demonstrated with case studies to overcome the issues identified in the survey.",2007
"A Framework for Collaborative Top-Down Assembly Design","This paper presents a system framework to support the collaborative top-down assembly design. As the first step, the framework is devised to enable distributed designers to conduct collaborative layout design, 3D skeleton design and detailed design of a product in a top-down manner in a distributed environment. To effectively support the collaborative top-down assembly design, a multi-resolution and distributed product assembly model is proposed as a product representation in the framework. With the help of the framework the designers could conduct those design activities of top-down assembly design that need plenty of collaboration such as the collaborative determination of assembly relationship and coupled structural parameters. A variation propagation mechanism is also developed to guarantee the consistency of the distributed assembly model. A preliminary system prototype based on replicated client-server architecture is implemented.",2007
"A Design Strategy for Collaborative Decision Making in a Distributed Environment","In most instances it is necessary to partition a product realization process into a set of design activities. Design decisions are an important type of design activity. In order to ensure that information flows among different design activities are achieved without difficulty, uniform representation of the information is necessary. A design activity template and a solution template are presented in this paper to support the information flow in a design process. Activity template provides an option to describe the design problem so that engineers know exactly what kind of design decision is preferable. Solution template provides an option to describe the design solution. The paper also introduces an approach to manage and deliver design freedom from one engineering team to the other. In mechanical design, it is essential to give engineers a feasible design space so that they can choose the design solutions that best satisfy the design problem. A chip design example is used to illustrate the different concepts presented in this paper for collaborative decision making.",2007
"An Approach to Improved Decentralized Design: The Modified Approximation-Based Decentralized Design (MADD) Framework","The design of large scale complex engineering systems requires interaction and communication between multiple disciplines and decentralized subsystems. One common fundamental assumption in decentralized design is that the individual subsystems only exchange design variable information and do not share objective functions or gradients. This is because the decentralized subsystems can either not share this information due to geographical constraints or choose not to share it due to corporate secrecy issues. Game theory has been used to model the interactions between distributed design subsystems and predict convergence and equilibrium solutions. These game theoretic models assume that designers make perfectly rational decisions by selecting solutions from their ",2007
"Design Optimization of a Laptop Computer Using Aggregate and Mixed Logit Demand Models With Consumer Survey Data","Laptop computers are designed in a variety of shapes and sizes in order to satisfy diverse consumer preferences. Each design is optimized to attract consumers with a particular set of preferences for design tradeoffs. Gaining a better understanding of these tradeoffs and preferences is beneficial to both laptop designers and to consumers. This paper introduces an engineering model for laptop computer design and a demand model derived from a main-effects choice-based conjoint survey. Several demand model specifications are compared, including linear-in-parameters and discrete part-worth specifications for aggregate multinomial logit and mixed logit models. An integrated optimization scheme combines the engineering model with each demand model form for profit maximization. The solutions of different optimal laptop designs and market share predictions resulting from the unique characteristics of each demand model specification are examined and compared.",2007
"A Game-Theoretic Approach to Finding Market Equilibria for Automotive Design Under Environmental Regulation","Recent research has extended prior efforts to integrate firm-level objectives into engineering design optimization models by further enlarging the scope to investigate the effects of regulation on the design decisions of profit-seeking firms in competition. In particular, one study examined the effects of environmental policy on vehicle design decisions by integrating quantitative models of engineering performance, market demand, production cost and regulatory penalties in a joint optimization framework using game theory to model the effects of competition on design and pricing. Model complexity and the solution methods used to solve for market equilibria in prior research have led to a limitation where the prior approach is too computationally intensive to allow extensive parametric studies on the effects of policy changes on design. To address this issue, we present an alternative game-theoretic approach utilizing necessary and sufficient conditions with Nash conditions to find market equilibria in an oligopoly of automakers, and we use this approach to examine the resulting optimal design responses under various regulation scenarios.",2007
"Measurement of Headlight Form Preference Using a Choice Based Conjoint Analysis","The measurement and understanding of user aesthetic preference for form is a critical element to the product development process and has been a design challenge for many years. In this article preference is represented in a utility function directly related to the engineering representation for the automobile headlight. A method is proposed to solicit and measure customer preferences for shape of the automobile headlight using a choice task on a main-effects conjoint survey design to discover and design the most preferred shape.",2007
"Conjoint-HoQ: A Quantitative Methodology for Consumer-Driven Design","This paper presents a methodology, Conjoint-HoQ, which is meant to provide an improvement over the current House of Quality (HoQ) tool. The improvement comes in the form of ",2007
"Preference Inconsistency in Multidisciplinary Design Decision Making","Research from behavioral psychology and experimental economics asserts that individuals construct preferences on a case-by-case basis when called to make a decision. A common, implicit assumption in engineering design is that user preferences exist a priori. Thus, preference elicitation methods used in design decision making can lead to preference inconsistencies across elicitation scenarios. This paper offers a framework for understanding preference inconsistencies, within and across individual users. We give examples of three components of this new framework: comparative, internal, and external inconsistencies across users. The examples demonstrate the impact of inconsistent preference construction on common engineering and marketing design methods, including discrete choice analysis, modeling stated vs. revealed preferences, and the Kano method and thus QFD. Exploring and explaining preference inconsistencies produces new understandings of the relationship between user and product.",2007
"Theoretical Design and Control Analysis of Reconfigurable Parallel Kinematic Machine Tools","This paper introduces the theoretical design and control of reconfigurable parallel kinematic machine tools. First, the general concept of reconfigurable parallel kinematic machine (RPKM) and its growing demand are introduced. Second, the design of reconfigurable parallel kinematic machines is discussed and the geometric modeling of such structures is presented and explained. The potential applications of this type of machine are described. Finally, a case study for one of the proposed structures is conducted, including kinematic/dynamic modeling and control, some results and simulation are demonstrated.",2007
"Design for Product Adaptability","Adaptable design is a new design paradigm to create designs and products that can be easily changed to satisfy different requirements. Adaptable design aims at identifying the designs and products considering functionality, manufacturing efforts, customization and environment friendliness. This research focuses on adaptable design considering product adaptability. In this work, product adaptability is evaluated by three measures including extendibility of functions, upgradeability of modules, and customizability of components. Various design candidates created in adaptable design are evaluated by different life-cycle evaluation measures including product adaptability of design, part and assembly costs of manufacturing, and operationability by customers. Since different evaluation measures are modeled in different units, the grey relational analysis method is employed to integrate the different evaluation measures for prioritizing different design candidates. A case study is given to demonstrate the effectiveness of the introduced adaptable design approach.",2007
"Flexible and Reconfigurable Systems: Nomenclature and Review","The demands on today’s products have become increasingly complex as customers expect enhanced performance across a variety of diverse and changing system operating conditions. Reconfigurable systems are capable of undergoing changes in order to meet new objectives, function effectively in varying operating environments, and deliver value in dynamic market conditions. Research in the design of such responsive and changeable systems, however, currently faces impediments in effective and clear discourse due to ambiguity in terminology. Definitions of the terms flexibility and reconfigurability, two related concepts in reconfigurable system design, are explored based on their original lexical meanings and current understanding in design literature. Design techniques that incorporate flexibility both in the design (form) and performance (function) space are presented. Based upon this literature survey, a classification scheme for flexibility is proposed, and its application to reconfigurable system design is explored. This paper also presents recent methodologies for reconfigurable system design and poses important research questions that remain to be investigated.",2007
"Adaptive Complex Method for Efficient Design Optimization","Box’s Complex method for direct search has shown promise when applied to simulation based optimization. In direct search methods, like Box’s Complex method, the search starts with a set of points, where each point is a solution to the optimization problem. In the Complex method the number of points must be at least one plus the number of variables. However, in order to avoid premature termination and increase the likelihood of finding the global optimum more points are often used at the expense of the required number of evaluations. The idea in this paper is to gradually remove points during the optimization in order to achieve an adaptive Complex method for more efficient design optimization. The proposed method shows encouraging results when compared to the Complex method with fix number of points and a quasi-Newton method.",2007
"Approximated Unimodal Region Elimination Based Global Optimization Method for Engineering Design","Computer analysis and simulation based design optimization requires more computationally efficient global optimization tools. In this work, a new global optimization algorithm based on design experiments, region elimination and response surface model, namely Approximated Unimodal Region Elimination Method (AUREM), is introduced. The approach divides the field of interest into several unimodal regions using design experiment data; identify and rank the regions that most likely contain the global minimum; form a response surface model with additional design experiment data over the most promising region; identify its minimum, remove this processed region, and move to the next most promising region. By avoiding redundant searches, the approach identifies the global optimum with reduced number of objective function evaluations and computation effort. The new algorithm was tested using a variety of benchmark global optimization problems and compared with several widely used global optimization algorithms. The experiments results present comparable search accuracy and superior computation efficiency, making the new algorithm an ideal tool for computer analysis and simulation black-box based global design optimization.",2007
"Bayesian Analysis of Adaptive One-Factor-at-a-Time Experimentation","This paper considers the problem of achieving improvements through adaptive experimentation. To limit the focus we consider only design spaces with discrete two-level factors. We prove that, in a Bayesian framework, one factor at a time experimentation is an optimally efficient response to step by step accrual of sample information. We derive Bayesian predictive distributions for experimentation outcomes given natural conjugate priors. Using an example based on fatigue life of weld repaired castings, we show how to use our results.",2007
"Multi Agent Normal Sampling Technique (MANST) for Global Optimization","The current work discusses a novel global optimization method called the Multi-Agent Normal Sampling Technique (MANST). MANST is based on systematic sampling of points around agents; each agent in MANST represents a candidate solution of the problem. All agents compete with each other for a larger share of available resources. The performance of all agents is periodically evaluated and a specific number of agents who show no promising achievements are deleted; new agents are generated in the proximity of those promising agents. This process continues until the agents converge to the global optimum. MANST is a standalone global optimization technique. It is benchmarked with six well-known test cases and the results are then compared with those obtained from Matlab™ 7.1 GA Toolbox. The test results showed that MANST outperformed Matlab™ 7.1 GA Toolbox for the benchmark problems in terms of accuracy, number of function evaluations, and CPU time.",2007
"Non-Dominated Sorting Genetic Quantum Algorithm for Multi-Objective Optimization","This paper presents a new multi-objective optimization method, which is inspired from the idea of non-dominated sorting genetic algorithm (NSGA) and genetic quantum algorithm (GQA). The GQA has been tested on well known test beds in single objective optimization and compared with the genetic algorithm (GA) in the lead author’s previous work [22]. This paper aims to apply the idea of GQA to multi-objective optimization (MOO). The developed method is called non-dominated sorting genetic quantum algorithm (NSGQA). The developed method is tested with benchmark problems collected from literature, which have characteristics representing various aspects of a MOO problem. Test results show that NSGQA has better performance on most benchmark problems than currently popular MOO methods such as the NSGA. The integration of GQA with MOO, and the systematic comparison with other MOO methods on benchmark problems, should be of general interest to researchers on MOO and to practitioners using MOO methods in design.",2007
"Optimization of a Vehicle Space Frame Under Ballistic Impact Loading","Shock from impact loading may risk the lives of the occupants of a military vehicle and damage the sensitive electronic components within it. A finite element model (FEM) for a space-frame based military vehicle is presented in this paper. An approach is developed to optimize the design of the joints within the space frame structure to reduce the mass of the vehicle while maintaining its structural integrity. The process starts by creating a parametric FEM of the vehicle. The optimization variables are the lengths of joint branches. The effect of joint location within the space frame is also explored. The problem is subject to geometry and stress constraints. Results show that a mass reduction can be achieved without adversely affecting integrity of the vehicle.",2007
"Optimization of Bicycle Frames Using Genetic Algorithms","A bicycle frame is optimized for the lightest weight by using genetic algorithms in this study. Stresses of five rods in the bicycle frame less than the material yielding strength with consideration of the factor of safety are the constraints. A two-dimensional model of the frame is created. Equilibrium equations are derived and loads acting on rods are determined. A known function is used to verify feasibility of the program generated. Effects of the mutation rate, the crossover rate and the number of generation on the mean and the standard deviation of the fitness value are studied. The optimal solutions with the outer diameters and the inner diameters of the front frame rods to be 0.040 m and 0.038 m, respectively, the outer diameters and the inner diameters of the rear frame rods to be 0.024 m and 0.021m, respectively, and the weight of the bicycle frame to be 0.896 kg are recommended for the bicycle frame.",2007
"Generalization of Topological Sensitivity and Its Application to Defeaturing","A particularly challenging problem in CAD/CAE is the handling of small geometric details during finite element analysis (FEA). The presence of such details can significantly increase the computational complexity of FEA, while hindering its automation. Therefore, designers typically resort to ",2007
"A Hybrid Method for Surrogate Model Updating in Engineering Design Optimization","This paper addresses the critical issue of effectiveness, efficiency, and reliability in simulation-based design optimization under surrogate model uncertainty. Specifically, it presents a novel method to build surrogate models iteratively with sufficient fidelity for accurately capturing global optimal design solutions at a minimal cost. The salient feature of the proposed method lies in its unique preference of focusing necessarily high fidelity at potential global optimal regions of surrogate models. The proposed method is the synergic integration of the multiple preference point method, which updates surrogate model at current local optimal points predicted with data-mining techniques in genetic algorithm setup, and the maximum variance point method, which updates surrogate model at the point associated with the maximum prediction variance. Through illustrative comparison studies on thirty different optimization scenarios derived from 15 different test functions, the proposed method demonstrates the tangible reliability advancement. The experimental results indicate that the proposed method can be a reliable updating method in surrogate-model-based design optimization for efficiently locating the global optimal point/points in various kinds of optimization scenarios featured by single/multiple global optimal point/points that may exist at the corners of design space, inside design space, or on the boundaries of design space.",2007
"Pseudo-Hierarchical Multistage Model for System of Systems Design and Operations","This paper describes a multilevel, multistage approach to system of systems design optimization where a system design is linked with system allocation along the multistage decision making horizon. The approach is composed of two parts: pseudo-hierarchical formulation (i.e., how to model the ",2007
"Finite Element Analysis Applied to Ergonomic Design of 2-Piece Aluminum Beverage Cans and Bottles","This paper has introduced the finite element analysis (FEA) into the ergonomic design to evaluate the human feelings numerically and objectively, and then into the optimization design of beverage containers considering human factors. In the design of the end of can (the lid of can), experiments and the FEA of indenting vertically the fingertip pulp by a probe and the tab of end have been done to observe force responses and to study feelings in the fingertip. A numerical simulation of finger lifting the tab for opening the can has also been performed, and discomfort in the fingertip has been evaluated numerically to present the finger-accessibility of the tab. The comparison of finger-accessibility between two kinds of tab ring shape designs showed that the tab that may have a larger contact area with the finger is better. In the design of beverage bottles served hot drinks, the FEA of tactile sensation of heat has been performed to evaluate numerically the touch feeling of the finger when holding the hot bottle. The numerical simulations of embossing process have also been performed to evaluate the formability of various rib-shape designs. The optimum design has then been done considering the hot touch feeling as well as the metal sheet formability.",2007
"Including Preference in Anthropometry-Driven Models for Design","In the design of artifacts that interact with people, the spatial dimensions of the user population are often used to size and engineer the artifact. The variability in body dimensions (called “anthropometry”) is used to indicate how much adjustability or how many sizes are required to accommodate the intended user population. However, anthropometry is not the only predictor of these kinds of interactions. For example, two vehicle drivers with similar body dimensions might have very different preferred locations for the seat. The variability not predicted by body dimensions can be considered “preference”. Well-conceived models considering all sources of variability can can facilitate the application of design automation tools such as optimization and robust design methodologies, resulting in products that are safer, cost effective, and more accessible to broader populations (including people with disabilities). In contrast, poor models and those that fail to include a preference component can produce misleading results that under- or over-approximate accommodation and prescribe inappropriate amounts of adjustability. This paper reviews common methods of designing for human variability, demonstrating the use and strengths and weaknesses of each. This is done in the context of a simple, univariate case study to determine the appropriate allocation of adjustability to achieve a desired accommodation level.",2007
"Evaluation Method of Drinking Ease for Aluminum Beverage Bottles","This paper has investigated effects of the bottle opening size on drinking feelings in order to improve the comfort level of consumers when drinking directly from the opening of aluminum bottle. A survey over 120 subjects has been performed based on a drinking test using three kinds of bottles with opening diameters of 28, 33 and 38 mm, respectively. Two questionnaires have been conducted. Statistical analysis results of Questionnaire 1 have shown that 33-mm opening is best for adult consumers with no matter the type of contents, gender and the mouth size. The factor analysis results of Questionnaire 2 based on Kansei Engineering have shown that drinking feeling is affected by two common factors, which considered as the flow from the bottle to the mouth and the flow adjustability. Moreover, the fluid-dynamics analysis model has been developed to simulate the bottled liquid in a drinking action consisting of survey results and experimental observations of consumers’ drinking actions. Numerical simulations have been performed to understand how consumers control the flow during the drinking actions. It is found that the consumers usually try to realize the ideal and preferable condition by adjusting the inclination angle of the bottle.",2007
"Adaptive Deposition Coverage Toolpath Planning for Metal Deposition Process","Coverage toolpath planning is very critical to deposition quality in layered manufacturing especially for metal deposition processes. The correct choice of toolpath patterns will make it possible to build a fully dense and functional metal part. The major consideration when selecting a toolpath pattern is the complete coverage of the to-be-deposited geometry which means no voids should happen. This paper presents the research on the toolpath coverage efficiency and the strategies to predict the possibility of the occurrence of deposition voids so that the appropriate toolpath pattern can be applied to avoid deposition voids. The contour-parallel offsetting pattern and the adaptive zigzag toolpath pattern will be applied as the alternate options and the final adaptive deposition coverage toolpath will be the combination of these two basic patterns depending on the prediction results of the occurrence of the deposition voids. The experiment has demonstrated that the adaptive toolpath pattern can greatly improve the reliability of the coverage path planning in deposition processes.",2007
"Automated Synthesis of MEMS Fabrication Sequences Using Graph Grammars","The design of any MEMS component is subject to stringent manufacturing constraints. The knowledge about these constraints seems to be available to designers who have experienced the details of MEMS fabrication. In this paper, we put forth the idea of automatically generating a fabrication sequence for surface micro-machined MEMS components using the knowledge stored in grammar rules. As an analogy to CAD tools used in mechanical systems, we envision creating a tool which has the Pro-Engineer approach of determining fabrication sequences for a machine tool based on the final part shape. This tool could be an integral addition to the current MEMS design software so that the designers can freely draft devices and then allow an automated process to determine the fabrication sequence. In this paper we give a brief introduction about the graph grammars. Data from already designed MEMS components is extracted in the form of rules to create an expert system. We have also included an example of generation of the fabrication sequence for several MEMS components.",2007
"Adaptive Slicing of Moving Least Squares Surfaces: Toward Direct Manufacturing of Point Set Surfaces","Rapid advancement of 3D sensing techniques has lead to dense and accurate point cloud of an object to be readily available. The growing use of such scanned point sets in product design, analysis and manufacturing necessitates research on direct processing of point set surfaces. In this paper, we present an approach that enables the direct layered manufacturing of point set surfaces. This new approach is based on adaptive slicing of moving least squares (MLS) surfaces. Salient features of this new approach include: 1) it bypasses the laborious surface reconstruction and avoids model conversion induced accuracy loss; 2) the resulting layer thickness and layer contours are adaptive to local curvature and thus it leads to better surface quality and more efficient fabrication; 3) the MLS surface naturally smoothes the point cloud and allows up-sampling and down-sampling, and thus it is robust even for noisy or sparse point sets. Experimental results of the slicing algorithm on both synthetic and scanned point sets are presented.",2007
"Product Reverse Engineering Based on Growth Design Process","Current reverse engineering approach is an effective way for technology progress of developing countries. Based on analysis of existing reverse engineering technology, a new concept of Product Reverse Engineering (PRE) is proposed and its theoretical framework is discussed first in this paper in order to extend its application from components to the overall product structure and design process. Then a brief introduction is made to the technical system architecture and key techniques for PRE, which include the rapid solid modeling, integrating with existing CAD systems through STEP file, assembly modeling for conceptual structure, and reverse design process reconstruction. Finally, a prototype system PRE-DARFAD is developed with initial verification by a fixture design example.",2007
"High Performance Dirichlet Parametrization Through Triangular Bezier Surface Interpolation for Deformation of CAE Meshes",,2007
"Curve Shape Modification and Fairness Evaluation","A method to generate a quintic NURBS curve which passes through the given points is described. In this case, there are four more equations than there are positions of the control points. Therefore, four gradients which are the first derivative of a NURBS equation are assigned to the given points. In addition to this method, another method to generate a quintic NURBS curve which passes through the given points and which has the first derivative at these given points is described. In this case, a linear system will be underdetermined, determined or overdetermined depending on the number of given points with gradients. A method to modify NURBS curve shape according to the specified radius of curvature distribution to realize an aesthetically pleasing freeform curve is described. The differences between the NURBS curve radius of curvature and the specified radius of curvature is minimized by introducing the least-squares method. A criterion for a fair curve is proposed. Evaluation whether the designed curve is fair or not is accomplished by a comparison of the designed curve to a curve whose radius of curvature is monotone. The radius of curvature is specified by linear, quadratic, and cubic function using the least-squares method. A curve whose radius of curvature is reshaped by one of these algebraic functions is considered as a fair curve. The curvature vector of the curve is used to evaluate the fairness. The comparison of unit curvature vectors is used to evaluate the directional similarity of the curve. The comparison of the curvature is used to evaluate the similarity of the magnitude of curvature vectors. If the directional similarity of the designed curve is close to the fair curve, and also the similarity of the curvature is close to the fair curve, the designed curve can be judged as a fair curve.",2007
"Application of Interactive Deformation to Assembled Mesh Models for CAE Analysis","Mesh deformation, which is sometimes referred to as mesh morphing in CAE, is useful for providing various shapes of meshes for CAE tools. This paper proposes a new framework for interactively and consistently deforming assembly models of sheet structure for mechanical parts. This framework is based on a surface-based deformation, which calculates the vertex positions so that the mean curvature normal is preserved at each vertex in a least squares sense. While existing surface-based deformation techniques cannot simultaneously deform assembly mesh models, our method allows us to smoothly deform disconnected meshes by propagating the rotations and translations through disconnected vertices. In addition, we extend our deformation technique to handle non-manifold conditions, because shell structure models may include non-manifold edges. We have applied our method to assembly mesh models of automobile parts. Our experimental results have shown that our method requires almost the same pre-processing time as existing methods and can deform practical assembly models interactively.",2007
"Pre-Programmed Failure Behavior Using Biology-Inspired Structures","Core (filler) materials are key components of the sandwich panel and box-beams that are used in the design of lightweight structures. They perform a variety of elastic-range functions such as transferring and supporting working stresses and energy and collapse management. There is an increasing demand, however, for post-yield performance characteristics such as buckling control, impact toughness, and maintenance of component strength after damage. Low density is also an important consideration, as overall component mass is critical in most applications. These cellular solids need to perform well under normal working stress conditions, yet still resist damage from simple and unavoidable low velocity impacts. A new design approach is suggested by biological systems that have evolved for toughness and damage tolerance (bones, trees, plants, corals, etc.). These systems share the relatively low density cellular arrangements of common synthetic core materials, but also exhibit variable density gradients within the core. (Figures 1 and 2) This paper describes engineering design methods that are inspired by such biology. The result is that a design’s failure modes can be more effectively “designed-in”, controlling locations and amounts of failure.",2007
"Finding All Undercut-Free Parting Directions for Solids of Revolution","For molding and casting processes, geometries that have undercut-free parting directions (UFPDs) are preferred for manufacturing. However, existing approaches either cannot identify all UFPDs or cannot run at interactive speeds (the best exhaustive algorithm, unimplemented, runs at O(n4 ) time theoretically). Our proposed feature-based approach avoids testing the whole Gaussian sphere of potential directions by first efficiently identify all UFPDs for individual features such as extruded and revolved features, thus significantly reducing test space and running time. In this paper, we describe a fast algorithm to find all UFPDs for solids of revolution. The algorithm is based on analyzing the constructing 2D generator profiles, building on our previous results for 2-moldability analysis of polygons. The running time is O(n), where n is the geometric complexity of the 2D generator profile. For parts containing multiple solids of revolution, the set of possible UFPDs can be significantly reduced based upon an analysis of each such feature, efficiently identifying many as non-2-moldable or reducing the search space for exhaustive algorithms that find all UFPDs.",2007
"Wavelet SDF-Reps: Solid Modeling With Volumetric Scans","This paper describes a new formulation of solid modeling that addresses the issue of including parts whose geometry is determined from volumetric scans (CT, MRI, PET, etc.) along with parts whose geometry is designed by traditional computer-aided design (CAD) operations. Such issues arise frequently in the design of medical devices or prostheses where fit and/or interference between man-made artifacts and existing anatomy are essential considerations, but the modeling formulation presented is not limited to medical applications and can be applied to any parts whose volume can be actually or virtually scanned. Scanner data typically comprises a grid of intensity values and segmentation must be performed to determine the extent of the part. In current practice, the segmented scanner data is run through a polygonizer to obtain an approximate tessellation of the object’s surface. Even in the best case scenario where the triangles obtained form a closed surface that accurately approximates the surface of the scanned object, such triangulated models can be problematic due to excessive size. We present an alternative approach based on recent advances in segmentation with level set methods. The output of the level set computation is a grid of approximate values for the signed distance from each grid point to the nearest point on the surface of the scanned object. We propose interpolating the grid of signed distance values to obtain an implicit or function-based representation (f-rep) for the object, and we introduce appropriate wavelets to effectively perform the interpolation while also providing a number of other useful properties including data compression, inherently multi-scale modeling, and capabilities for skeletal-based modeling operations.",2007
"Matching Geometric Shapes in 4D Space Incorporating Curvature Information","The Iterative Closest Point (ICP) algorithm and its variants are widely used in matching different patches of 3-Dimensional (3D) scanning data. In this paper, a 4-Dimensional (4D) based approach is proposed to improve the robustness of the ICP algorithm. Considering curvatures of the given geometries as an extra dimension, the existing ICP algorithm can be extended to 4D space. The reason of using this additional information is that it introduces an extra dimension of similarity in the shape matching algorithm, thus improves the effectiveness of the optimization process. Using a variant of the Laplacian smoothing tool, high frequency noise and interferences in the curvature domain are suppressed and the principal geometric features are addressed. By a 4D to 3D orthogonal projection, the matched geometries are projected back to 3D space, where the existing ICP algorithm in 3D is applied as a fine-tuning tool. Numerical implementations on several sets of scanning data demonstrate the robustness of the proposed method. The converging process and the speed of the propose method are investigated as well.",2007
"Motion-Based Shape Deformation of Solid Models","Mechanical designs undergo numerous geometric changes throughout the design process. Performing these changes relies, whenever possible, on the parametric models used to create the initial geometry. However, a number of open issues prevent the current parametric modeling systems to support many practical design situations, which, in turn, forces the geometry to evolve independently of the original parametric model. The fact that every parametric update can be expressed in terms of a sequence of shape deformations implies that the same geometric updates could be obtained, at least in principle, via shape deformation procedures that parameterize the deformation itself. In this paper we propose a new approach to create and edit solid models by introducing a geometric deformation procedure that relies on motion interpolation. We show that the proposed approach induces a parametrization of the deformation that allows direct control and editing of the deformation, is capable of preserving important geometric invariants such as constant cross-sectional properties of the deformed models, and maintains the ability to perform parametric optimization of the associated solid models. We conclude by discussing advantages and limitations of this approach as well as a number of important research directions that we will pursue in the near future.",2007
"A Point Membership Classification for Sweeping Solids","Sweeps are considered to be one of the basic representation schemes in solid modeling, and have numerous applications in very diverse fields ranging from engineering design and manufacturing to computer graphics. Despite their prevalence, many properties of the general sweeps are not well understood. Furthermore, boundary evaluation algorithms for 3-dimensional solid objects currently exist only for reasonably simple objects and motions. One of the main reasons for this state of affairs is the lack of a generic point membership test for sweeps. In this paper we describe a point membership classification (PMC) for sweeping solids of arbitrary complexity moving according to one parameter ",2007
"Quasi-Optimal Mesh Segmentation Via Region Growing/Merging","Recently meshes of engineering objects are easily acquired by 3D laser or high energy X-ray CT scanning systems, and these meshes are widely used in product developments. To effectively use scanned meshes in engineering applications, such as inspection, CAD model reconstruction, and convergent-type CAE, we need to segment meshes and extract desirable regions and their approximating surfaces as preprocessing. Engineering objects are commonly represented as a set of analytic surfaces, such as planes, cylinders, spheres, cones, and tori. Therefore, the mesh surface of engineering objects needs to be approximated as a set of analytic surfaces. Moreover, a mesh surface should be approximated with a minimum number of analytic surfaces and their approximating error should be minimized as a result of segmentation. We call the segmentation that satisfies these two conditions the ",2007
"Development of Validation Metrics for Vehicle Frontal Impact Simulation","This research addresses the development of validation metrics for vehicle frontal impact simulation. The model validation metrics provide a quantified measurement of the difference between CAE simulation and physical test. They are useful to develop an objective model evaluation procedure for eventually achieving the goal of zero or near zero prototyping. In this research, full frontal crash pulses are chosen as the key items to be compared in the vehicle frontal impact simulation. Both physics- and mathematics-based metrics are investigated. The physics-based metric include a method of using a simplified step function representation and the mathematics-based metrics include methods of wavelet decomposition, corridor violation plus area, and metrics used in a commercial code ADVISER, respectively. They are all correlated to subject matter experts’ rating through optimal weightings. A new metric, considering variabilities from both experts and metrics for frontal crash pulse, is proposed. One example is used to demonstrate its application.",2007
"A Development Methodology for Parametric Synthesis Tools","Software to support the solution generation phase of the engineering design process has been developed in academia for decades. Computational synthesis software enables generation of solutions on both conceptual and embodiment level. This paper focuses on the class of parametric design, such as documented in mechanical engineering handbooks. Examples include machine elements such as bearings, springs, fasteners, transmissions, etc. A parametric synthesis tool automates the engineering design process from functional requirements to quantified solutions, for a single machine element. Since the amount of machine elements is vast and software development time should be low, a generic methodology is helpful to speed up this process. This paper discusses such a methodology to develop synthesis tools for the class of parametric designs. It includes an analysis-oriented approach to formalize the design process’ parameters in terms of embodiment, performance and scenario. Mathematical constraint solving techniques are used to generate candidate solutions. Graphical presentation and exploration of the solution space is done with interactive plots. A standardized layout for the graphical user interface is suggested to allow uniform and intuitive use. A demonstrator is developed using the described methodology and several challenges are discussed for improved constraint solving techniques, more advanced visualization and handling problems with higher complexity. Although small in size, parametric design processes are time consuming due to their reoccurring nature. Developing synthesis tools for these designs will allow engineers to save time and improve design quality.",2007
"Incorporating Customer Preferences and Market Trends in Vehicle Package Design","Demand models play a critical role in enterprise-driven design by expressing revenues and costs as functions of product attributes. However, existing demand modeling approaches in the design literature do not sufficiently address the unique issues that arise when complex systems are being considered. Current approaches typically consider customer preferences for only quantitative product characteristics and do not offer a methodology to incorporate customer preference-data from multiple component/subsystem-specific surveys to make product-level design trade-offs. In this paper, we propose a hierarchical choice modeling approach that addresses the special needs of complex engineering systems. The approach incorporates the use of qualitative attributes and provides a framework for pooling data from multiple sources. Heterogeneity in the market and in customer-preferences is explicitly considered in the choice model to accurately reflect choice behavior. Ordered logistic regression is introduced to model survey-ratings and is shown to be free of the deficiencies associated with competing techniques, and a Nested Logit-based approach is proposed to estimate a system-level demand model by pooling data from multiple component/subsystem-specific surveys. The design of the automotive vehicle occupant package is used to demonstrate the proposed approach and the impact of both packaging design decisions and customer demographics upon vehicle choice are investigated. The focus of this paper is on demonstrating the demand (choice) modeling aspects of the approach rather than on the vehicle package design.",2007
"A Case Study of Design Process and Development of a Design Enabling Tool for Wright Metal Products","This case study investigates the design process followed by a small to medium scale enterprise (SME) that primarily depends on special expertise in the form of a few key individuals, who design products mainly based on past experience, augmented by trial and error. This is an inefficient, time consuming, and expensive way of designing products and evaluating their performance. This led to development of a specialized and affordable design enabler that facilitates engineering analysis, noticeably absent in SME’s current design process. Further, the design enabler forms the foundation for extending the scope to include rule-based systems, optimization and case based reasoning that would assist designers in efficient product development.",2007
"A Process Model and Data Mining to Support Designing Families of Services","Product family design is a cost-effective way to achieve mass customization by allowing highly differentiated products to be developed from a common platform while targeting individual products to distinct market segments. Recent trends seek to apply and extend principles from product family design to new service development. In this paper, we extend concepts from platform-based product family design to create a novel methodology for module-based service family design. The new methodology helps identify a service platform along with variant and unique modules in a service family by integrating service-based process analysis, ontologies, and data mining. A function-process matrix and a service process model are investigated to define the relationships between the service functions and the service processes offered as part of a service. An ontology is used to represent the relationships between functional hierarchies in a service. Fuzzy clustering is employed to partition service processes into subsets for identifying modules in a given service family. The clustering result identifies the platform and its modules using a platform level membership function. We apply the proposed methodology to determine a new platform using a case study involving a family of banking services.",2007
"Quantity Dimension Indexing for Design Knowledge Management","Although many knowledge management techniques based on text expression have been developed, they are not necessarily sufficient for managing engineering design knowledge. In this paper, we propose quantity dimension indexing of design knowledge as a fundamental method for design knowledge management. Physical quantities describing physical phenomena can be represented as vectors in a seven-dimensional space where the orthogonal axes are the seven base units of the SI (The International System of Units). Because of the generality, objectivity and universality of the SI, this space covers all physical quantities that appear in the past, present and future design knowledge and design problems, and the same quantities are represented as the same vectors regardless of the differences in people, products, domains, organizations, nations and languages. We assume that the similarities of physical phenomena lead to similarities in the dimensions of quantities describing the phenomena, and propose to use this seven-dimensional vector for estimating the similarity of design knowledge from the viewpoint of physical phenomena. Based on this basic idea, we mathematically define similarity between two quantities using quantity dimensions. We prepared design knowledge examples and retrieval keys and conducted design knowledge retrieval and design knowledge similarity estimation by quantity dimension indexing and confirmed that we obtained adequate results without using a concept dictionary or thesaurus elaborated in advance, which are indispensable in the text approach.",2007
"Representation and Analysis Challenges in Design for Part-Reuse: An Automotive Case Study","Product Platform and Product Family Design is reshaping the way that many companies develop products. But how well are the CAD and PLM technologies keeping pace with this advancement? This paper presents data from a three month ethnographic study of an expert automotive body engineer. His assignment is to modify the design of an existing body structural member for use in the next-generation vehicle. The modification was necessitated by manufacturability issues. Observations and subsequent interviews revealed that manipulation time, model reuse and representation of part interfaces (such as welds) presented challenges to the body engineer and collaborating analysis engineers. Despite re-use of a physical part, the engineer had to create a new CAD model. The redesign involved breaking the original part into two pieces. The engineer sketched initial design concepts on paper because manipulation time in the CAD system was so lengthy. After determining the design concept, the engineer created a new CAD model, including new weld locations, and passed it along to analysis engineers for stiffness and crashworthiness FEA testing. Hand-offs between design and analysis engineers were challenged by the PLM system. The paper ends by making recommendations for improving CAD and PLM tools.",2007
"Empirical Models for Non-Deterministic Simulation-Based Robust Design","We propose a method for metamodeling non-deterministic computer intensive simulations for use in robust design. Generalized linear models for mean responses and heteroscadastic response variances are iteratively estimated in an integrated manner. Estimators that may be used for predicting the mean and variance models are introduced and metamodels of variance are developed. The usefulness of this metamodeling approach in efficient uncertainty analyses of non-deterministic, computationally-intensive simulation models for robust design methods is illustrated with the example of the design of a linear cellular alloy heat exchanger with randomly distributed cracks in the cell walls.",2007
"Managing Design Process Complexity: A Value-of-Information Based Approach for Scale and Decision Decoupling","Design processes for multiscale, multifunctional systems are inherently complex due to the interactions between scales, functional requirements, and the resulting design decisions. While complex design processes that consider all interactions lead to better designs; simpler design processes where some interactions are ignored are faster and resource efficient. In order to determine the right level of simplification of design processes, designers are faced with the following questions: ",2007
"Q2S2: Merging Qualitative Information in Sequential DOE","Sequential sampling refers to a set of design of experiment (DOE) methods where the next sample point is determined by information from previous experiments. This paper introduces a qualitative and quantitative sequential sampling (Q2S2) technique, in which optimization and user knowledge is used to guide the efficient choice of sample points. This method combines information from multiple fidelity sources including computer simulation models of the product, first principals involved in design, and designer’s qualitative intuitions about the design. Both ",2007
"Using Maximum Likelihood Estimation to Estimate Kriging Model Parameters","A kriging model can be used as a surrogate to a more computationally expensive model or simulation. It is capable of providing a continuous mathematical relationship that can interpolate a set of observations. One of the major issues with using kriging models is the potentially computationally expensive process of estimating the best model parameters. One of the most common methods used to estimate model parameters is Maximum Likelihood Estimation (MLE). MLE of kriging model parameters requires the use of numerical optimization of a continuous but possibly multi-modal log-likelihood function. This paper presents some enhancements to gradient-based methods to make them more computationally efficient and compares the potential reduction in computational burden. These enhancements include the development of the analytic gradient and Hessian for the log-likelihood equation of a kriging model that uses a Gaussian spatial correlation function. The suggested algorithm is very similar to the Scoring algorithm traditionally used in statistics, a Newton-Raphson gradient-based optimization method.",2007
"Reducing the Number of Variables in a Response Surface Approximation: Application to Thermal Design","Response surface approximations (RSA) are a common tool in engineering, often constructed based on finite element (FE) simulations. For some design problems, the FE models can involve a high number of parameters. However it is advantageous to construct the RSA as function of a small number of variables. The purpose of this paper is to demonstrate that a significant reduction in the number of variables needed for an RSA is possible through physical reasoning, dimensional analysis and global sensitivity analysis. This approach is demonstrated for a transient thermal problem, but it is applicable to any FE based surrogate model construction. The thermal problem considered is the design of an integrated thermal protection system (ITPS) for spacecraft reentry where an RSA of the maximum bottom face temperature was needed. The FE model used to evaluate the maximum temperature depended on 15 parameters of interest for the design: 9 thermal material properties and 6 geometric parameters of the ITPS panel. A small number of assumptions simplified the thermal equations allowing easy nondimensionalization, which together with a global sensitivity analysis showed that the maximum temperature mainly depends on only two nondimensional parameters. These were selected to be the design variables of the RSA for maximum temperature. The RSA was still fitted to the original non-simplified FE simulations. Having only two variables allowed a dense design of experiments thus providing a very good quality of fit. Consequently the major error remaining in the RSA is due to the fact that the two nondimensional variables account for only part (albeit the major part) of the dependence on the original 15 variables. This error was checked and good agreement was found. The two-dimensional nature of the RSA allowed graphical representation, which was used for material selection from among hundreds of possible materials for the design optimization of an ITPS panel.",2007
"Reduction of Non-Monomial Basis in the Dimensional Analysis of a Dynamical System","Dimensional Analysis (DA) is a tool often used to relate models and specimens to the actual product or system based on the hypothesis that the two regimes follow the same physical laws and are hence dimensionally equivalent. While this has been a conventional use of the process, we extend the technique to dynamic systems to develop state equations that allow for design studies and optimization. A methodical approach is detailed coupled with an example of a toy water-rocket assembly. A modified methodology (heuristic) is also discussed to condense non-monomial basis systems using simple physical laws and a novel reduction process. Experimental verification is provided to complement the analysis procedure. Efficacy of the process is highlighted in comparison with the conventional close-form approach.",2007
"A New Variable Fidelity Optimization Framework Based on Model Fusion and Objective-Oriented Sequential Sampling","Computational models with variable fidelity have been widely used in engineering design. To alleviate the computational burden, surrogate models are used for optimization without recourse to expensive high-fidelity simulations. In this work, a model fusion technique based on Bayesian Gaussian process modeling is employed to construct cheap, surrogate models to integrate information from both low-fidelity and high-fidelity models, while the interpolation uncertainty of the surrogate model due to the lack of sufficient high-fidelity simulations is quantified. In contrast to space filling, the sequential sampling of a high-fidelity simulation model in our proposed framework is objective-oriented, aiming for improving a design objective. Strategy based on periodical switching criteria is studied which is shown to be effective in guiding the sequential sampling of a high-fidelity model towards improving a design objective as well as reducing the interpolation uncertainty. A design confidence (DC) metric is proposed to serves as the stopping criterion to facilitate design decision making against the interpolation uncertainty. Numerical and engineering examples are provided to demonstrate the benefits of the proposed methodology.",2007
"Optimal Partitioning and Coordination Decisions in Decomposition-Based Design Optimization","Solution of complex system design problems using distributed, decomposition-based optimization methods requires determination of appropriate problem partitioning and coordination strategies. Previous optimal partitioning techniques have not addressed the coordination issue explicitly. This article presents a formal approach to simultaneous partitioning and coordination strategy decisions that can provide insights on whether a decomposition-based method will be effective for a given problem. Pareto-optimal solutions are generated to quantify tradeoffs between the sizes of subproblems and coordination problems, as measures of the computational costs resulting from different partitioning-coordination strategies. Promising preliminary results with small test problems are presented. The approach is illustrated on an electric water pump design problem.",2007
"Multiobjective Collaborative Robust Optimization (McRO) With Interval Uncertainty and Interdisciplinary Uncertainty Propagation","Real-world engineering design optimization problems often involve systems that have coupled disciplines with uncontrollable variations in their parameters. No approach has yet been reported for the solution of these problems when there are multiple objectives in each discipline, mixed continuous-discrete variables, and when there is a need to account for uncertainty and also uncertainty propagation across disciplines. We present a Multiobjective collaborative Robust Optimization (McRO) approach for this class of problems that have interval uncertainty in their parameters. McRO obtains Multidisciplinary Design Optimization (MDO) solutions which are as best as possible in a multiobjective and multidisciplinary sense. For McRO solutions, the sensitivity of objective and/or constraint functions is kept within an acceptable range. McRO involves a technique for interdisciplinary uncertainty propagation. The approach can be used for robust optimization of MDO problems with multiple objectives, or constraints, or both together at system and subsystem levels. Results from an application of the approach to a numerical and an engineering example are presented. It is concluded that the McRO approach can solve fully coupled MDO problems with interval uncertainty and can obtain solutions that are comparable to an all-at-once robust optimization approach.",2007
"Integrated Structure Decomposition, Optimization and Control Design","In this paper, an integrated optimization, controller design and reduced order finite element modeling based approach is presented for structural design. The proposed approach involves structure decomposition, subcontroller design, system controller assembly, and multiobjective optimization. The concept of structure decomposition with compatible and incompatible interfaces is presented for a control/optimum system problem, and developed for problems with compatible interfaces involving substructure controller design and multiobjective optimization. The substructure information obtained through finite element analysis is synthesized to reconstruct a reduced order model for the entire structure. Based on SSSC (Substructure Synthesis-Substructure Controller), a controller is designed for each substructure. The global controller is obtained by assembling all subcontrollers designed at the substructure level. A multiobjective optimum formulation is presented based on structure decomposition and controller design. Four objective functions are simultaneously optimized. These include a stability robustness index, structural weight, controller energy, and a controller performance index. Numerical examples are presented to demonstrate the effectiveness of the proposed methodology. Results obtained using the proposed approach are compared with those obtained from optimization of the entire structure.",2007
"A Sequential Linear Programming Coordination Algorithm for Analytical Target Cascading","Decomposition-based strategies, such as analytical target cascading (ATC), are often employed in design optimization of complex systems. Achieving convergence and computational efficiency in the coordination strategy that solves the partitioned problem is a key challenge. A new convergent strategy is proposed for ATC, which coordinates the interactions among subproblems using sequential lineralizations. Linearity of subproblems is maintained using ",2007
"Diagonal Quadratic Approximation for Parallelization of Analytical Target Cascading","Analytical Target Cascading (ATC) is an effective decomposition approach used for engineering design optimization problems that have hierarchical structures. With ATC, the overall system is split into subsystems, which are solved separately and coordinated via target/response consistency constraints. As parallel computing becomes more common, it is desirable to have separable subproblems in ATC so that each subproblem can be solved concurrently to increase computational throughput. In this paper, we first examine existing ATC methods, providing an alternative to existing nested coordination schemes by using the block coordinate descent method (BCD). Then we apply diagonal quadratic approximation (DQA) by linearizing the cross term of the augmented Lagrangian function to create separable subproblems. Local and global convergence proofs are described for this method. To further reduce overall computational cost, we introduce the truncated DQA (TDQA) method that limits the number of inner loop iterations of DQA. These two new methods are empirically compared to existing methods using test problems from the literature. Results show that computational cost of nested loop methods is reduced by using BCD and generally the computational cost of the truncated methods, TDQA and ALAD, are superior to other nested loop methods with lower overall computational cost than the best previously reported results.",2007
"A Concept for a Material That Softens With Frequency","This work presents design concepts to synthesize composite materials with special dynamic properties, namely, materials that soften at high frequencies. Such dynamic properties are achieved through the use of a two-phase material that has inclusions of a viscoelastic material of negative elastic modulus in a typical matrix phase that has a positive elastic modulus. A possible realization of the negative stiffness inclusion phase is presented. A numerical homogenization technique is used to compute the average viscoelastic properties of the composite. The method and the properties of a composite material designed with it are demonstrated through an example.",2007
"A Template-Based Approach for Multilevel Blast Resistant Panel Design","Multilevel design is a subset of engineering design in which design problems are defined and analyzed at various levels of model complexity or resolution. Due to the potential for propagated uncertainty in a multilevel design process, design goals for maximizing system robustness to uncertainty in noise and control factors are included in the Blast resistant panels (BRP) design process. Blast resistant panels (BRPs) are sandwich structures consisting of two solid panels surrounding a honeycomb core. Under impulse loading, BRPs experience less deflection than similarly loaded solid panels of equal mass due to core crushing. In order to manage complexity in BRP concurrent product and materials design, a multilevel design approach is proposed. Additionally, in order to collect and store BRP design information in a modular and reusable format, a template-based design approach is implemented in BRP multilevel design. In this paper, a generic multilevel design template based on existing design methods (the compromise Decision Support Problem and the Inductive Design Exploration Method) is presented. The multilevel design template is then particularized and applied to BRP preliminary design, highlighting the advantages of a templatebased approach to multilevel design.",2007
"Predictive Carbon Nanotube Models Using the Eigenvector Dimension Reduction (EDR) Method","It has been reported that a carbon nanotube (CNT) is one of the strongest materials with their high failure stress and strain. Moreover, the nanotube has many favorable features, such as high toughness, great flexibility, low density, and so on. This discovery has opened new opportunities in various engineering applications, for example, a nanocomposite material design. However, recent studies have found a substantial discrepancy between computational and experimental material property predictions, in part due to defects in the fabricated nanotubes. It is found that the nanotubes are highly defective in many different formations (e.g., vacancy, dislocation, chemical, and topological defects). Recent parametric studies with vacancy defects have found that the vacancy defects substantially affect mechanical properties of the nanotubes. Given random existence of the nanotube defects, the material properties of the nanotubes can be better understood through statistical modeling of the defects. This paper presents predictive CNT models, which enable to estimate mechanical properties of the CNTs and the nanocomposites under various sources of uncertainties. As the first step, the density and location of vacancy defects will be randomly modeled to predict mechanical properties. It has been reported that the Eigenvector Dimension Reduction (EDR) method performs probability analysis efficiently and accurately. In this paper, Molecular Dynamics (MD) simulation with a modified Morse potential model is integrated with the EDR method to predict the mechanical properties of the CNTs. To demonstrate the feasibility of the predicted model, probabilistic behavior of mechanical properties (e.g., failure stress, failure strain, and toughness) is compared with the precedent experiment results.",2007
"A Function-Based Approach for Integrated Design of Material and Product Concepts","Designing advanced multifunctional materials and products in an integrated fashion starting from the conceptual stage provides designers with increased flexibility to achieve system performance goals that were not previously achievable. Today however, product designers commonly select more or less advanced materials from selection charts or catalogs, rather than designing them along with the product from the conceptual stage on. In order to increase a designer’s flexibility in the conceptual stage and render conceptual materials design more systematic, hence less ad-hoc and intuitive, the main contribution is the development of a systematic approach to the integrated design of material and product concepts from a systems perspective. This systematic approach is focused on developing multilevel function structures, including the material levels. Based on functional analysis, abstraction and synthesis, multiscale phenomena and associated governing solution principles are mapped to functional relationships. Hence, multilevel function structures are embodied into principal solution alternatives based on comprehensive identification and integration of phenomena and associated governing solution principles occurring at multiples levels and time and length scales. In this paper, the function-based approach to integrated design of material and product concepts is illustrated through the systematic design of reactive material containment system concepts. Having developed an overall reactive material containment system function structure, a more detailed function structure on the materials level is created. For dominating functional relationships at the materials level, governing solution principles are identified on multiple scales. The most promising solution principles are then classified in morphological charts. Combining solution principles in a systematic fashion including the materials level, product and material system concepts are identified. The most promising system concepts, in other words the principal solution alternatives that narrow the gap to desired system performance goals, are selected and illustrated in concept selection charts. A selected material and product system concept is then characterized in terms of its specific properties, which are to be tailored to the functional requirements and performance goals in subsequent embodiment design processes. By developing concepts of the product and material as an integrated system, materials design becomes more systematic and hence less ad-hoc and intuitive. At the same time, designers are enabled to realize new functionality and achieve system performance goals that were not previously achievable.",2007
"A Dual Environment for 3D Modeling With User-Defined Free Form Features","Modeling with free form features has become the standard in Computer-Aided Design (CAD). With the increasing complexity of free form CAD models, features offer a high-level approach to modeling shapes. However, in most commercial modeling packages, only a static set of free form features is available. Researchers have tried to solve this problem by coming up with methods for user-driven free form feature definition, but failed to connect their methods to a means to instantiate these user-driven free form features on a target surface. Reversely, researchers have proposed tools for modeling with free form features, but these methods are time-intensive in that they are as of yet unsuitable for pre-defined features. This paper presents a new method for user-driven feature definition, as well as a method to instantiate these user-defined features on a target surface. We propose the concept of a dual environment, in which the definition of a feature is maintained simultaneously with its instance on a target surface, allowing the user to modify the definition of an already instantiated feature. This dual environment enables dynamic feature modeling, in which the user is able to change the definition of instantiated features on-the-fly. Furthermore, the proposed instantiation method is independent from the type of shape representation of the target surface and thereby increases the applicability of the method. The paper includes an extensive application example and discusses the results and shortcomings of the proposed methods.",2007
"Reverse Engineering of Geometric Surfaces Using Tabu Search Optimization Technique","Creating unavailable geometric models from existing parts plays an important role in the process of reverse engineering, for which the accuracy and fitting time of the created models are important factors. This paper proposes the use of Tabu Search (TS) technique in the optimal fitting of NURBS (Non Uniform Rational B-Spline) surfaces to laser-scanned point clouds of free-form surfaces for existing parts. The fitting process involves the initial estimation of the NURBS surface control points using least-squares approximation, followed by optimization of NURBS weights to minimize fitting error. Optimization is performed using a hybrid coding scheme, namely; Modified Continuous Reactive Tabu Search (M-C-RTS), in which a combinatorial optimization component, based on Reactive Tabu Search (RTS), co-operates with Sequential Quadratic Programming (SQP), as a local minimizer. The developed fitting algorithm was applied to a number of simulated free-form surfaces in addition to a laser-scanned PC mouse. Implementation was carried out using MATLAB software and the results were compared to those obtained using Genetic Algorithms (GAs) in an earlier publication. The results show the superiority of the proposed algorithm to the GA-based method with respect to the number of objective function evaluations (about 50% reduction). In addition to this time saving achievement, and surprisingly, M-C-RTS proved to be capable of finding better solutions than GAs.",2007
"Correspondences Matching on 3D Mesh Models","This paper presents an approach to match correspondences on 3D meshes, which is an important step for the design automation of customized freeform objects. For a given template model with a set of anchor points defined (knots of semantic features), we identify the corresponding points on the target model by minimizing the sum of differences by a series of transformation regardless of their differences in postures, scales and/or positions. The basic idea of our algorithm is to transform the target model to the template model iteratively. Once the correspondences between the surface points on the target model and the template are determined, we have essentially found the semantic features on the target model. We achieve this goal by four major transformations: 1) ",2007
"Procedural Design of Imprint Rolls for Fluid Pathway Fabrication","This paper discusses a novel method for designing imprint rolls for the fabrication of fluid pathways. Roller imprint processes have applications in diverse areas including fuel cell manufacturing and microfluidic device fabrication. Robust design methods are required for developing imprint rolls with optimal features. In the method discussed in this paper, the rolls are designed procedurally with the fluid pathway design given as input. The pathways are decomposed into repeating features (or tiles), and the rolls are designed by first modeling a small set of unique tiles and then combining them to model the entire roll. The tiling strategy decreases the complexity of the model, and reduces the time taken for designing the rolls. The modular nature of the tiles also improves the efficiency of post-processing operations like feature identification and optimization, and the generation of toolpaths for machining the roll.",2007
"OAM+: An Assembly Data Model for Legacy Systems Engineering","This paper presents a new assembly model named open assembly model plus (OAM+) to support legacy systems engineering (LSE). LSE is a collection of technologies for prolonging the life of old mechanical systems. Rapid Re-Engineering System (RRES), a subsystem of LSE is geared towards the fast production of redesigned parts customized to the manufacturing capability available. RRES requires the extraction of initial part geometry, parameters, interfacing constraints, kinematic constraints, and technical function. These specifications need to be imprinted on the CAD model before iterative redesign. A CAD data model is needed that can carry all the functional constraints. A detailed comparison of all the available assembly model shows that none of them can provide all these requirements in one place. Assembly feature based, object oriented assembly model OAM+ is developed to meet these requirements in one model. OAM+ can be used to perform kinematic analysis, force analysis and can exchange feature data using N-Rep feature definition language between different modules of RRES. OAM+ is based on part and assembly features in N-Rep.",2007
"Robust Product Design Optimization Method Using Hierarchical Representations of Characteristics","Product design optimizations usually require the optimization of not only all performance characteristics, but also the robustness of certain performance characteristics. Obtaining optimum design solutions is far from easy, since this requires evaluations of numerous related characteristics that usually have complicated and conflicting interrelationships. Some of these characteristics can include variations of one type or another, such as manufacturing process variations, variations pertaining to the environments where the product is used, variations in how long-term use affects certain product characteristics, and so on. The difficulty of obtaining optimum design solutions is thus compounded by the need to carry out specific optimizations that provide sufficient robustness to safely accommodate anticipated ranges of variations. This paper expands the hierarchical multiobjective optimization method based on simplification and decomposition of characteristics so that optimizations can be concurrently conducted for both performance characteristics and maximization of robustness against characteristic variances. A principal cause of variations in performance characteristics is variations in the contact conditions of joints, and the utility of the proposed robust product design optimization method is demonstrated by applying it to machine-tool models that include joints.",2007
"An Optimal Design Method for Reducing Brake Squeal in Disc Brake Systems","Minimizing brake squeal is one of the most important issues in the development of high performance braking systems. Recent advances in numerical analysis, such as finite element analysis, have enabled sophisticated analysis of brake squeal phenomena, but current design methods based on such numerical analyses still fall short in terms of providing concrete performance measures for minimizing brake squeal in high performance design drafts at the conceptual design phase. This paper proposes an optimal design method for disc brake systems that specifically aims to reduce brake squeal by appropriately modifying the shapes of the brake system components. First, the relationships between the occurrence of brake squeal and the geometry and characteristics of various components is clarified, using a simplified analysis model. Next, a new design performance measure is proposed for evaluating brake squeal performance and an optimization problem is then formulated using this performance measure as an objective function. The optimization problem is solved using Genetic Algorithms. Finally, a design example is presented to examine the features of the optimal solutions and confirm that the proposed method can yield useful design information for the development of high performance braking systems that minimize brake squeal.",2007
"Cycle-Based Robot Drive Train Optimization Utilizing SVD Analysis","Designing a drive train for an industrial robot is a demanding task where a set of design variables need to be determined so that optimal performance is obtained for a wide range of different duty cycles. The paper presents a method where singular value decomposition (SVD) is used to reduce the design variable set. The application is a six degree of freedom serial manipulator, with nine drive train parameters for each axis and the objective is to minimize the cycle time on 122 representative design cycles without decreasing the expected lifetime of the robot. The optimization is based on a simulation model of the robot and conducted on a reduced set of the initial duty cycles and with the design variables suggested by the SVD analysis. The obtained design reduces the cycle time with 1.6% on the original design cycles without decreasing the life time of the robot.",2007
"A Component Based Optimization Approach for Modular Robot Design","In this paper, an approach for modular design of industrial robots is presented. The approach is to introduce an objectoriented simulation model of the robot and combine this with a discrete optimization algorithm. The simulation model of the industrial robot is developed in Modelica, an object oriented modeling and simulation language, and simulated in the Dymola tool. The optimization algorithm used is a modification of the Complex method that has been developed in Matlab and connected to the simulation program. The optimization problem includes selecting components such as gearboxes and motors from a component catalogue and the objective function considers minimization of cost with constraints on gear box lifetime. Furthermore, the correctness of the model has been verified by comparison with an in-house simulation code with high accuracy.",2007
"A Method for Benchmarking Product Family Design Alternatives","Today’s companies are pressured to develop platform-based product families to increase variety while keeping production costs low. Determining why a platform works, and alternatively why it does not, is an important step in the successful implementation of product families and product platforms in any industry. Internal and competitive benchmarking is essential to obtain knowledge of how successful product families are implemented, thus avoiding potential pitfalls of a poor product platform design strategy. While the two fields of product family design and benchmarking have been growing rapidly lately, we have found few tools that combine the two for product ",2007
"Recommending a Platform Leveraging Strategy Based on the Homogeneous or Heterogeneous Nature of a Product Line","Platform-based product development depends on many factors, including technology, cost, competition, and life cycle considerations, and many companies would benefit from knowing more about the nature of their product families and how they impact platform-based product development. We assert that the development of a product platform and its derivative family of products is also impacted by the homogenous/heterogeneous nature of the products being developed, which has received little attention in the engineering literature. The current study introduces an original metric for assessing the homogeneity/heterogeneity in a given family: the Homogeneity versus Heterogeneity Ratio (HHR), which works at two levels of abstraction, namely, family and function. This study focuses on the platform leveraging strategy and takes an interest in two other aspects of platform development: the specification of the family and the necessary differentiation. To support platform design, the HHR",2007
"Three Dimensional Design Structure Matrix With Cross-Module and Cross-Interface Analyses","Many companies that struggle with product variety and configuration management issues turn to a module-based design approach. Although this approach is well-known to be efficient for managing variety of a product family, current methods do not enable designers to handle both modularity and variety within a product family. The Design Structure Matrix (DSM) has been widely used to identify modules within a product, but its use to identify modules across a family of products has been limited. In this context we propose two tools based on an extension of the basic DSM to manage variety of an entire product family. The Variety Design Structure Matrix, DSMV , handles variety of the product family and 3D Design Structure Matrix, DSM3D , enables visual analysis of across the entire product family. These two tools, combined into a single approach, enable analysis of the product family at many levels — family product, module, and interfaces — to better specify modules and interfaces across all of the products in the family. A case study involving a family of three single-use cameras is used to demonstrate the application of these new DSMs and accompanying cross-module and cross-interface analyses. This new approach can be applied during detailed studies as well as in the early stages of the design process.",2007
"A Product Platform Development Method Using QFD","Quality Function Deployment (QFD) was initially developed to aid in designing a quality product by interconnecting customer needs in a market segment with technical requirements. Although it assists in improving product quality, it does not have a function to examine technical requirements across the major market segments serviced by a company’s product lines and to aid in developing product platform concepts. In this paper, we present a product platform development method using QFD that aids in developing platform concepts as well as improving the understanding of product family design. This method includes platform planning and platform concept exploration. Platform planning describes the extent to which a variety of products share common components, and platform concepts then are explored, which are the arrangement of common components. This paper uses an electric razor example to illustrate the proposed method.",2007
"A Customer-Driven Approach to One-of-a-Kind Product Design","This research addresses the issues to identify the optimal product design based on individual customer requirements in one-of-a-kind production (OKP). In this work, a function decomposition approach is introduced for modeling the variations of design functions, configurations, and parameters in generic OKP product families. Requirements of individual customers are modeled at two different levels: function level and technical level. Customized OKP products are created from the generic OKP product families based on customer requirements. The optimal product design is identified from feasible design candidates through optimization. An industrial case study is given to demonstrate the effectiveness of the introduced approach.",2007
"Product Family Concept Generation and Validation Through Predictive Decision Tree Data Mining and Multi-Level Optimization","The formulation of a product family requires extensive knowledge about the product market space and also the technical limitations of a company’s engineering design and manufacturing processes. We present a methodology to significantly reduce the computational time required to achieve an optimal product portfolio by eliminating the need for an exhaustive search of all possible product concepts. This is achieved through a data mining decision tree technique that generates a set of product concepts that are subsequently validated in the engineering design level using multi-level optimization techniques. The final optimal product portfolio evaluates products based on the following three criteria: 1) The ability to satisfy customer’s price and performance expectations (based on predictive model) defined here as the feasibility criterion. 2) The feasible set of products/variants validated at the engineering level must generate positive profit that we define as the optimality criterion. 3) The optimal set of products/variants should be a manageable size as defined by the enterprise decisions makers and should therefore not exceed the product portfolio limit. The strength of our work is to reveal the tremendous savings in time and resources that exist when data mining predictive techniques are applied to the formulation of an optimal product portfolio. Using data mining tree generation techniques, a customer response data set of 40,000 individual product preferences is narrowed down to 46 product family concepts and then validated through the multilevel engineering design response of feasible architectures. A cell phone example is presented and an optimal product portfolio solution is achieved that maximizes company profit, while concurrently satisfying customer product performance expectations.",2007
"Robust Product Family Consolidation and Selection Using the Hypothetical Equivalents and Inequivalents Method","The design and development of effective product lines is a challenge in modern industry. Companies must balance diverse product families that satisfy wide ranging customer demands with practical business needs such as combining manufacturing processes and using similar materials, for example. In this paper, the issue of consolidating an existing product family is addressed. Specifically, the Hypothetical Equivalents and Inequivalents Method (HEIM) is utilized in order to select an optimal product family configuration. In previous uses, HEIM has been shown to assist a decision maker in selecting one concept from a set when concept attributes conflict with each other. In this extension of HEIM, the optimization problem’s constraints are formulated using two different value functions, and common solutions are identified in order to select an optimal family of staplers. The result is then compared with the result found using a multi-attribute utility theory (MAUT) based approach. While each method has its advantages and disadvantages, and MAUT provides a necessary first step for product family consolidation and selection, a robust solution is achieved through HEIM.",2007
"An Extension of the Commonality Index for Product Family Optimization","One critical aim of product family design is to offer distinct variants that attract a variety of market segments while maximizing the number of common parts to reduce manufacturing cost. Several indices have been developed for measuring the degree of commonality in existing product lines to compare product families or assess improvement of a redesign. In the product family optimization literature, commonality metrics are used to define the multi-objective tradeoff between commonality and individual variant performance. These ",2007
"A Single-Stage Gradient-Based Approach for Solving the Joint Product Family Platform Selection and Design Problem Using Decomposition","A core challenge in product family optimization is to develop a single-stage approach that can optimally select the set of variables to be shared in the platform(s) while simultaneously designing the platform(s) and variants within an algorithm that is efficient and scalable. However, solving the joint product family platform selection and design problem involves significant complexity and computational cost, so most prior methods have narrowed the scope by treating the platform as fixed or have relied on stochastic algorithms or heuristic two-stage approaches that may sacrifice optimality. In this paper, we propose a single-stage approach for optimizing the joint problem using gradient-based methods. The combinatorial platform-selection variables are relaxed to the continuous space by applying the commonality index and consistency relaxation function introduced in a companion paper. In order to improve scalability properties, we exploit the structure of the product family problem and decompose the joint product family optimization problem into a two-level optimization problem using analytical target cascading so that the system-level problem determines the optimal platform configuration while each subsystem optimizes a single product in the family. Finally, we demonstrate the approach through optimization of a family of ten bathroom scales; Results indicate encouraging success with scalability and computational expense.",2007
"An Investigation of the Impact of Assembly Sequencing on the Product Family Design Outcomes","While many approaches have been proposed to optimize the product family design for measures of cost, revenue and performance, many of these approaches fail to incorporate the complexity of the manufacturing issues into family design decision-making. One of these issues is assembly sequencing. This paper presents a simulation study by which the impact of assembly sequencing on the product family design outcomes is investigated. Overall, the results indicate that when the product family design takes into account the assembly sequencing decisions, the outcomes at the shop floor level improve. The results have implications for companies that are looking into increasing their revenue without increasing their investment in the shop floor.",2007
"Switchgear Component Commonality Design Method Considering Delivery Lead-Time and Inventory Level","Product families are groups of related products that take advantage of part commonalities at various levels to streamline delivery of maximal product variety with minimal cost impact and as short as possible lead-times. This paper proposes a new integrated product design method for build-to-order production system based products, using the product family concept, which considers product performance, delivery lead-time and inventory cost. The development and discussion of this method uses a switchgear design problem as a concrete and practical design case. A build-to-order production system has been applied to switchgear manufacturing due to its small-scale production and a variety of customer requirements. However, if the risk of maintaining unsold inventory can be decreased, manufacturers can justify holding an amount of versatile inventory. In this paper, inventory production system is applied to the switchgear production problem to shorten the delivery lead-time. The switchgear design and production problem is formulated using three objective functions, which are subassembly procurement lead-time, inventory cost and area occupied by various switchgear configurations. Moreover, to assist inventory cost evaluations, a simulation procedure for the inventory system is proposed. The proposed method is used to obtain a Pareto optimal solution set of the three objective functions. Finally, an example switchgear design problem is solved to illustrate that optimal use of component commonalities across different modules can significantly reduce inventory costs, while also shortening product delivery lead-times.",2007
"Process Platform Formation for Product Families","In accordance with the product families, process platforms have been recognized as a promising tool for companies to configure optimal, yet similar, production processes for producing different products. This paper tackles process platform formation from large volumes of production data available in companies’ production systems. A data mining methodology based on text mining and tree matching is developed for the formation of process platforms. A case study of high variety production of vibration motors for mobile phones is reported to prove the feasibility and potential of forming process platforms using text mining and tree matching.",2007
"Application of Product Platform Design for Engineered System","The focus of this paper is on the design of the engineered system comprising various subsystems that have functional interactions amongst themselves. Depending on the system architecture, subsystems can be classified into one of the two types: scalar and modular subsystems. Each subsystem is defined by various primary and secondary parameters, performance criteria, and compatibility constraints. The objective function is formulated as minimization of total cost of all the subsystems while meeting the required performance criteria. The cost of an individual subsystem is a function of parameters of that particular subsystem. The complete engineered system level problem has been formulated as a non linear programming optimization problem. The application of the proposed methodology is demonstrated using an example of the automotive truck family.",2007
"A Framework for Designing Balanced Product Platforms by Estimating Versatility of Components","Mass customization is a common trend in industries and platform-based product family strategy is widely used for an efficient mass customization. While commonization of a platform is a viable mean for reducing the customization cost, it also has a risk of losing some market share due to its limitation on differentiating individual products. This trade-off requires a platform to be balanced between commonality and distinctiveness of products. In this paper, we focus on developing a versatile platform that maximizes the use of common components while facilitating differentiations which are highly effective for increasing the market share of a product family. A versatile platform is comprised of versatile components which do not restrict effective differentiations even if it is commonized. To determine a certain component is versatile or not, we considered which specifications are preferred to be differentiated in the market and how much change would be required for the component to differentiate a specification. With these two measures, we define a versatility index representing how versatile a component is. Components with higher versatility values are appropriate to be platformized since they are less likely to be changed for differentiations. Furthermore, identification of non-versatile components may provide a clue for improving architecture of the product. The proposed method is applied to the PC mouse design, which yields reasonable alternatives for platform design.",2007
"Function Block Design to Enable Adaptive Job Shop Operations","Small volume and high product-mix contribute greatly to the complexity of job shop operations. In addition, shop floor uncertainty or fluctuation is another issue regularly challenging manufacturing companies, including job delay, urgent job insertion, fixture shortage, missing tool, and even machine breakdown. Targeting the uncertainty, we propose a function block based approach to generating adaptive process plans. Enabled by the function blocks, a so-generated process plan is responsive and tolerant to an unpredictable change. This paper presents in detail how a function block is designed and what it can do during process plan execution. It is expected that this new approach can largely enhance the dynamism of fluctuating job shop operations.",2007
"Improving Confidence in Simulation-Based Design Through Error Functions","Understanding of uncertainty in the data and models used in design simulations matures during the design process as the design progresses from vague requirements through to its full embodiment and detail. Failure to take account of uncertainty in the information that is used in and generated from simulation processes poses risks to decisions based upon these. This paper presents a classification scheme based on the extent and nature of uncertainty in the correlations between simulation predictions and the evidence for a specific performance criterion. The classification allows development of a confidence scale and associated error functions for characterizing the discrepancy between the correlations of design performance parameters and evidence, in the presence of uncertainty. Together, the confidence scale and error functions may provide a greater understanding of uncertainty and errors in simulation processes. In the context of parametric design, the approach provides a mechanism for building up greater understanding of the simulation performance across a feasible design space. A case study on the design of shrink-fits is used to illustrate the framework for handling uncertainty in a systematic and organized manner. The theoretical and practical limitations and further work will be discussed.",2007
"Interval Reliability Analysis","Traditional reliability analysis uses probability distributions to calculate reliability. In many engineering applications, some nondeterministic variables are known within intervals. When both random variables and interval variables are present, a single probability measure, namely, the probability of failure or reliability, is not available in general; but its lower and upper bounds exist. The mixture of distributions and intervals makes reliability analysis more difficult. Our goal is to investigate computational tools to quantify the effects of random and interval inputs on reliability associated with performance characteristics. The proposed reliability analysis framework consists of two components — direct reliability analysis and inverse reliability analysis. The algorithms are based on the First Order Reliability Method and many existing reliability analysis methods. The efficient and robust improved HL-RF method is further developed to accommodate interval variables. To deal with interval variables for black-box functions, nonlinear optimization is used to identify the extreme values of a performance characteristic. The direct reliability analysis provides bounds of a probability of failure; the inverse reliability analysis computes the bounds of the percentile value of a performance characteristic given reliability. One engineering example is provided.",2007
"Evidence Theory-Based Reliability Analysis and Optimization in Engineering Design","Engineering design under uncertainty has gained considerable attention in recent years. There exist two different types of uncertainties in practical engineering applications: aleatory uncertainty that is classified as objective and irreducible uncertainty with sufficient information on input uncertainty data and epistemic uncertainty that is a subjective and reducible uncertainty that is caused by the lack of knowledge on input uncertainty data. Among several alternative tools to handle uncertainty, evidence theory has proved to be computationally efficient and stable tool for reliability analysis and design optimization under aleatory and/or epistemic uncertainty involved in engineering systems. This paper attempts to give a better understanding of uncertainty in engineering design with a general overview. The overview includes theoretical research, computational development, and performable ability consideration of evidence theory during recent years. At last, perspectives on future research are stated.",2007
"Reliability Estimation for Time-Dependent Problems Using a Niching Genetic Algorithm","A reliability analysis method is presented for time-dependent systems under uncertainty. The system response is modeled as a parameterized random process. A double-loop optimization algorithm is used. The inner loop calculates the maximum response in time using a global-local search method, and transforms a time-dependent problem into a time-independent one. The outer loop calculates multiple most probable points (MPPs) which are commonly encountered in vibration problems. The dominant MPPs with the highest contribution to the probability of failure are identified. A niching genetic algorithm is used because of its ability to simultaneously identify multiple solutions. All potential MPPs are initially identified approximately and their location is efficiently refined using a gradient-based optimizer with local metamodels. Among all MPPs, the significant ones are identified using a correlation analysis. Approximate limit states are built at the identified MPPs, and the system failure probability is estimated using bi-modal bounds. The vibration response of a cantilever plate under random oscillating pressure load and a point load illustrates the proposed method. The finite-element model is used to calculate the response.",2007
"A Monte Carlo Reliability Assessment for Multiple Failure Region Problems Using Approximate Metamodels","An efficient Monte Carlo reliability assessment methodology is presented for engineering systems with multiple failure regions and potentially multiple most probable points. The method can handle implicit, nonlinear limit-state functions, with correlated or non-correlated random variables, which can be described by any probabilistic distribution. It uses a combination of approximate or “accurate-on-demand,” global and local metamodels which serve as indicators to determine the failure and safe regions. Samples close to limit states define transition regions between safe and failure domains. A clustering technique identifies all transition regions which can be in general disjoint, and local metamodels of the actual limit states are generated for each transition region. A Monte Carlo simulation calculates the probability of failure using the global and local metamodels. A robust maximin “space-filling” sampling technique is used to construct the metamodels. Also, a principal component analysis addresses the problem dimensionality making therefore, the proposed method attractive for problems with a large number of random variables. Two numerical examples highlight the accuracy and efficiency of the method.",2007
"Supervised Bayesian Models to Carry Out Kansei Engineering of Car Dashbaords","The issue of this paper is the use of perceptual evaluations of products as a simulation platform for improving decision making in the design process of a new product. In previous work, we proposed bayesian kansei models using ",2007
"A New Inverse Reliability Analysis Method Using MPP-Based Dimension Reduction Method (DRM)","There are two commonly used reliability analysis methods of analytical methods: linear approximation - First Order Reliability Method (FORM), and quadratic approximation - Second Order Reliability Method (SORM), of the performance functions. The reliability analysis using FORM could be acceptable for mildly nonlinear performance functions, whereas the reliability analysis using SORM is usually necessary for highly nonlinear performance functions of multi-variables. Even though the reliability analysis using SORM may be accurate, it is not desirable to use SORM for probability of failure calculation since SORM requires the second-order sensitivities. Moreover, the SORM-based inverse reliability analysis is very difficult to develop. This paper proposes a method that can be used for multi-dimensional highly nonlinear systems to yield very accurate probability of failure calculation without requiring the second order sensitivities. For this purpose, the univariate dimension reduction method (DRM) is used. A three-step computational process is proposed to carry out the inverse reliability analysis: constraint shift, reliability index (β) update, and the most probable point (MPP) approximation method. Using the three steps, a new DRM-based MPP is obtained, which computes the probability of failure of the performance function more accurately than FORM and more efficiently than SORM.",2007
"Reliability Based Design Optimization With Correlated Input Variables Using Copulas","For the performance measure approach (PMA) of RBDO, a transformation between the input random variables and the standard normal random variables is necessary to carry out the inverse reliability analysis. For reliability analysis, Rosenblatt and Nataf transformations are commonly used. In many industrial RBDO problems, the input random variables are correlated. However, often only limited information such as the marginal distribution and covariance could be practically obtained, and the input joint probability distribution function (PDF) is very difficult to obtain. Thus, in literature, most RBDO methods assume all input random variables are independent. However, in this paper, it is found that the RBDO results can be significantly different when the input variables are correlated. Thus, various transformation methods are investigated for development of a RBDO method for problems with correlated input variables. It is found that Rosenblatt transformation is impractical for problems with correlated input variables due to difficulty of constructing a joint PDF from the marginal distributions and covariance. On the other hand, Nataf transformation can construct the joint CDF using the marginal distributions and covariance, and thus applicable to problems with correlated random input variables. The joint CDF is Nataf model, which is called a Gaussian copula in the copula family. Since the Gaussian copula can describe a wide range of the correlation coefficient, Nataf transformation can be widely used for various types of correlated input variables. In this paper, Nataf transformation is used to develop a RBDO method for design problems with correlated random input variables. Numerical examples are used to demonstrate the proposed method. Also, it is shown that the correlated random input variables significantly affect the RBDO results.",2007
"Robust Design Concept in Possibility Theory and Optimization for System With Both Random and Fuzzy Input Variables","Whereas the robust design concept has been well established in the probability theory, it has not been developed in the possibility theory. For problems where accurate statistical information for input data is not available, a possibility-based (or fuzzy set) robust design concept is proposed in this paper by investigating the similarity between the membership function of the fuzzy variable and the cumulative distribution function of the random variable. Based on the probability-possibility consistency principle, a random variable that corresponds to the fuzzy variable is introduced in this paper in order to define the robust design concept for the fuzzy variable. For the system with input fuzzy variables, the robustness measure of the output performance is computed using the performance measure integration (PMI) method, while the integration points are obtained from the inverse possibility analysis by using the maximal possibility search method with interpolation (MPS). For the system with mixed random and fuzzy input variables, the robustness measure of the output performance is computed using PMI method, with the integration points obtained from the inverse mixed analysis by using the maximal failure search method (MFS). A new mixed (random and fuzzy) variable robust design optimization (MVRDO) method is proposed and several numerical examples are used to verify the robust design concept in the possibility theory and the MVRDO formulation.",2007
"Updating Uncertainty Assessments: A Comparison of Statistical Approaches","The performance of a product that is being designed is affected by variations in material, manufacturing process, use, and environmental variables. As a consequence of uncertainties in these factors, some items may fail. Failure is taken very generally, but we assume that it is a random event that occurs at most once in the lifetime of an item. The designer wants the probability of failure to be less than a given threshold. In this paper, we consider three approaches for modeling the uncertainty in whether or not the failure probability meets this threshold: a classical approach, a precise Bayesian approach, and a robust Bayesian (or imprecise probability) approach. In some scenarios, the designer may have some initial beliefs about the failure probability. The designer also has the opportunity to obtain more information about product performance (e.g. from either experiments with actual items or runs of a simulation program that provides an acceptable surrogate for actual performance). The different approaches for forming and updating the designer’s beliefs about the failure probability are illustrated and compared under different assumptions of available information. The goal is to gain insight into the relative strengths and weaknesses of the approaches. Examples are presented for illustrating the conclusions.",2007
"Decision Making and Constraint Tradeoff Visualization for Design Under Uncertainty","In design optimization problems under uncertainty, two conflicting issues are generally of interest to the designer: feasibility and optimality. In this research, we adopt the philosophy that design, especially under uncertainty, is a decision making process, where the associated tradeoffs can be conveniently understood using multiobjective optimization. The importance of constraint feasibility and the associated tradeoffs, especially in the presence of equality constraints, is examined in this paper. We propose a three-step decision making framework that facilitates effective decision making under uncertainty: (1) formulating a multiobjective problem that effectively models the tradeoffs under uncertainty, (2) generating design alternatives by solving the proposed multiobjective robust design formulation, and (3) choosing a final design using filtering and constraint uncertainty visualization schemes. The proposed framework can be used to systematically explore the design space from a constraint tradeoff perspective. A tolerance synthesis example is used to illustrate the proposed decision making process.",2007
"Conservative Estimations of Reliability With Limited Sampling","The objective of this paper is to provide a method of safely estimating reliability based on small samples. First, it is shown that the commonly used estimators of the parameters of the normal distribution function are biased, and they tend to lead to unconservative estimates of reliability. Then, two ways of making this estimation conservative are proposed: (1) adding constraints when a distribution is fitted to the data to bias it to be conservative, and (2) using the bootstrap method to estimate the bias needed for a given level of conservativeness. The relationship between the accuracy and the conservativeness of the estimates is explored for a normal distribution. In particular, detailed results are presented for the case when the goal is 95% likelihood to be conservative. The bootstrap approach is found to be more accurate for this level of conservativeness. It is then applied to the reliability analysis of a composite panel under thermal loading. Finally, we explore the influence of sample sizes and target probability of failure on estimates quality, and show that for a constant level of conservativeness, small samples and low probabilities can lead to a high risk of large overestimation while this risk is limited to a very reasonable value for samples above.",2007
"Reliable Space Pursuing for RBDO With Black-Box Performance Functions","Reliability-based design optimization (RBDO) is intrinsically a double-loop procedure since it involves an overall optimization and an iterative reliability assessment at each search point. Due to the double-loop procedure, the computational expense of RBDO is normally very high. Current RBDO research is focused on performance functions having explicit analytical expression and readily available gradients. This paper addresses a more challenging type of RBDO problem in which the performance functions are computation intensive. These computation intensive functions are often considered as a “black-box” and their gradients are not available or not reliable. Based on the reliable design space (RDS) concept proposed earlier by the authors, this paper proposes a Reliable Space Pursuing (RSP) approach, in which RDS is first identified and then gradually refined while optimization is performed. It theoretically avoids the nested optimization and probabilistic assessment loop. This approach can apply to RBDO problems with either analytical or blackbox performance functions. Three well known numerical problems from the literature are used to test and demonstrate the effectiveness of RSP.",2007
"Bayesian Reliability Based Design Optimization Using Eigenvector Dimension Reduction (EDR) Method","In the last decade, considerable advances have been made in Reliability-Based Design Optimization (RBDO). It is assumed in RBDO that statistical information of input uncertainties is completely known (aleatory uncertainty), such as a distribution type and its parameters (e.g., mean, deviation). However, this assumption is not valid in practical engineering applications, since the amount of uncertainty data is restricted mainly due to limited resources (e.g., man-power, expense, time). In practical engineering design, most data sets for system uncertainties are insufficiently sampled from unknown statistical distributions, known as epistemic uncertainty. Existing methods in uncertainty based design optimization have difficulty in handling both aleatory and epistemic uncertainties. To tackle design problems engaging both epistemic and aleatory uncertainties, this paper proposes an integration of RBDO with Bayes Theorem, referred to as Bayesian Reliability-Based Design Optimization (Bayesian RBDO). However, when a design problem involves a large number of epistemic variables, Bayesian RBDO becomes extremely expensive. Thus, this paper presents a more efficient and accurate numerical method for reliability method demanded in the process of Bayesian RBDO. It is found that the Eigenvector Dimension Reduction (EDR) Method is a very efficient and accurate method for reliability analysis, since the method takes a sensitivity-free approach with only 2",2007
"A Hierarchical Statistical Sensitivity Analysis Method for Complex Engineering Systems","The method of Statistical Sensitivity Analysis (SSA) is playing an increasingly important role in engineering design, especially with the consideration of uncertainty. However, applying SSA to the design of complex engineering systems is not straight forward due to both computational and organizational difficulties. In this paper, a Hierarchical Statistical Sensitivity Analysis (HSSA) method is developed to facilitate the application of SSA to the design of complex systems especially those follow hierarchical modeling structures. A top-down strategy for HSSA is introduced to only invoke the SSA of critical submodels based on the significance of submodel performances. A simplified formulation of the Global Statistical Sensitivity Index (GSSI) is studied to represent the effect of a lower-level submodel input on a higher-level model response by aggregating the submodel SSA results across intermediate levels. A sufficient condition under which the simplified formulation provides an accurate solution is derived. To improve the accuracy of the GSSI formulation for a general situation, a modified formulation is proposed by including an Adjustment Coefficient (",2007
"A Comparative Study of Uncertainty Propagation Methods for Black-Box Type Functions","It is an important step in deign under uncertainty to select an appropriate uncertainty propagation (UP) method considering the characteristics of the engineering systems at hand, the required level of UP associated with the probabilistic design scenario, and the required accuracy and efficiency levels. Many uncertainty propagation methods have been developed in various fields, however, there is a lack of good understanding of their relative merits. In this paper, a comparative study on the performances of several UP methods, including a few recent methods that have received growing attention, is performed. The full factorial numerical integration (FFNI), the univariate dimension reduction method (UDR), and the polynomial chaos expansion (PCE) are implemented and applied to several test problems with different settings of the performance nonlinearity, distribution types of input random variables, and the magnitude of input uncertainty. The performances of those methods are compared in moment estimation, tail probability calculation, and the probability density function (PDF) construction. It is found that the FFNI with the moment matching quadrature rule shows good accuracy but the computational cost becomes prohibitive as the number of input random variables increases. The accuracy and efficiency of the UDR method for moment estimations appear to be superior when there is no significant interaction effect in the performance function. Both FFNI and UDR are very robust against the non-normality of input variables. The PCE is implemented in combination with FFNI for coefficients estimation. The PCE method is shown to be a useful approach when a complete PDF description is desired. Inverse Rosenblatt transformation is used to treat non-normal inputs of PCE, however, it is shown that the transformation may result in the degradation of accuracy of PCE. It is also shown that in black-box type of system the performance and convergence of PCE highly depend on the method adopted to estimate its coefficients.",2007
"Complementary Interaction Method (CIM) for System Reliability Analysis","Researchers desire to evaluate system reliability uniquely and efficiently. Despite years of research, little progress has been made on system reliability analysis. Up to now, bound methods for system reliability prediction have been dominant. For system reliability bounds, the probabilities of the second or higher order joint events are assumed to be known exactly although there is no numerical method to evaluate them effectively. Two primary challenges in system reliability analysis are how to evaluate the probabilities of the second or higher order joint events and how to uniquely obtain the system reliability so that the system reliability can be used for Reliability-Based Design Optimization (RBDO). This paper proposes the Complementary Interaction Method (CIM) to define system reliability in terms of the probabilities of the component events, ",2007
"Innovative Six Sigma Design Using the Eigenvector Dimension-Reduction (EDR) Method","This paper presents an innovative approach for quality engineering using the Eigenvector Dimension Reduction (EDR) Method. Currently industry relies heavily upon the use of the Taguchi method and Signal to Noise (S/N) ratios as quality indices. However, some disadvantages of the Taguchi method exist such as, its reliance upon samples occurring at specified levels, results to be valid at only the current design point, and its expensiveness to maintain a certain level of confidence. Recently, it has been shown that the EDR method can accurately provide an analysis of variance, similar to that of the Taguchi method, but is not hindered by the aforementioned drawbacks of the Taguchi method. This is evident because the EDR method is based upon fundamental statistics, where the statistical information for each design parameter is used to estimate the uncertainty propagation through engineering systems. Therefore, the EDR method provides much more extensive capabilities than the Taguchi method, such as the ability to estimate not only mean and standard deviation of the response, but also the skewness and kurtosis. The uniqueness of the EDR method is its ability to generate the probability density function (PDF) of system performances. This capability, known as the probabilistic “what-if” study, provides a visual representation of the effects of the design parameters (e.g., its mean and variance) upon the response. In addition, the probabilistic “what-if” study can be applied across multiple design parameters, allowing the analysis of interactions among control factors. Furthermore, the implementation of the probabilistic “what-if” study provides a basis for performing robust design optimization. Because of these advantages, it is apparent that the EDR method provides an alternative platform of quality engineering to the Taguchi method. For easy execution by field engineers, the proposed platform for quality engineering using the EDR method, known as Quick Quality Quantification (Q3 ), will be developed as a Microsoft EXCEL add-in.",2007
"Sensitivity-Free Approach for Reliability-Based Robust Design Optimization","This paper attempts to integrate a derivative-free probability analysis method to Reliability-Based Robust Design Optimization (RBRDO). The Eigenvector Dimension Reduction (EDR) method is used for the probability analysis method. It has been demonstrated that the EDR method is more accurate and efficient than the Second-Order Reliability Method (SORM) for reliability and quality assessment. Moreover, it can simultaneously evaluate both reliability and quality without any extra expense. Three practical engineering problems (vehicle side impact, layered bonding plates, and lower control arm) are used to demonstrate the effectiveness of the EDR method.",2007
"Differential Evolution Applied to the Design of a Three-Dimensional Vehicular Structure","The purpose of this paper is to demonstrate the application of Differential Evolution to a realistic design optimization test problem. The present contribution regards the improvements implemented to the original basic algorithm as well as the application of a new algorithm for dealing with the unique challenges associated with real world optimization problems. The selected example is a three-dimensional vehicular structure optimization problem modeled using the commercial Finite Element software ANSYS®  that has a combination of continuous and discrete design variables. The use of traditional gradient-based optimization algorithms is thus not practical. The numerical results presented indicate that the Differential Evolution algorithm is able to find the optimum design for the proposed problem. The algorithm is robust in the sense that it is capable of dealing with the numerical noise involved in the modeling of the system and to manipulate discrete design variables, accordingly.",2007
"On Optimization of 2D Compliant Mechanisms Using Honeycomb Discretization With Material-Mask Overlay Strategy","Novel honeycomb tessellation and material mask overlay methods are proposed in this paper to obtain optimal planar compliant topologies free from checkerboard and point flexure pathologies. A cardinal reason, namely the presence of strain-free rotation regions in rectangular cell based discretization is identified to be a cause in appearance of such singularities. With each hexagonal cell sharing an edge with its neighboring cells, strain-free displacements are not permitted anywhere in the continuum. The new material assignment approach manipulates material within a group of cells as opposed to a single cell thereby reducing the number of variables making optimization efficient. Optimal solutions obtained are free from intermediate material states and can be manufactured directly after design, without requiring any post processing. The proposed procedure is illustrated using two classical examples in 2D compliant mechanisms solved using genetic algorithm.",2007
"Design of Piezoelectric Actuator With In-Plane Motion Using Topology Optimization","In this paper, topology optimization is used to study the design of piezoelectric actuator with in-plane motion. Two case studies are reported, the maximization of the in-plane motion toward a pre-defined direction and the maximization of the output force. In addition to volume density as design variable used in the conventional topology optimization, a new design variable, electrode density, is introduced to model the electrode topology on the piezoelectric plate surface. Based on the electrode potential model, the relation between the nodal potential and the electrode density is established. Sensitivity analyses of objective function with respect to volume density and electrode density are derived from the adjoint method. Examples of optimized piezoelectric actuators from the proposed method are presented and discussed.",2007
"Indirect Encoding of Structures for Evolutionary Design","The use of Evolutionary Computations (EC’s) has become one of the primary methods in the field of automated design synthesis. The overwhelming majority of EC’s in use today use a direct encoding, where an individual is described by its gene string. This means that every engineering domain must create its own encoding scheme, making implementation in new fields difficult and slow. Additionally, direct encoding does not produce symmetry or modularity, unless these attributes are written into the encoding scheme ",2007
"Interactive Mesh-Free Stress Analysis for Mechanical Design Assembly With Haptics","This paper describes a virtual reality application that performs fast stress reanalysis coupled with virtual reality and haptics that allows rapid evaluation of multiple designs throughout the product design process. The Interactive Virtual Design Application (IVDA) allows the engineer to interactively explore new design geometry while simultaneously examining the finite element analysis results. In the presence of other parts in the assembly, the new shape can be analyzed and modified, taking into consideration mating part fits. This approach supports concurrent product design and assembly methods prototyping. A “two-step” approach utilizing Taylor series approximations and Pre-conditioned Conjugate Gradient methods is used to perform quick reanalysis during interactive shape modification. The virtual environment provides an immersive three-dimensional workspace. Haptics are used to provide feedback of the stress gradient as the part geometry is changed, thus facilitating the designer’s understanding of the impact of shape change on product performance.",2007
"Visual Steering Commands for Trade Space Exploration: User-Guided Sampling With Example","Recent advancements in computing power and speed provide opportunities to revolutionize trade space exploration, particularly for the design of complex systems such as automobiles, aircraft, and spacecraft. In this paper, we introduce three Visual Steering Commands to support trade space exploration and demonstrate their use within a powerful data visualization tool that allows designers to explore multidimensional trade spaces using glyph, 1-D and 2-D histogram, 2-D scatter, scatter matrix, and parallel coordinate plots; linked views; brushing; preference shading and Pareto frontier display. In particular, we define three user-guided samplers that enable designers to explore (1) the entire design space, (2) near a point of interest, or (3) within a region of high preference. We illustrate these three samplers with a vehicle configuration model that evaluates the technical feasibility of new vehicle concepts. Future research is also discussed.",2007
"Virtual Reality Systems: A Method to Evaluate the Applicability Based on the Design Context","Virtual Reality (VR) technologies provide novel modes of human computer interaction that can be used to support industrial design processes. The integration can be successful if supported by a method to qualify, select and design the VR technologies according to the company’s requirements in order to improve collaboration in extended enterprises and timesaving. The aim of the present work is the definition of a method to translate the company’s expectations into heuristic values that allow the benchmarking of VR systems. The method has been tested on a real test case.",2007
"Representing Historically Based Component-Function Relationships Through Design Templates","Functional analysis of systems is a common engineering application during different stages of design. Conceptual designers as well as post-development designers use the process to gather useful information about the system that is under consideration. The functional basis and component taxonomy are collective approaches to describe these systems in unique languages. Since many designers naturally think in terms of physical components, it is more difficult for them to grasp fundamental concepts necessary to functionally model a system properly. A new design instrument, component functional templates, has been developed as a means to link the functional basis and component taxonomy together in one coherent visual form that can be used by novice designers as an invaluable skill-building tool. Principal components analysis (PCA) is used to extract historical data from many consumer products whose design information has been stored in an online repository produced by the UMR Design Engineering Lab. This paper presents the approach and derivation of the templates, along with valid examples of template groupings that result from the analysis. An application of the templates is presented in a case study on the drive train of a bicycle where the templates prove to sufficiently begin the modeling process and provide room for unique manipulation that accurately describes functional requirements of the subsystem.",2007
"Immersive Product Configurator for Conceptual Design","Currently, new product concepts are evaluated by developing detailed virtual part and assembly models with traditional Computer Aided Design (CAD) tools followed by appropriate analyses (e.g., finite element analysis, computational fluid dynamics, etc.). The creation of these models and analyses are tremendously time consuming. If a number of different conceptual configurations have been determined, it may not be possible to model and analyze each of them. Thus, promising concepts might be eliminated based solely on insufficient time to assess them. In addition, the virtual models and analyses performed are usually of much higher detail and accuracy than what is needed for such early assessment. By eliminating the time-consuming complexity of a CAD environment and incorporating qualitative assessment tools, engineers could spend more time evaluating additional concepts, which were previously abandoned due to time constraints. In this paper, a software framework, the Advanced Systems Design Suite (ASDS), for creating and evaluating conceptual design configurations in an immersive virtual reality environment is presented. The ASDS allows design concepts to be quickly modeled, analyzed, and visualized. It incorporates a PC user interface with an immersive virtual reality environment to ease the creation and assessment of conceptual design prototypes. The development of the modeling and assessment tools are presented along with a test case to demonstrate the usability and effectiveness of the framework.",2007
