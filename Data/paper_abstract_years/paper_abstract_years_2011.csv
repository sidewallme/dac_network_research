title,Abstract,year
"An Explicit Level-Set Approach for Structural Topology Optimization","Level-set approaches are a family of domain classification techniques that rely on defining a scalar level-set function (LSF), then carrying out the classification based on the value of the function relative to one or more thresholds. Most continuum topology optimization formulations are at heart, a classification problem of the design domain into structural materials and void. As such, level-set approaches are gaining acceptance and popularity in structural topology optimization. In conventional level set approaches, finding an optimum LSF involves solution of a Hamilton-Jacobi system of partial differential equations with a large number of degrees of freedom, which in turn, cannot be accomplished without gradients information of the objective being optimized. A new approach is proposed in this paper where design variables are defined as the explicit values of the LSF at knot points, then a Kriging model is used to interpolate the LSF values within the rest of the domain so that classification into material or void can be performed. Perceived advantages of the explicit level-set (ELS) approach include alleviating the need for gradients of objectives and constraints, while maintaining a reasonable number of design variables that is independent from the mesh size. A hybrid genetic algorithm (GA) is then used for solving the optimization problem(s). An example problem of a short cantilever is studied under various settings of the ELS parameters in order to infer the best practice recommendations for tuning the approach. Capabilities of the approach are then further demonstrated by exploring its performance on several test problems.",2011
"Mixed Discrete and Continuous Variable Optimization Based on Constraint Aggregation and Relative Sensitivity","This work presents a new approach for solving nonlinear mixed discrete-continuous variable problems with constraints. The proposed method falls under the category of direct search methods for discrete variables. Different from the traditional direct search methods that determine the search direction based on decreasing objective function within the feasible space, a relative sensitivity that jointly considers change in objective and constraint functions is introduced in this work to help determining the search direction. For feasible discrete points, the coordinate direction with the maximum relative sensitivity is taken as the search direction, so that the objective function value decreases the fastest with minimum increase in constraint values. For infeasible points, the search direction is determined by the minimum relative sensitivity, so that the points can be dragged into the feasible region with constraints decreasing the fastest and minimum increase of the objective. In addition, in order to reduce the number of constraints and calculate the relative sensitivity, a constraint aggregation technique with Kreisselmeier-Steinhauser function is applied to transform all constraints into an equivalent differentiable inequality constraint. The efficacy and accuracy of the proposed approach is demonstrated with different types of test problems and application to a design problem. The proposed method has advantages in solving nonlinear mixed discrete-continuous variable problems with constraints compared to other existing methods.",2011
"Optimization of the Cutting Conditions for High Speed Drilling of Woven Composites","The present work proposes a new algorithm for the optimization of cutting parameters in the high speed drilling of woven composites. The cutting parameters under consideration are the feed rate and the spindle speed. Three performance parameters are to be minimized. These are the exit delamination, the surface roughness and the thrust force. These performance parameters are observed experimentally. One of the challenges that face the experimental testing of these parameters is the high cost of the drilling tools and specimen materials. Therefore, the minimization of the number of experimental tests is a necessary requirement. The algorithm presented hybridizes Kriging as a meta-modeling technique with evolutionary multi-objective optimization to optimize the cutting parameters while intelligently selecting the new set of cutting parameters in each iteration. After starting with a factorial design of the search space, and after testing the performance criteria at these points, the algorithm fits a multi-dimensional surface using Kriging. This step is followed by an evolutionary search on the fitted model. The search spreads a population of search points in the direction of better performance criteria as well as in the direction of un-sampled space. The previous two steps are conducted iteratively for a pre-defined number of iterations. In the final iteration, the population of search points is clustered to yield a small number of new points at which the new experiments will be conducted. The whole process is iterated until the maximum number of allowable experiments is achieved. The algorithm is tested using an existing set of previously published experimental data that are dense enough to predict the actual response surface of the performance criteria. Results showed that the algorithm smartly moved into the direction of higher performance criteria with a low number of experimental trials.",2011
"Optimization of the Shock Mitigation Layer in the Space Frame Joints of an Armored Vehicle","Armored vehicles have to survive multiple threats such as projectile or land mines. The shocks induced by these threats can harm vehicle occupants or damage sensitive electronic components. Therefore, a goal of modern armored vehicle design is to reduce transmitted shocks to critical components. In this paper, finite element (FE) models of an armored vehicle prototype having the internal space frame structure with the aforementioned features are developed. One model comprises of only solid elements, while another model is created with purely beam elements. The beam elements model is used for optimization studies whose objective is to reduce the shocks within the vehicle, due to mine blast while maintaining its overall structural integrity. The thickness of the rubberized shock mitigation layer at the joints of the space frame is varied during the optimization process. The optimization problem is solved using the Successive Heuristic Quadratic Approximation (SHQA) algorithm, which combines successive quadratic approximation with an adaptive random search while varying the bounds of the search space. The entire optimization process is carried out within the MATLAB environment. The results show that a significant reduction in the shock can be achieved using this approach.",2011
"Non-Probabilistic Based Topology Optimization Under External Load Uncertainty With Eigenvalue-Superposition of Convex Models","In this paper the Eigenvalue-Superposition of Convex Models (ESCM) based topology optimization method for solving topology optimization problems under external load uncertainties is presented. The load uncertainties are formulated using the non-probabilistic based unknown-but-bounded convex model. The sensitivities are derived and the problem is solved using gradient based algorithm. The proposed ESCM based method yields the material distribution which would optimize the worst structure response under the uncertain loads. Comparing to the deterministic based topology optimization formulation the ESCM based method provided more reasonable solutions when load uncertainties were involved. The simplicity, efficiency and versatility of the proposed ESCM based topology optimization method can be considered as a supplement to the sophisticated reliability based topology optimization methods.",2011
"A Spatial Grammar for the Computational Design Synthesis of Vise Jaws","For the machining and assembly of mechanical parts, their secure fixation in a defined position is crucial. To achieve this task, flexible fixture devices (FFDs) are the industry standard for small and medium batch-sizes. Unlike dedicated fixtures, FFDs allow for the fixation of different part shapes, increasing their applicability and economic efficiency. Aiming to create a low-cost and autonomous FFD, a reconfigurable vise with adaptable jaws was developed. The jaws can be machined to a variety of shapes to securely hold prismatic and cylindrical parts. In this paper, a spatial grammar approach for the computational design synthesis of these customizable jaws is presented. Different sets of rules for the generation of 3D solid models of vise jaws based on the model of the workpiece to be held are developed and realized in a CAD environment. The approach is verified by generating jaw designs for example parts.",2011
"A Knowledge-Based Approach for the Automated Design of Robotic/Human Manufacturing Workcells in Hazardous Environments","The design of manufacturing systems in hazardous environments is complex, requiring interdisciplinary knowledge to determine which components and operators (human or robotic) are feasible. When conceptualizing designs, some options may be overlooked or unknowingly infeasible due to the design engineers’ lack of knowledge in a particular field or ineffective communication of requirements between disciplines. Computational design tools can help alleviate many of the problems encountered in this design task. We create a knowledge-based system (KBS) utilizing CLIPS to automate the synthesis of conceptual manufacturing system designs in radioactive environments. The KBS takes a high-level functional description of a process and uses FBS modeling to generate multiple designs with generic components retrieved from a database and low-level manufacturing task sequences. Using this approach, many options are explored and operator task compatibility is directly addressed. The KBS is applied to the design of glovebox processing systems at Los Alamos National Laboratory (LANL).",2011
"A New Approach for Designing a Gearbox for a New Kind of Independently Controllable Transmissions","This paper deals with a new approach for the development of a gearbox of a new kind of variable transmissions, the independently controllable transmission (ICT). It provides a continuous output speed which is independent of the input speed. In the beginning the requirements of a gearbox are mentioned. Design guidelines are the starting point for the design process of the ICT gearbox. Some of these guidelines will be applied on the gearbox of the ICT to model rules which were needed for the design process. The process of the product design of the gearbox can be improved more efficiently by using multidisciplinary design optimization with evolutionary algorithms and topology optimization. This paper describes an approach for the development of this gearbox and the preparation of the optimization models based on a process chain, which is a guide for the next steps of creating the design process in detail. The first step is a parametric CAD model which consists of different parts and provides the complete design space of the gearbox.",2011
"Topology Optimization of Fluid Channels Using Generative Graph Grammars","This paper presents a new technique for topology optimization of fluid channels using generative design methods. The proposed method uses the generative abilities of graph grammars with simulation and analysis power of conventional CFD methods. The graph grammar interpreter GraphSynth is used to carry out graph transformations, which define different topologies for a given multi-inlet multi-outlet problem. The generated graphs are first transformed into meaningful 3D shapes. These solutions are then analyzed by a CFD solver to find the optimum. The effectiveness of the proposed method is checked by solving a variety of available test problems and comparing them with those found in the literature. Furthermore by solving complex problems the robustness and effectiveness of the method is tested.",2011
"Particle Swarm Optimization With Crossover and Mutation Operators Using the Diversity Criteria","Particle Swarm Optimization is a population based globalized search algorithm that mimics the behavior of swarms. It belongs to the larger class of evolutionary algorithms as widely used stochastic technique in the global optimization field. Since the PSO is population based, it requires no auxiliary information, such as the gradient of the problem.",2011
"A New Method for Design Decisions Using Decision Topologies","This paper shows how reliability block diagrams can be used as a decision making tool. The premise behind the idea is that classical decision analysis while very powerful, does not provide tractability in assessing utility functions and their use in making decisions. Our recent work has shown that visual representation of systems using a reliability block diagram can be used to describe a decision situation. In decision making, we called these block diagrams decision topologies. We show that decision topologies can be used to make many engineering decisions and can replace decision analysis for most decisions. The paper proves that at the limit, using decision topologies is entirely consistent with decision analysis for both single attribute and multiattribute cases. The main advantages of the proposed method are that (1) it provides a visual representation of a decision situation, (2) it can easily model tradeoffs, (3) it allows binary attributes, (4) it can be used when limited information is available, and (5) it can be used in a low-fidelity sense to quickly make a decision. The paper details the theoretical basis of the proposed method and highlights its benefits. An example is used to demonstrate how decision topologies can be used in practice.",2011
"Towards Understanding the Role of Interaction Effects in Visual Conjoint Analysis","We investigate consumer preference interactions in visual choice-based conjoint analysis, where the conjoint attributes are parameters that define shapes shown to the respondent as images. Interaction effects are present when preference for the level of one attribute is dependent on the level of another attribute. When interaction effects are negligible, a main-effects fractional factorial experimental design can be used to reduce data requirements and survey cost. This is particularly important when the presence of many parameters or levels makes full factorial designs intractable. However, if interaction effects are relevant, a main-effects design creates biased estimates and potentially misleading conclusions. Most conjoint studies assume interaction effects are negligible; however, interactions may play a larger role for shape parameters than for other types of attributes. We conduct preliminary tests on this assumption in three visual conjoint studies. The results suggest that interactions can be either negligible or dominant in visual conjoint, depending on both consumer preferences and shape parameterization. When interactions are anticipated, it is possible in some cases to re-parameterize the shape such that interactions in the new space are negligible. Generally, we suggest that randomized designs are better than fractional factorial designs at avoiding bias due to the presence of interactions and/or the organization of profiles into choice sets.",2011
"Expanding the Bottom-Up Design Approach Through Integrating Design Attitudes Into Set-Based Design","In distributed design systems, designers are related to each other through couplings, however they have limited control over the design variables. Any inconsistency in the design system can result in design conflicts through these couplings. Modeling designer attitudes can help to understand inconsistencies and manage conflicts in design processes. We expand the bottom-up design approach through agent-based modeling techniques to another level where designers can make decisions directly on their wellbeing values that represent how their desires are satisfied. Set-based design and constraint programming techniques are used to explore the imprecision of the design activities. Monte Carlo simulations are performed to evaluate the performance of our approach. The results show that the number of design conflicts and their harshness can be lowered when the design process is defined with our approach.",2011
"Preference Construction, Sequential Decision Making, and Trade Space Exploration","This paper develops and explores the interface between two related concepts in design decision making. First, design decision making is a ",2011
"Investigating the Relationship Between Product Design Complexity and FDA for Medical Device Development","Product complexity has been studied as an important factor to decrease the cost and time of the development process. With this purpose, prior research has included the development of design complexity metrics as a method to assess and decrease complexity. Recent studies have also focused on the comparison of complexity metrics for the particular case of medical devices development (MDD). However, the major issue relevant to MDD has not been addressed; the relationship between FDA regulations and the device complexity is not clarified. Therefore, to increase MDD safety and decrease the time to market, we must understand the regulatory decision process and rules. In this paper, we investigate the relation between different complexity metrics and FDA’s decision time using a sample of 100 hip replacement devices. Bayesian network learning is used to explore in detail local relationships between different variables, both complexity measures and product variables. This relationship was found significant for the first two clusters of the analysis. However, for a third cluster it is speculated that FDA decision time does not depend solely upon the degree of medical device complexity. Company or organization relevant variables could be playing a greater role than just complexity. Additional questions are drawn based on the results that must be investigated.",2011
"Multi-Objective Robust Optimization Design of a Fixed-Speed Horizontal Axis Wind Turbine","The produced power and the thrust force exerted on the wind turbine are two conflicting objectives in the design of a floating horizontal axis wind turbine. Meanwhile, the variations in design variables and design environment parameters are unavoidable. The variations include the small variations in the design variables due to manufacturing errors, and the large variations in the wind speed. Therefore, two robustness indices are introduced in this paper. The first one characterizes the robustness of multi-objective optimization problems against small variations in the design variables and the design environment parameters. The second robustness index characterizes the robustness of multi-objective optimization problems against large variations in the design environment parameters. The robustness of the solutions based on the two robustness indices is treated as a vector defined in the robustness function space. As a result, the designer can compare the robustness of all Pareto optimal solutions and make a decision. Finally, the multi-objective robust optimization design of a fixed-speed horizontal axis wind turbine illustrates the proposed methodology.",2011
"Multi-Stage Optimization of Wind Farms With Limiting Factors","Larger onshore wind farms are often installed in phases, with discrete smaller sub-farms being installed and becoming operational in succession until the farm as a whole is completed. An extended pattern search (EPS) algorithm that selects both local turbine position and geometry is presented that enables the installation of a complete farm in discrete stages, exploring optimality of both incremental sub-farm solutions and the completed project as a whole. The objective evaluation is the maximization of profit over the life of the farm, and the EPS uses modeling of cost based on an extensive cost analysis by the National Renewable Energy Laboratory (NREL). The EPS uses established wake modeling to calculate the power development of the farm, and allows for the consideration of multiple or overlapping wakes.",2011
"Wave Energy Extraction Maximization in Irregular Ocean Waves Using Pseudospectral Methods","Energy extraction from ocean waves and conversion to electrical energy is a promising form of renewable energy, yet achieving economic viability of wave energy converters (WECs) has proven challenging. In this article, the design of a heaving cylinder WEC will be explored. The optimal plant (i.e. draft and radius) design space with respect to the design’s optimal control (i.e. power take-off trajectory) for maximum energy production is characterized. Irregular waves based on the Bretschneider wave spectrum are considered. The optimization problem was solved using a pseudospectral method, a direct optimal control approach that can incorporate practical design constraints, such as power flow, actuation force, and slamming. The results provide early-stage guidelines for WEC design. Results show the resonance frequency required for optimal energy production with a regular wave is quite different than the resonance frequency found for irregular waves; specifically, it is much higher.",2011
"Design of Thin Film Solar Cell Material Structures for Reliability and Performance Robustness","An exponential growth of photovoltaic (PV) technologies in the past decade has paved a path to a sustainable solar-powered world. The development of alternative PV technologies with low-cost and high-stability materials has attracted a growing amount of attention. One of these alternatives is the use of second generation thin film PV technologies. However, even in the presence of their bandgap properties, a major issue faced by most thin film solar cells is the low output efficiency due to manufacturing variability and uncertain operating conditions. Thus, to ensure the reliability and performance robustness of the thin film PV technologies, the design of the solar cell is studied. To represent the thin film PV technologies, a copper gallium (di)selenide (CIGS) solar cell model is developed and optimized with Reliability-based Robust Design Optimization (RBRDO) method. This model takes into account the variability of the structure and the material properties of the CIGS solar cells, and assumes an ideal-weather operating condition. This study presents a general methodology to optimize the design of the CIGS PV technologies and could be used to facilitate the development and assessment of new PV technologies with more robust performance in efficiency and stability.",2011
"Design Optimization of a Solar-Powered Reverse Osmosis Desalination System for Small Communities","Fresh water availability is essential for the economic development in small communities in remote areas. In desert climate, where naturally occurring fresh water is scarce, seawater or brackish water from wells is often more abundant. Since water desalination approaches are energy intensive, a strong motivation exists for the design of cost-effective desalination systems that utilize the abundant renewable energy resource; solar energy. This paper presents an optimization model of a solar-powered reverse osmosis (RO) desalination system. RO systems rely on pumping salty water at high pressure through semi-permeable membrane modules. Under sufficient pressure, water molecules will flow through the membranes, leaving salt ions behind, and are collected in a fresh water stream. Since RO system are primarily powered via electricity, the system model incorporates photovoltaic (PV) panels, and battery storage for smoothing out fluctuations in the PV power output, as well as allowing system operation for a number of hours after sunset. Design variables include sizing of the PV solar collectors, battery storage capacity, as well as the sizing of the RO system membrane module and power elements. The objective is to minimize the cost of unit volume produced fresh water, subject to constraints on production capacity. A genetic algorithm is used to generate and compare optimal designs for two different locations near the Red Sea and Sinai.",2011
"Assessing Long-Term Wind Conditions by Combining Different Measure-Correlate-Predict Algorithms","This paper significantly advanced the hybrid measure-correlate-predict (MCP) methodology, enabling it to account for the variations of both wind speed and direction. The advanced hybrid MCP method used the recorded data of multiple reference stations to estimate the long-term wind condition at the target wind plant site with greater accuracy than possible with data from a single reference station. The wind data was divided into different sectors according to the wind direction, and the MCP strategy was implemented for each wind sector separately. The applicability of the proposed hybrid strategy was investigated using four different MCP methods: (i) linear regression; (ii) variance ratio; (iii) artificial neural networks; and (iv) support vector regression. To implement the advanced hybrid MCP methodology, we used the hourly averaged wind data recorded at six stations in North Dakota between the years 2008 and 2010. The station Pillsbury was selected as the target plant site. The recorded data at the other five stations (Dazey, Galesbury, Hillsboro, Mayville, and Prosper) was used as reference station data. The best hybrid MCP strategy from different MCP algorithms and reference stations was investigated and selected from the 1,024 combinations. The accuracy of the hybrid MCP method was found to be highly sensitive to the combination of individual MCP algorithms and reference stations used. It was also observed that the best combination of MCP algorithms was strongly influenced by the length of the correlation period.",2011
"Designing a System of Plug-In Hybrid Electric Vehicle Charging Stations","In this paper we present a two-step approach for the design of a system of Plug-in Hybrid Electric Vehicle (PHEV) charging stations. Our approach consists of a simulation model and a mathematical model. The simulation model formulates the charging station’s ability to meet charging demand by using discrete event simulation. The mathematical model formulates the design decisions made when designing the charging stations, i.e. locations and configurations of charging stations, using the compromise Decision Support Problem (cDSP). Waiting time, service time, number of slots (chargers) and demand are key inputs for the simulation model. Output of the simulation model, which is the service level of the charging stations, is used as an input for the mathematical model. By compromising between maximizing the service level, maximizing the demand coverage, minimizing the installation cost for slots and minimizing distance between charging stations and demand nodes, design decisions are taken in the mathematical model. Our focus in this paper is on the method which is widely applicable. However the approach is presented and evaluated for a data set from Dallas County, Texas.",2011
"Optimum Solar-Powered HDH Desalination System for Semi-Isolated Communities","For semi-isolated communities, fresh water may be scarce; however, brackish water or seawater can be easily accessed. This provides a drive to develop optimum-cost desalination system for such communities. This paper presents the optimization of a water-heated solar-powered humidification-dehumidification (HDH) desalination system with variable saline water flow rate. The design variables include the sizing of solar collector, storage tank and its internal heat exchanger, humidifier and dehumidifier. A program was developed to predict performance based on selected weather data file and optimize the system for minimum unit cost of produced fresh water. System cost is predicted via different first-order estimators. A tailored optimization technique is used and compared to a genetic algorithm procedure in the design optimization for local climate and market. A case study develops an optimum desalination plant for the Red Sea near the city of Hurgada and compared to previously developed system.",2011
"Sensitivity of Array-Like and Optimized Wind Farm Output to Key Factors and Choice of Wake Models","The creation of wakes, with unique turbulence characteristics, downstream of turbines significantly increases the complexity of the boundary layer flow within a wind farm. In conventional wind farm design, analytical wake models are generally used to compute the wake-induced power losses, with different wake models yielding significantly different estimates. In this context, the wake behavior, and subsequently the farm power generation, can be expressed as functions of a series of key factors. A quantitative understanding of the relative impact of each of these factors is paramount to the development of more reliable power generation models; such an understanding is however missing in the current state of the art in wind farm design. In this paper, we quantitatively explore how the farm power generation, estimated using four different analytical wake models, is influenced by the following key factors: (i) incoming wind speed, (ii) land configuration, and (iii) ambient turbulence. The sensitivity of the maximum farm output potential to the input factors, when using different wake models, is also analyzed. The extended Fourier Amplitude Sensitivity Test (eFAST) method is used to perform the sensitivity analysis. The power generation model and the optimization strategy is adopted from the Unrestricted Wind Farm Layout Optimization (UWFLO) framework. In the case of an array-like turbine arrangement, both the first-order and the total-order sensitivity analysis indices of the power output with respect to the incoming wind speed were found to reach a value of 99%, irrespective of the choice of wake models. However, in the case of maximum power output, significant variation (around 30%) in the indices was observed across different wake models, especially when the incoming wind speed is close to the rated speed of the turbines.",2011
"Effects of Uncertain Land Availability, Wind Shear, and Cost on Wind Farm Layout","The robust optimization presented in this paper is formulated to assist in early-stage wind farm development. It can help wind farm developers predict project viability and can help landowners predict where turbines will be placed on their land. A wind farm layout is optimized under multiple sources of uncertainty. Landowner participation is represented with a novel uncertain model of willingness-to-accept monetary compensation. An uncertain wind shear parameter and economies-of-scale cost reduction parameter are also included. Probability Theory, Latin Hypercube Sampling, and Compromise Programming are used to form the robust design problem and minimize the two objectives: the normalized mean and standard deviation of Cost-of-Energy. The results suggest that some landowners that will only accept high levels of compensation are worth pursuing, while others are not.",2011
"An Activity-Based Costing Method to Support Market-Driven Top-Down Product Family Design","As more and more companies offer product families rather than individual products, the competitive advantage of product platforming is shrinking. In order to compete companies need to link marketing and engineering so that designers are able to make decisions about critical trade-offs between cost and performance. The current methods for market-driven platform designs use traditional product costing where indirect costs are assigned to individual products based on relative production quantities. Because of increasing product diversity and decreasing direct labor costs, the ratio of indirect costs to total cost of products is increasing. A method for use during the design stage of top-down product family design is needed to assign indirect costs to individual products based on the product’s consumption of indirect resources. An activity-based costing method for top-down product family design is presented here. This method allows the designer to model indirect costs as a function of engineering attributes, creating a framework for top-down product platform optimization that provides a more accurate estimation of cost than traditional product costing methods. An illustrative example shows that an activity-based costing model predicts different profitability from a traditional costing system for a number of different motor designs.",2011
"Predicting Consumer Choice Set Using Product Association Network and Data Analytics","Although discrete choice analysis has been shown to be useful for modeling consumer preferences and choice behaviors in the field of engineering design, information of choice set composition is often not available in majority of the collected consumer purchase data. When a large set of choice alternatives exist for a product, such as automotive vehicles, randomly choosing a small set of product alternatives to form a choice set for each individual consumer will result in misleading choice modeling results. In this work, we propose a data-analytics approach to mine existing data of choice sets and predict the choice set for each individual customer in a new choice modeling scenario where the choice set information is lacking. The proposed data-analytics approach integrates product association analysis, network analysis, consumer segmentation, and predictive analytics. Using the J.D. Power vehicle survey as the existing choice set data, we demonstrate that the association network approach is capable of recognizing and expressively summarizing meaningful product relations in choice sets. Our method accounts for consumer heterogeneity using the stochastic generation algorithm where the probability of selecting an alternative into a choice set integrates the information of customer profile clusters and products chosen frequencies. By comparing multiple multinomial logit models using different choice set compositions, we show that the choice model estimates are sensitive to the choice set compositions and our proposed method leads to improved modeling results. Our method also provides insights into market segmentation that can guide engineering design decisions.",2011
"A Bayesian Approach to Extrinsic Versus Intrinsic Uncertainty in Design for Market Systems","This article illustrates how variance in the predictive distribution of the profit objective function in a design for market systems model can be decomposed into two components using a simulation based Bayesian approach introduced in the econometrics literature. The first component, intrinsic uncertainty, would be retained in the model even if the model calibration parameter values, such as parameters representing customer preferences, were known with certainty. The second component, extrinsic uncertainty, stems from lack of precision regarding model calibration parameters such as customer preferences. The simulation based approach overcomes a key problem in decomposing uncertainty for the typical design for market systems problem by overcoming the difficulties associated with analytical treatment of non-normal distributions. The variance decomposition approach is demonstrated for the design of a handheld grinder power tool. Following the same Bayesian decision analysis framework the variance simulation method can be applied to other design for market system problems with other objective functions and with additional sources of uncertainty.",2011
"Strategies for Consumer Control of Complex Product Forms in Generative Design Systems","In recent years, the number of products that can be tailored to consumers’ needs and desires has increased dramatically; there are many opportunities to individualize the colors, materials or options of products. However, current trends indicate that the future consumer will not be satisfied with mere material and color choices, but will desire control over form as well. While it is technically feasible to allow consumers to partially mass-customize the form of products subject to functional and production constraints through the use of a generative design system, the question of how the control of form should be presented to the user arises. The issue becomes especially important when the product form is based on complex morphologies, which require in-depth knowledge of their parameters to be able to control them fully. In this paper, we discuss this issue and present and test two strategies for controlling complex forms in consumer-oriented generative design systems, one offering the user full control over the design (“total control” strategy), while the other automatically generates designs for the user (“no control” strategy). The implementation of those two control strategies in a generative design system for two categories of products (bookshelf and table) and five types of morphologies are described and tested with a number of design interested participants to estimate their level of satisfaction with the two control strategies. The empirical study shows that the participants enjoyed both the total control and no control strategies. The development of the full control modes for the five morphologies was on the other hand not straightforward, and in general, making the controls meaningful to the consumer can be difficult with complex morphologies. It seems that a consumer-oriented generative design system with two different control strategies, as the ones presented in this article, would offer the most satisfaction.",2011
"Robust Supply Chain Network Design by Considering Demand-Side Uncertainty and Supply-Side Disruption","In this paper we propose a method that includes stochastic and robust models for designing the network structure of a three-tiered supply chain involving suppliers, manufacturers and retailers under random demands of markets and by considering disruption probabilities in the procurement facilities and connecting links of the chain. Demands of the markets are as assumed to be random variables with normal distribution and the disruption of the chain is formulated by defining different scenarios. Having substitutable facilities and extra production capacities makes it possible to overcome the negative effects of demand fluctuations and disruption of facilities. First we determine the optimal number, location and capacity of facilities in the first and third tiers and the best flow of material and product throughout the chain in a way to maximize the expected profit from the chain. Then to reduce the difference between the performances of the chain in different scenarios, we propose the ISP-Robust technique based on P-Robust technique to make the model robust. We discuss the differences and advantages of this technique in comparison to the P-Robust technique. We illustrate our method using data from the petrochemical industry.",2011
"Extracting Consumer Preference From User-Generated Content Sources Using Classification","The use of online, user-generated content for consumer preference modeling has been a recent topic of interest among the engineering and marketing communities. With the rapid growth of many different types of user-generate content sources, the tasks of reliable opinion extraction and data interpretation are critical challenges. This research investigates one of the largest and most-active content sources, Twitter, and its viability as a content source for preference modeling. Support Vector Machine (SVM) is used for sentiment classification of the messages, and a Twitter query strategy is developed to categorize messages according to product attributes and attribute levels. Over 7,000 messages are collected for a smartphone design case study. The preference modeling results are compared with those from a typical product review study, including over 2,500 product reviews. Overall, the results demonstrate that consumers do express their product opinions through Twitter; thus, this content source could potentially facilitate product design and decision-making via preference modeling.",2011
"Enhanced Targeted Initial Populations for Multiobjective Product Line Optimization","Initial populations for genetic algorithms are often created using randomly generated designs in an effort to maximize the genetic diversity in the design space. However, research indicates that the inclusion of solutions generated based on domain knowledge (i.e. non-random solutions) can notably improve the performance of the genetic algorithm with respect to solution performance and/or computational cost for convergence. This performance increase is extremely valuable for computationally expensive problems, such as product line optimization. In prior research, the authors demonstrated these improvements for product line design problems where market share of preference was the performance objective. Initial product line solutions were constructed from products that had the largest product-level utility for individual respondents. However, this simple product identification strategy did not adequately scale to accommodate the richer design problem associated with multiple objectives. This paper extends the creation of targeted initial populations to multiobjective product line design problems by using the objectives of the problem, instead of product level utility, to identify candidate designs. A MP3 player and vehicle feature packaging product line design problems are used to demonstrate this approach and assess the improvement of this modification.",2011
"Sensitivity of Vehicle Market Share Predictions to Alternative Discrete Choice Model Specifications","When design decisions are informed by consumer choice models, uncertainty in the choice model and its share predictions creates uncertainty for the designer. We take a first step in investigating the variation in and accuracy of market share predictions by characterizing fit and forecast accuracy of multinomial logit, mixed logit, and nested logit models over a variety of utility function specifications for the US light duty new vehicle market. Using revealed preference data for years 2004–2006, we estimate a multinomial logit model for each combination of a chosen set of utility function covariates found in the literature. We then use each of the models to predict vehicle shares for the 2007 market and examine several metrics to measure fit and predictive accuracy. We find that the best models selected using any of the proposed metrics outperform random guessing yet retain substantial error in fit and prediction for individual vehicle models. For example, with no information (random guessing) 30% of share predictions are within 0.2% absolute share error in a market with an average share of ∼0.4%, whereas for the best models 70% are within 0.2% (for the 2007 vehicle market this translates to an error of ∼33,000 units sold). Share predictions are sensitive to the presence of utility covariates but less sensitive to the form. Models that perform well on one metric tend to perform well on the other metrics as well. In particular, models selected for best fit have comparable forecast error to those with the best forecasts, and residual error in model fit is a major source of forecast error.",2011
"Determining Vehicle-Level Specifications for a New Car Program Considering Market Environments and Engineering Design Constraints","Designing a marketable automobile for target markets is an important challenge in the globalized market environment. What makes the vehicle planning even more challenging includes various factors such as engineering feasibility, change of market environment, enforced regulations, and ongoing advancement of technology. In this circumstance, the complexity of automobile design arises from the fact that both the target market’s customer preferences and engineering characteristics should be taken into account in balanced as well as quantitative manners.",2011
"A Framework for System Design Optimization Based on Maintenance Scheduling With Prognostics and Health Management","The optimal maintenance scheduling of systems with degrading components is highly coupled with the design of the system and various uncertainties associated with the system, including the operating conditions, the interaction of different degradation profiles of various system components, and the ability to measure and predict degradation using prognostics and health management (PHM) technologies. Due to this complexity, designers need to understand the correlations and feedback between the design variables and lifecycle parameters to make optimal decisions. A framework is proposed for the high level integration of design, component degradation, and maintenance decisions. The framework includes constructing screening models for rapid design evaluation, defining a multi-objective robust optimization problem, and using sensitivity studies to compare trade-offs between different design and maintenance strategies. A case example of power plant condenser is used to illustrate the proposed framework and advise how designers can make informed comparisons between different design concepts and maintenance strategies under highly uncertain lifecycle conditions.",2011
"Concurrent Design of Functional Reliability and Failure Prognosis for Engineered Resilience","This paper presents a new system design platform and approaches leading to the development of resilient engineered systems through integrating design of system functions and prognosis of function failures in a unified design framework. Failure prognosis plays an increasingly important role in complex engineered systems since it detects, diagnoses, and predicts the system-wide effects of adverse events, therefore enables a proactive approach to deal with system failures at the life cycle use phase. However, prognosis of system functional failures has been largely neglected in the past at early system design stage, mainly because quantitative analysis of failure prognosis in the early system design stage is far more challenging than these activities themselves that have been mainly carried out at the use phase of a system life cycle. In this paper, a generic mathematical formula of resilience and predictive resilience analysis will be introduced, which offers a unique way to consider lifecycle use phase failure prognosis in the early system design stage and to systematically analyze their costs and benefits, so that it can be integrated with system function designs concurrently to generate better overall system designs. Engineering design case studies will be used to demonstrate the proposed design for resilience methodology.",2011
"Design of a Robust Classification Fusion Platform for Structural Health Diagnostics","Efficient health diagnostics provides benefits such as improved safety, improved reliability, and reduced costs for the operation and maintenance of engineered systems. This paper presents a multi-attribute classification fusion approach which leverages the strengths provided by multiple membership classifiers to form a robust classification model for structural health diagnostics. Health diagnosis using the developed approach consists of three primary steps: (i) fusion formulation using a k-fold cross validation model; (ii) diagnostics with multiple multi-attribute classifiers as member algorithms; and (iii) classification fusion through a weighted majority voting with dominance system. State-of-the-art classification techniques from three broad categories (i.e., supervised learning, unsupervised learning, and statistical inference) were employed as the member algorithms. The proposed classification fusion approach is demonstrated with a bearing health diagnostics problem. Case study results indicated that the proposed approach outperforms any stand-alone member algorithm with better diagnostic accuracy and robustness.",2011
"Computerized Systematic Approach to Fault Tree Analysis Based on Quantity Dimension Indexing","Fault tree analysis (FTA) is an effective method of ensuring the security and safety of the product by identifying all the possible causes of the problem and fixing them. However, it is not easy for a designer to construct a complete fault tree about various physical phenomena without any misunderstanding or oversight, and some computerized method of managing (i.e., storing, searching and utilizing) knowledge about FTA is needed. To solve the problem, the authors have proposed and studied a method and software tool for knowledge management of FTA based on quantity dimension indexing as a design knowledge management method to avoid ambiguity of literal expression about physical phenomena. In the previous method and software, however, fault values of quantities were limited as just above- and below-normal, and dynamic phenomena such as oscillation could not be described. In this paper, the authors introduce a systematically classified definition of fault values as above/below normal, one-side/both-sides, constant/varying, monotonic/non-monotonic and sudden/gradual, and expand the computerized systematic approach to FTA. Feasibility of the method was examined by applying it to fault tree examples made in a company.",2011
"A Copula-Based Sampling Method for Data-Driven Prognostics and Health Management","This paper develops a Copula-based sampling method for data-driven prognostics and health management (PHM). The principal idea is to first build statistical relationship between failure time and the time realizations at specified degradation levels on the basis of off-line training data sets, then identify possible failure times for on-line testing units based on the constructed statistical model and available on-line testing data. Specifically, three technical components are proposed to implement the methodology. First of all, a generic health index system is proposed to represent the health degradation of engineering systems. Next, a Copula-based modeling is proposed to build statistical relationship between failure time and the time realizations at specified degradation levels. Finally, a sampling approach is proposed to estimate the failure time and remaining useful life (RUL) of on-line testing units. Two case studies, including a bearing system in electric cooling fans and a 2008 IEEE PHM challenge problem, are employed to demonstrate the effectiveness of the proposed methodology.",2011
"A Heat Transfer Model for the Conceptual Design of a Biomass Cookstove for Developing Countries","The use of biomass cookstoves to meet household energy needs has a profound impact on the life and health of individuals, families, and communities in the developing world. This paper introduces an experimentally validated heat transfer analysis model for use during the conceptual design process of a biomass cookstove to be used in the developing world. This steady-state model of a shielded, natural-draft biomass cookstove fitted with a flat-bottomed pot with pot-shield was developed using published experimental data that included 63 variations of 15 operating, geometrical, and material variables. The model provides the essential information needed to support decision making during the cookstove conceptual design process by predicting heat transfer efficiency as a function of stove geometry, construction material, firepower, and fuel moisture content.",2011
"Eight Principles Derived From the Engineering Literature for Effective Design for the Developing World","This paper reviews the findings of several engineering researchers and practitioners on the topic of design for the developing world. We arrange these findings into eight guiding principles aimed at helping those who are searching for effective and sustainable approaches for design for the developing world. The findings reviewed come from the mechanical engineering discipline, as well as from other engineering disciplines. For each principle, we provide references to various studies as a means of supporting the principle. We also provide a detailed example of each principle. Finally, based on our own experience and based on the many papers reviewed, we provide a succinct list of suggestions for using each principle. Ultimately, we believe that the stated principles help overcome the challenges of design for the developing world, which are often dominated by designer unfamiliarity with poverty and foreign culture, as well as by the constraint of extreme affordability.",2011
"Establishing Consumer Need and Preference for Design of Village Cooking Stoves","In some villages the use of wood cooking stoves accounts for more than three-quarters of total village energy use. Because of this the design of clean, affordable, and desirable cooking stoves can have a dramatic impact on human health and the local economy. Unfortunately, too often development projects fail. For example, an estimated 30% of water projects in sub-Saharan Africa have failed prematurely in the last 20 years, and only 10% of cooking stove programs started in the 1980s were operational two years after startup. Similar anecdotal evidence suggests a mixed record of success for other energy, infrastructure, health, and sanitation projects in the developing world. In part, these failures occur because of a lack of design questions and design methods to identify consumer need and preference during the problem definition phase of the product design. Because isolated rural villages are generally far from the design engineers’ previous experiences it is even more important to gather in-depth primary data in isolated rural villages. Based on data collected during in-depth field visits to villages in rural West Africa during a village energy study this paper proposes a structured process for collecting the data necessary to design cookstoves that meet local needs, fit within local contexts, and create an aspirational experience that fosters a sustainable solution.",2011
"Techno-Economic Design of Off-Grid Domestic Lighting Solutions Using HOMER","Kerosene, candles, and disposable batteries are commonplace in the developing world for rural domestic lighting. These technologies come with negative health and environmental effects that are well documented and often form the basis for engineering design. The immediate and near-term concerns that families experience on a daily basis are also important — economics, quality of light, and quality of service. Families in off-grid rural villages often spend more than half of their energy-related expenditures on domestic lighting. Many technologies have been implemented to provide low-cost and renewable power for lighting, yet these efforts have had a mixed record of success due to persistent financial barriers, issues of consumer acceptance and adoption, and a variety of technical complications. The incidence of these problems can be reduced by completing a techno-economic comparison of alternatives during conceptual design. This paper compares three major categories of off-grid domestic lighting projects: (1) centralized electrification with a micro-grid, (2) battery charging stations, and (3) solar lanterns. The HOMER Energy software is used to compare these options using data gathered from rural villages in Africa. To offer a comparison to existing options available, this paper provides a full financial comparison to a base case — kerosene lanterns — to suggest financing strategies and business models for the options investigated.",2011
"Structural Complexity Quantification for Engineered Complex Systems and Implications on System Architecture and Design","The complexity of today’s highly engineered products is rooted in the interwoven architecture defined by its components and their interactions. Such structures can be viewed as the adjacency matrix of the associated dependency network representing the product architecture. To evaluate a complex system or to compare it to other systems, numerical assessment of its structural complexity is essential. In this paper, we develop a quantitative measure for structural complexity and apply the same to real-world engineered systems like gas turbine engines. It is observed that low topological complexity implies centralized architectures and that higher levels of complexity generally indicate highly distributed architectures. We posit that the development cost varies non-linearly with structural complexity. Empirical evidence of such behavior is presented from the literature and preliminary results from simple experiments involving assembly of simple structures further strengthens our hypothesis. We demonstrate that structural complexity and modularity are not necessarily negatively correlated using a simple example. We further discuss distribution of complexity across the system architecture and its strategic implications for system development efforts.",2011
"Utilizing Emergent Levels to Facilitate Complex Systems Design: Demonstrated in a Synthetic Biology Domain","Designing complex systems often requires consideration of many components interacting across vast scales of space and time, thus producing highly challenging design spaces to search. In particular, nano-based technologies may require considerations of how nanoscale (10",2011
"Evolutionary Design of Cellular Self-Organizing Systems","In this paper, a genetic algorithm (GA) is used to discover interaction rules for a cellular self-organizing (CSO) system. The CSO system is a group of autonomous, independent agents that perform tasks through self-organization without any central controller. The agents have a local neighborhood of sensing and react only to other agents within this neighborhood. Their interaction rules are a simple set of direction vectors based on a flocking model. The five local interaction rules are assigned relative weights, and the agents self-organize to display some emergent behavior at the system level. The engineering challenge is to identify which sets of local rules will cause certain desired global behaviors. The global required behaviors of the system, such as flocking or exploration, are translated into a fitness function that can be evaluated at the end of a multi-agent based simulation run. The GA works by tuning the relative weights of the local interaction rules so that the desired global behavior emerges, judged by the fitness function. The GA approach is shown to be successful in tuning the weights of these interaction rules on simulated CSO systems, and, in some cases, the GA actually evolved qualitatively different local interaction “strategies” that displayed equivalent emergent capabilities.",2011
"Probabilistic Design of Smart Sensing Functions for Structural Health Monitoring and Prognosis","Significant technological advances in sensing and communication promote the use of large sensor networks to monitor structural systems, identify damages, and quantify damage levels. Prognostics and health management (PHM) technique has been developed and applied for a variety of safety-critical engineering structures, given the critical needs of the structure health state awareness. The PHM performance highly relies on real-time sensory signals which convey the structural health relevant information. Designing an optimal structural sensor network (SN) with high detectability is thus of great importance to the PHM performance. This paper proposes a generic SN design framework using a detectability measure while accounting for uncertainties in material properties and geometric tolerances. Detectability is defined to quantify the performance of a given SN. Then, detectability analysis will be developed based on structural simulations and health state classification. Finally, the generic SN design framework can be formulated as a mixed integer nonlinear programming (MINLP) using the detectability measure and genetic algorithms (GAs) will be employed to solve the SN design optimization problem. A power transformer study will be used to demonstrate the feasibility of the proposed generic SN design methodology.",2011
"Resilient Design of Complex Engineered Systems","This paper presents a complex network and graph spectral approach to calculate the resiliency of complex engineered systems. Resiliency is a key driver in how systems are developed to operate in an unexpected operating environment, and how systems change and respond to the environments in which they operate. This paper deduces resiliency properties of complex engineered systems based on graph spectra calculated from their adjacency matrix representations, which describes the physical connections between components in a complex engineered systems. In conjunction with the adjacency matrix, the degree and Laplacian matrices also have eigenvalue and eigenspectrum properties that can be used to calculate the resiliency of the complex engineered system. One such property of the Laplacian matrix is the algebraic connectivity. The algebraic connectivity is defined as the second smallest eigenvalue of the Laplacian matrix and is proven to be directly related to the resiliency of a complex network. Our motivation in the present work is to calculate the algebraic connectivity and other graph spectra properties to predict the resiliency of the system under design.",2011
"Descriptor-Based Methodology for Designing Heterogeneous Microstructural Materials System","In designing a microstructural materials system, there are several key questions associated with design representation, design evaluation, and design synthesis: how to quantitatively represent the design space of a heterogeneous microstructure system using a small set of design variables, how to efficiently reconstruct statistically equivalent microstructures for design evaluation, and how to quickly search for the optimal microstructure design to achieve the desired material properties. This paper proposes a new descriptor-based methodology for designing microstructural materials systems. A descriptor-based characterization method is proposed to provide a quantitative representation of material morphology using a small set of microstructure descriptors covering features of material composition, dispersion status, and phase geometry at different levels of representation. A descriptor-based multi-phase microstructure reconstruction algorithm is developed which allows efficient stochastic reconstruction of microstructures for Finite Element Analysis (FEA) of material behavior. The choice of descriptors for polymer nanocomposites is verified by establishing a mapping between the finite set of descriptors and the infinite dimensional correlation function. Finally, the descriptor-based representation allows the use of parametric optimization approach to search the optimal microstructure design that meets the target material properties. To improve the search efficiency, this paper employs state-of-the-art computational design methods such as Design of Experiment (DOE), metamodeling, statistical sensitivity analysis, and multi-objective optimization. The proposed methodology is demonstrated using the design of a polymer nanocomposites system.",2011
"Design Methods for Integrated Design of Blast Resistance Panels and Materials","Integrated Materials and Products Design (IMPD) differs in the way that materials as well as product layout are designed or optimized in a concurrent manner to meet design requirements. IMPD allows the specific performance required in a product to be achieved by tailoring materials and product, since system performance will not be limited by a pre-chosen material employed in conventional, material-selection-based design. In this study, Blast Resistance Panels (BRPs) with square honeycomb core are designed based on this new design approach to further enhance the performance of BRPs.",2011
"An Energy-Based Design Approach for a Meso-Structure With High Shear Flexure","This paper presents a design approach for developing meso-structure for high shear flexure in considering distribution of strain energy for a unit cell. Currently, flexible components are often designed with elastomers to take advantage of their unique properties of low shear modulus and high elongation. However, elastomers exhibit high loss modulus at a high frequency when they are subjected to cyclic loading. As a design requirement to find an alternative material in one of the sub systems in the extraterrestrial rover, materials with high elongation but low energy loss is investigated using a meso-structure design approach. In this paper, an approach to design a meso-structure exhibiting shear flexure is developed by conducting comparative studies of shear flexure on three equivalent configurations: auxetic, honeycomb, and sinusoidal. Based on this comparative study, a new hypothesis is proposed that specific strain energy distribution pattern in these meso-structure has a direct impact on the high shear flexure performance. This proposition is verified by developing a new meso-structure, termed ‘S’-Type, which is compared with auxetic and sinusoidal auxetic meso-structures on their shear flexure ability. It is shown from this comparative analysis that the ‘S’-Type meso-structure exhibits higher shear flexure than the other two meso-structures at 5, 10, 20, and 40 MPa of effective shear moduli. Hence, based on this result, a four-step design approach is proposed to design future meso-structures with high shear flexure.",2011
"A Comparison of Design Approaches to Meso-Structure Development","The design protocols of meso-structures to satisfy given properties have been an interesting area in material science. Having various mechanical properties by modifying the topology of unit cells is a major issue in developing new meso-structures. This paper reviews different design methodologies to design meso-structures. For each method, an algorithm is presented and compared. Computational methods including topology optimization, parametric optimization, and synthesis methods are among the most popular methods for design meso-structures. Ultimately, it is found that there is a gap in systematic design methods for developing new meso-structure architectures as the current methods are limited to parametric sizing and selection. This gap will be addressed in future research.",2011
"The Boundary Smoothing in Discrete Topology Optimization of Structures","In discrete topology optimization, material state is either solid or void and there is no topology uncertainty caused by intermediate material state. A common problem of the current discrete topology optimization is that boundaries are unsmooth. Unsmooth boundaries are caused by corners in topology solutions. Although the outer corner cutting and inner corner filling strategy can mitigate corners, it cannot eliminate them. 90-degree corners are usually mitigated to 135-degree corners under the corner handling strategy. The existence of corners in topology solutions is because of the subdivision model. If regular triangles are used to subdivide design domains, corners are inevitable in topology solutions. To eradicate corner from any topology solution, a subdivision model is introduced in this paper for the discrete topology optimization of structures. The design domain is discretized into quadrilateral design cells and every quadrilateral design cell is further subdivided into triangular analysis cells that have a curved hypotenuse. With the presented subdivision model, all boundaries and connections are smooth in any topology solution. The proposed subdivision approach is demonstrated by two discrete topology optimization examples of structures.",2011
"Multiscale Topology Optimization of Structures and Periodic Cellular Materials","The introduction of cellular materials models in topology optimization allows designers to achieving significant weight reductions in structural applications. However, higher material savings and increased performance can be achieved if the material and the structure topologies are concurrently designed. The objective of this paper is to incorporate and establish a design methodology to obtaining optimal macro-scale structures and the corresponding optimal meso-scale periodic material designs in continuum design domains. The proposed approach makes use of homogenization theory to establish communication bridges between both material and structural scales. The periodicity constraint makes such cellular materials manufacturable. Penalization methods are used to obtaining binary solutions in both scales. This proposed methodology is demonstrated in the design of compliant mechanisms and structures of minimum compliance. The results demonstrate potential benefits when this multi-scale design algorithm when applied to the design of ultra-lightweight structures.",2011
"Hierarchical Design of Composite Materials With Negative Stiffness Inclusions Using a Bayesian Network Classifier","Recent research in the field of composite materials has shown that it is theoretically possible to produce composite materials with macroscopic mechanical stiffness and loss properties that surpass those of conventional composites. This research explores the possibility of designing and fabricating these composite materials by embedding small volume fractions of negative stiffness inclusions in a continuous host material. Achieving high stiffness and loss from these materials by design, however, is a nontrivial task. This paper presents a hierarchical multiscale material model for these materials, coupled with a set-based, multilevel design approach based on Bayesian network classifiers. Bayesian network classifiers are used to map promising regions of the design space at each hierarchical modeling level, and then the maps are intersected to identify sets of multilevel or multiscale solutions that are likely to provide desirable system performance. Length scales range from the behavior of the structured microscale negative stiffness inclusions to the effective properties of mesoscale composite materials to the performance of an illustrative macroscale component — a vibrating beam coated with the high stiffness, high loss composite material.",2011
"Application of Regional Strain Energy in Topology Optimization With Inertia Relief Analysis","In finite element analysis, inertia relief solves the response of an unconstrained structure subject to constant or slowly varying external loads with static analysis computational cost. It is very attractive to utilize it in topology optimization to design structures under unbalanced loads, such as in impact and drop phenomena. In this paper, regional strain energy formulation and inertia relief is integrated into topology optimization to design protective structure under unbalanced loads. For background, the equations of inertia relief are introduced and a commonly used solving method is revisited. Then the regional strain energy formulation for topology optimization with inertia relief is proposed and its sensitivity is derived from the adjoint method. Based on the solving method, the sensitivity is evaluated term by term to simplify the results. The simplified sensitivity can be calculated easily using the output of commercial finite element packages. Finally, the effectiveness of this formulation is shown in the first example and the proposed regional strain energy formulation for topology optimization with inertia relief are presented and discussed in the protective structure design examples.",2011
"Ceramic Matrix Composite Materials by Design Using Robust Variable Fidelity Optimization","Ceramic matrix composites (CMC) have been widely studied to tailor desired properties at high temperatures. However, research applications involving design tool development for multi-phase material design are at an early stage of development. While numerical CMC modeling provides significant insight on the material performance, the computational cost of the numerical simulations and the type of variables involved in these models are a hindrance for the effective application of design methods. This technical challenge heightens with the need of considering the uncertainty of material processing and service. For this reason, few design researchers have addressed the design paradox that accompanies the rapid design space expansion in CMC material design.",2011
"Early-Stage Design of Rheologically Complex Materials via Material Function Design Targets","Rheological material properties are high-dimensional function-valued quantities, such as frequency-dependent viscoelastic moduli or non-Newtonian shear viscosity. Here we describe a process to model and optimize design targets for such rheological material functions. For linear viscoelastic systems, we demonstrate that one can avoid specific a priori assumptions of spring-dashpot topology by writing governing equations in terms of a time-dependent relaxation modulus function. Our approach embraces rheological design freedom, connecting system-level performance to optimal material functions that transcend specific material classes or structure. This technique is therefore material agnostic, applying to any material class including polymers, colloids, metals, composites, or any rheologically complex material. These early-stage design targets allow for broadly creative ideation of possible material solutions, which can then be used for either material-specific selection or later-stage design of novel materials.",2011
"Elastohydrodynamic Ball Bearing Optimization Using Genetic Algorithm and Heuristic Gradient Projection","Rolling element bearings operation depends on some variables contributing to the machine element performance. The present work attempts to improve the performance of rolling element bearings through the increase of fatigue life and the reduction of bearing wear. The formulation is based on Elastohydrodynamic to maximize the realistically evaluated minimum film thickness without significant increase in viscous friction torque. The multiobjective problem can then be stated as maximization of minimum film thickness and minimization of total friction torque. Design vectors are reduced in the present study relative to previous studies as some variables are considered as dependent variables. A new important parameter is introduced in this study as a design variable, which is the viscosity of lubricant (",2011
"PipeSynth: An Algorithm for Automated Topological and Parametric Design and Optimization of Pipe Networks","This paper describes a design automation approach that combines various optimization research and artificial intelligence methods for synthesizing fluid networks. Unlike traditional software tools available today, this approach does not rely on having any predefined network topology to design and optimize its networks. PipeSynth generates its designs by using only desired port locations, and the desired fluid properties at each of those ports. An ideal network is found by optimizing the number and connectivity of pipes and pipe fittings, the size and length of each pipe, and the size and orientation of each fitting. A Uniform-Cost-Search is used for topology optimization along with a combination of non-gradient based optimization methods for parametric optimization. PipeSynth demonstrates how advances in automated design can enable engineers to manage much more complex fluid network problems. PipeSynth uses a unique representation of fluid networks that synthesizes and optimizes networks one pipe at a time, in three-dimensional space. PipeSynth has successfully solved several problems containing multiple interlaced networks concurrently with multiple inputs and outputs. PipeSynth shows the power of automated design and optimization in producing solutions more effectively and efficiently than traditional design approaches.",2011
"Compliant Mechanism Design Using a Strain Based Topology Optimization Method","Energy based topology optimization method has been used in the design of compliant mechanisms for many years. Although many successful examples from the energy based topology optimization have been presented, optimized configurations of these designs are often very similar to their rigid linkage counterparts except using compliant joints in place of rigid links. It is obvious that these complaint joints will endure large deformations under the applied forces in order to perform the specified motions and the large deformation will produce high stress which is very undesirable in compliant mechanism design. In this paper, a strain based topology optimization method is proposed to avoid localized high deformation design which is one of the drawbacks using strain energy formulation. Therefore, instead of minimizing the strain energy for structural rigidity, a global effective strain functional is minimized in order to distribute the deformation within the entire mechanism while maximizing the structural rigidity. Furthermore, the physical programming method is adopted to accommodate both flexibility and rigidity design objectives. Comparisons of design examples from both the strain energy based topology optimization and the strain based method are presented and discussed.",2011
"Optimizing the Shear Beam of a Non-Pneumatic Wheel for Low Rolling Resistance","The design requirements of a low rolling loss non-pneumatic wheel are determined through a systematic optimization approach. In order to reduce the rolling resistance, linear elastic materials are considered instead of elastomers. To achieve an adequate compliance level, a metamaterial needs to be designed. The required metamaterial properties are determined as a result of an optimization where the metamaterial tensor components as well as the geometric dimensions are the design variables. This way the metamaterial can be designed such that the overall behavior of the non pneumatic wheel achieves the best performance in terms of compliance and contact patch pressure distribution. The resulting constitutive metamaterial properties of the shear layer can be used as prescribed constitutive properties to tailor the periodic mesostructure of a material by means of topology optimization.",2011
"Decomposition Templates and Joint Morphing Operators for Genetic Algorithm Optimization of Multi-Component Topology","This paper presents a continuum-based approach for multi-objective topology optimization of multi-component structures. Objectives include minimization of compliance, weight and as cost of assembly and manufacturing. Decision variables are partitioned into two main groups: those pertaining to material allocation within a design domain (base topology problem), and those pertaining to decomposition of a monolithic structure into multiple components (joint allocation problem). Generally speaking, the two problems are coupled in the sense that the decomposition of an optimal monolithic structure is not always guaranteed to produce an optimal multi-component structure. However, for spot-welded sheet-metal structures (such as those often found in automotive applications), certain assumptions can be about the performance of a monolithic structure that favor the adoption of a two-stage approach that decouples the base topology and joint allocation problems. A multi-objective genetic algorithm (GA) is used throughout the studies in this paper. While the problem decoupling in two-stage approaches significantly reduces the size of the search space and allows better performance of the GA, the size of the search space can still be quite enormous in the second stage. To further improve the performance, we propose a new mutation operator based on decomposition templates and localized joints morphing. A cantilever-loaded structure is then used as a metric to study and compare various setups of single and two-stage GA approaches.",2011
"On Usage Context of Hybrid Electric Vehicles in Choice Studies","Considering usage context attributes in choice modeling has been shown to be important when product performance highly depends on the usage context. To build a reliable choice model, it is critical to first understand the relationship between usage context attributes and customer profile attributes, then to identify the market segmentation characterized by both sets of attributes, and finally to construct a choice model by integrating data from multiple sources. This is a complex procedure especially when a large number of customer attributes are potentially influential to the product choice. Using the hybrid electric vehicle (HEV) as an example, this paper presents a systematic procedure and the associated data analysis techniques for implementing each of the above steps. Usage context and customer profile attributes extracted from both National Household Travel Survey (NHTS) and Vehicle Quality Survey (VQS) data are first analyzed to understand the relationship between usage context attributes and customer profile attributes. Next the principal component analysis is utilized to identify the key characteristics of hybrid vehicle drivers, and to determine the market segmentations of HEV and the critical attributes to include in choice models. Before the two sets of data are combined for choice modeling, statistical analysis is used to test the compatibility of the two datasets. A pooled choice model created by incorporating usage context attributes illustrates the benefits of context-based choice modeling using data from multiple sources. Even though NHTS and VQS have been used in the literature to study transportation patterns and vehicle quality ratings, respectively, this work is the first to explore how they may be used together to benefit the study of customer preference for HEVs.",2011
"Exploring Heterogeneity of Customer Preference to Balance Commonality and Market Coverage","Offering increased variety in a market is one method of capturing greater market share. However, we generally observe diminishing marginal returns in share as the size of the product line is increased. Leveraging commonality is a means of offsetting this constraint as it leads to reductions in manufacturing costs and build complexity. Product platforms strive to capitalize on the naturally occurring phenomena that yield commonality in a product line. The structure of design variable values of individually optimized products create opportunities for commonality in a bottom-up platform, while a top-down platform discovers opportunities for commonality through similarity in customer preferences. This paper explores the effect of changing the number of products, and commonality between those products, on market share. Results from designing a varying number of products independently are leveraged to create a bottom-up product platform. A top-down product platform approach based on a heterogeneous discrete choice model and a multiobjective genetic algorithm is presented that allow commonality decisions and product configuration to occur simultaneously. Using the platforming techniques presented in this paper, it is shown that the top-down platforming approach allows for more well-informed platformed design by providing knowledge of the tradeoff between commonality and market share.",2011
"Exploring Differences in Preference Heterogeneity Representation and Their Influence in Product Family Design","When using conjoint studies for market-based design, two model types can be fit to represent the heterogeneity present in a target market, discrete or continuous. In this paper, data from a choice-based conjoint study with 2275 respondents is analyzed for a 19-attribute combinatorial design problem with over 1 billion possible product configurations. Customer preferences are inferred from the choice task data using both representations of heterogeneity. The hierarchical Bayes mixed logit model exemplifies the continuous representation of heterogeneity, while the latent class multinomial logit model corresponds to the discrete representation. Product line solutions are generated by each of these model forms and are then explored to determine why differences are observed in both product solutions and market share estimates. These results reveal some potential limitations of the Latent Class model in the masking of preference heterogeneity. Finally, the ramifications of these results on the market-based design process are discussed.",2011
"A Modified Formulation for Automatic Synthesis of Planar Linkage Mechanisms","The idea of automatic mechanism synthesis is to find an optimal linkage type as well as its geometric dimensions for a given problem without using any pre-determined linkage type. As the first step towards the automatic mechanism synthesis, the authors proposed the use of unified planar linkage consisting of rigid blocks connected by stiffness-varying zero-length springs and formulated the synthesis problem as the iterative design optimization problem. In this investigation, we extend the automatic mechanism synthesis idea to more realistic large-sized problems by resolving several numerical difficulties observed in the earlier formulation such as instable convergence and many local optima. In particular, the objective and constraint functions in the optimization formulation are newly selected. The rationale for choosing such functions is given and the effectiveness of the proposed problem formulation is verified by designing planar mechanisms of complete paths.",2011
"Isogeometric Shape Design Sensitivity Analysis Using Mixed Transformation Method for Kronecker Delta Property","The isogeometric method is very effective in shape design optimization due to its effectiveness through the easy design parameterization and accurate sensitivities considering the higher order geometric terms. Due to non-interpolatory property of the NUBRS basis functions, however, the treatment of essential boundary condition is not as straightforward in the isogeometric analysis as in the finite element analysis. Taking advantages of the transformation method developed in meshfree methods, we investigate the isogeometric shape sensitivity analysis with the treatment of essential boundary conditions. Using the property that isogeometric basis functions do not depend on design changes, the transformed shape sensitivity equation is developed and verified for the problem having the essential boundary conditions. Numerical costs to construct the transformed basis function are not as much as the meshfree methods due to the NURBS property that only boundary nodes have their supports on the boundary. Through demonstrative numerical examples having the essential boundary conditions, the effectiveness of proposed design sensitivity analysis is verified.",2011
"Adjoint Design Sensitivity Analysis of Fracture Mechanics Using Molecular-Continuum Multiscale Approach","We have developed a multiscale design sensitivity analysis method for transient dynamics using a bridging scale method by a projection operator for scale decomposition. Employing a mass-weighted projection operator, we can fully decouple the equations of motion into fine and coarse scales using the orthogonal property of complimentary projector to the mass matrix. Thus, independent solvers in response analyses can be utilized for the fine scale analysis of molecular dynamic (MD) and the coarse scale analysis of finite element analysis. To reduce the size of problems and to improve the computational efficiency, a generalized Langevin equation is used for a localized MD analysis. Through demonstrative numerical examples, it turns out that the derived sensitivity analysis method is accurate and efficient compared with finite difference sensitivity.",2011
"Decomposition of System Level Reliability-Based Design Optimization to Reduce the Number of Simulations","It is computationally expensive to evaluate the overall system level reliability when several interacting failure modes are present. Therefore, it is even more expensive to optimize considering the system level reliability that accounts for the interactions between failure modes. In this paper, we decompose the system level reliability based optimization problem using surrogates into less expensive problems with fixed risk allocation for each failure mode. In addition, the fixed risk allocation problem is transformed from a purely probabilistic problem to a deterministic one through an iterative process of updating safety factors to limit the number of calls to evaluate the reliability. We found that the number of calls to the simulation to evaluate the system level reliability was reduced by 77% with this methodology.",2011
"Random Field Characterization With Insufficient Data Sets for Probability Analysis and Design","Random field is a generalization of a stochastic field, of which randomness can be characterized as a function of spatial variables. Examples of the random field can often be found as a geometry, material, and process variation in engineering products and processes. It has been widely acknowledged that consideration of the random field is quite significant to accurately predict variability in system performances. However, current approaches for characterizing the random field can only be applied to the situation with sufficient random field data sets and are not suitable to most engineering problems where the data sets are insufficient. The contribution of this paper is to model the random field based on the insufficient data sets such that sufficient data sets can be simulated or generated according to the random field modeling. Therefore, available random field characterization approaches and probability analysis methods can be used for probability analysis and design of many engineering problems with the lack of random field data sets. The proposed random field modeling is composed of two technical components including: 1) a Bayesian updating approach using the Markov Chain Monte Carlo (MCMC) method for modeling the random field based on available random field data sets; and 2) a Bayesian Copula dependence modeling approach for modeling statistical dependence of random field realizations at different measurement locations. Three examples including a mathematical problem, a heat generation problem of the Lithium-ion battery, and a refrigerator assembly problem are used to demonstrate the effectiveness of the proposed approach.",2011
"Conditions of Equivalence Between Continuum and Discrete Sensitivities","The purpose of this paper is to show equivalence between continuum and discrete formulations in sensitivity analysis when a linear velocity field is used. Shape sensitivity formulations are presented when the body forces and surface tractions depend on shape design variables. Especially, the continuum-discrete (C-D) and discrete-discrete (D-D) approaches are compared in detail. It is shown that the two methods are theoretically and numerically equivalent when the same discretization, numerical integration, and linear design velocity fields are used. The accuracy of sensitivity calculation is demonstrated using a cantilevered beam under uniform pressure and an arch dam crown cantilever under gravity and hydrostatic loading at the upstream face of the structure. It is shown that the sensitivity results are consistent with finite difference results, but different from the analytical sensitivity due to discretization and approximation errors of numerical analysis.",2011
"An Asymmetric Dimension-Adaptive Tensor-Product Method for Reliability Analysis","Reliability analysis plays an essential role in the development of structural systems. However, commonly used reliability analysis methods suffer from either the curse of dimensionality or the lack of accuracy in many structural problems. This paper presents an asymmetric dimension-adaptive tensor-product (ADATP) method to resolve the difficulties of existing reliability analysis methods. The proposed method leverages three ideas: (i) an asymmetric dimension-adaptive scheme to efficiently build the tensor-product interpolation considering both directional and dimensional importance, (ii) a hierarchical interpolation scheme using either piecewise multi-linear basis functions or cubic Lagrange splines, (iii) a hierarchical surplus as an error indicator to automatically detect the highly nonliner regions in a random space and adaptively refine the collocation points in these regions. The proposed method has three distinct features for reliability analysis: (a) automatically detecting and adaptively reproducing tri- and higher-variate interactions, (b) greatly alleviating the curse of dimensionality, and (c) no need of response sensitivities. Several mathematical and engineering problems involving high nonlinearity are used to demonstrate the effectiveness of the ADATP method.",2011
"Robust Design Optimization of Ball Grid Array Packaging","In this paper, comparisons of the design optimization of ball grid array packaging geometry based on the elastic and viscoelastic material properties are made. Six geometric dimensions of the packaging are chosen as input variables. Molding compound and substrate are modeled as elastic and viscoelastic, respectively. Viscoplastic finite element analyses are performed to calculate the strain energy densities (SED) of the eutectic solder balls. Robust design optimizations to minimize SED are carried out, which accounts for the variance of the parameters via Kriging dimension reduction method. Optimum solutions are compared with those by the Taguchi method. It is found that the effects of the packaging geometry on the solder ball reliability are significant, and the optimization results are different depending on the materials modeling.",2011
"Topology Optimization of Spacers for Maximizing Permeate Flux on Membrane Surface in Reverse Osmosis Channel","The objective of this study is to design spacers using fluid topology optimization in 2D crossflow Reverse Osmosis (RO) membrane channel to improve the performance of RO processes. This study is an initial attempt to apply topology optimization to designing spacers in RO membrane channel. The performance was evaluated by the quantity of permeate flux penetrating both upper and lower membrane surfaces. A coupled Navier-Stokes and Convection-Diffusion model was employed to calculate the permeate flux. To get reliable solutions, stabilization methods were employed with standard finite element method. The nine reference models which consist of the combination of circle, rectangular, triangle shape and zigzag, cavity, submerge configuration of spacers were simulated. Such models were compared with new model designed by topology optimization. The permeate flux at both membrane surfaces was determined as an objective function. In addition, permissible pressure drop along the channel and spacer volume were used as constraints. As a result of topology optimization as the permissible pressure drop changes in channel, characteristics of spacer design development was founded. Spacer design based on topology optimization was reconstructed to a simple one considering manufactuability and characteristics of development spacer design. When a simplified design was compared with previous 9 models, new design has a better performance in terms of permeate flux and wall concentration at membrane surface.",2011
"An Early-Stage Tool to Evaluate the Product Redesign Impact","In order to face the rapid changing market requirements, companies need methods and tools in order to implement flexibility over the whole product development process, from ideation to manufacturing. The proposed approach targets the development of a method to support decision making in product redesign activities. Design alternatives and product modifications can be rapidly evaluated in terms of feasibility, cost and time. The approach is based on a product structure multilevel representation, where functions, modules, assemblies and components are strictly interrelated. The representation allows criteria and rules in order to efficiently connect the elements within the same level and among levels. Such connections will contain the values to estimate the impact of analyzed product changes. In this way the structure will serve as evaluation tool in the early redesign phases. In order to manage and interact with the structure a software tool has been developed, called Modulor. This system allows modeling the product representation and rapidly evaluating the consequences in terms of change propagation. The tool was tested within the R&D department of a large sized company producing household appliances. Pilot studies have revealed shorter redesign cycles thanks to a broader understanding of implications while deciding among several implementation solutions.",2011
"A New Framework for Collaborative Set-Based Design: Application to the Design Problem of a Hollow Cylindrical Cantilever Beam","In the New Product Development processes, there are usually interacting multiple actors with multiple design objectives. Design objectives of the actors can be contradictorily related and design modifications related to some objectives may generate negative impacts on the other actors. Therefore conflicts may occur. We present a multi-actor design platform that is capable of preventing potential conflicts with ensuring the satisfaction and progress of the actors at a certain level. An indicator of wellbeing is developed in order to indentify the satisfaction and progress states of the actors. We use Set-based design approach and Constraint Satisfaction Problem solving techniques to deal with the multi-actor, multi-objective design problem. We apply our collaborative design approach to a multi-actor multiobjective engineering design problem of a hollow cylindrical cantilever beam with a load applied at the unsupported end.",2011
"Incorporating Process Architecture in the Evaluation of Stability in Distributed Design","In distributed design processes, individual design subsystems have local control over design variables and seek to satisfy their own individual objectives, which may also be influenced by some system level objectives. The resulting network of coupled subsystems will either converge to a stable equilibrium, or diverge in an unstable manner. In this paper, we study the dependence of system stability on the solution process architecture. The solution process architecture describes how the design subsystems are ordered and can be either sequential, parallel, or a hybrid that incorporates both parallel and sequential elements. In this paper we demonstrate that the stability of a distributed design system does indeed depend on the solution process architecture chosen and we create a general process architecture model based on linear systems theory. The model allows the stability of equilibrium solutions to be analyzed for distributed design systems by converting any process architecture into an equivalent parallel representation. Moreover, we show that this approach can accurately predict when the equilibrium is unstable and the system divergent when previous models suggest the system is convergent.",2011
"Probability of User Fit for Spatially Optimized Products","This study offers a new method for understanding the likelihood of acceptable fit for users of adjustable products and environments and is a useful tool for aiding the designer in making decisions about problems involving human variability. Accommodation, which describes the ability of a user to interact with a device or environment in a preferred way, is a key product performance metric. Methods that offer a better understanding of accommodation of broad user populations would allow for the design of products that are more cost-effective, safer, and/or lead to greater levels of customer satisfaction. This work uses parametric studies to explore the characteristics of a target user population and the probability of accommodating individuals of a given body size. Performance regions are identified in both the problem’s design space (the product dimensions under consideration) and the anthropometry space of the target population (the relevant body dimensions of product users). The existence of probability contours is a result of outcome uncertainty due to anthropometry-independent user preference, and the analysis is achieved by assessing binary accommodation of individuals using a “virtual fit” method with many iterations. Two case studies, one univariate and one bivariate in both performance and anthropometry spaces, are presented. An important outcome of the decision making framework described in this work is the ability to intuitively gauge who in the population of target users will be disaccommodated by a design and how to improve overall accommodation.",2011
"Granularity Enhancement of Extracted Preferential Probabilities From Design Team Discussion","Preferences are a formal way to represent a designer’s choices when assigning priorities for a set of possible design alternatives within the context of the design process. A design team’s preferences can change over the life of project, and knowledge of this evolution can be useful for understanding a team’s rationale as well as its confidence in a decision. This paper presents a “sliding window” approach (SPPT) to the extraction of preference related information from transcribed design team discussion. The approach suggested in this paper can assess design preferences over time with a finer granularity than a previous approach known as PPT, and removes perturbations that occur when there is little design team discussion. Both SPPT and PPT were applied to a discussion transcript. Results show good consistency among SPPT, PPT and survey results. SPPT is also able to detect more changes in design team preference.",2011
"Towards Parametric Environmental Profiles of Complex Industrial Systems in Preliminary Design Stage","The eco-design of complex industrial systems becomes a major issue for the concerned companies. Life Cycle Assessment (LCA) in particular is more adapted to “classical” products such as consumer goods. Performing LCA of such systems requires some thoughts to ensure significant results, for example concerning data granularity and quality. This article proposes a Life Cycle Assessment of an Alstom Grid AC/DC conversion substation for the aluminium industry. This LCA integrates first answers to the previous limitations. Very interesting results permit to orient the eco-design strategy of the company. Moreover they lead to imagine original ways to configure the system. Thus a first and simple parametric LCA model is proposed: four different electrical sources are used to analyze the sensitivity of the design factors to the environmental impacts of the substation. Based on design of experiments, a more advanced model of such a tool would permit to identify the best configuration in terms of environmental performance, costs and reliability.",2011
"Development of an Ontology for the Automatic Reconfiguration of a Vise-Type Fixture Device","The need for reconfigurable manufacturing systems has long been recognized as a key factor to gain the necessary flexibility for economically producing customized products. Automation of the reconfiguration processes is a challenge both on the hardware and the software level. Addressing this issue in the field of fixture design, a new reconfigurable fixture device for a CNC milling machine has been developed. The developed vise contains interchangeable jaws enabling the secure fixture of a wide variety of workpiece geometries. To enable automated reconfiguration, a reasoning system is needed that can determine feasible fixture configurations based on the given workpiece and part as well as the available fixture components. In this paper, an ontology for representing fixture design and reconfiguration knowledge for a specific reconfigurable vise-type fixture is introduced. The creation of the ontology is based on a systematic building methodology to identify necessary concepts, attributes and relations within the domain. Using Description Logic as the representation formalism, core concept definitions and relations are formalized to evaluate the correctness and consistency of the ontology. The ontology is validated both on the informal and on the formal level by its ability to find feasible fixture configurations, i.e. appropriate jaw pairs to fix example workpieces. The paper concludes with a discussion of the results and future work.",2011
"A Function Based Approach for Product Integration","Reconfigurable and multifunctional products are breeds of products that cater to the increased diversification of customer needs. Unlike single-state static products which can perform only one primary function, these products cater to different customer needs by performing more than one function with or without changing their configuration. However, there is a lack of systematic methods to support the conceptual task of combining two existing single-state products into an integrated product that provides multiple functions. In this paper, a function based approach is proposed which provides more rigorous support to assess the feasibility of integrating two products. The function structures of the existing products are combined to obtain the overall function structure of the reconfigurable product. Function sharing, based on quantified functional similarity, is proposed and applied to identify functions that can be shared by the same component. The information obtained from the function structure is then mapped to the components of two existing products to analyze their roles in the final reconfigurable product architecture. A case study illustrates the proposed approach by analyzing the integration of a power drill and a dust buster.",2011
"A Modular Design Approach to Improve the Life Cycle Performance Derived From Optimized Closed-Loop Supply Chain","Growing concerns for the environment should make every designer more carefully consider product design for the life cycle (DFLC). Although modularity is recognized for its potential to incorporate life cycle considerations into product architecture design, most modular design methods in the literature concentrate on generating highly-modular product architectures but lack the capability for assessing life cycle consequences of these modules in a supply chain. This paper proposes a methodology to find a robust modular architecture with minimal life cycle costs and environmental impacts at the design configuration stage. The objective of the proposed methodology is not to maximize modularity, but to adopt life cycle costing and life cycle assessment of a product in a closed-loop supply chain to identify the most beneficial modular structure. Further, capacity influence of the existing processing facilities in the supply chain on life cycle costs and environmental impacts is evaluated and discussed in this paper.",2011
"Multi-Discipline Design of a Wind Turbine","In this paper a Multi-Level System design (MLS) algorithm is presented and utilized for a wind turbine system analysis. The MLS guides the decision making process for designing a complex system where many alternatives and many mutually competing objectives and disciplines need to be considered and evaluated. Mathematical relationships between the design variables and the multiple discipline performance objectives are developed adaptively as the various design considerations are evaluated and as the design is being evolved. These relationships are employed for rewarding performance improvement during the decision making process by allocating more resources and influence to the disciplines which exhibit the improvement. Simulation tools developed by the National Renewable Energy Laboratory (NREL) are employed in the wind turbine design analysis. The Cost Of Energy (COE) comprises the overall system level objective, while performance improvements at two technical design disciplines are pursued at the same time. The optimal design of the blade geometry for maximum Annual Energy Production (AEP), and the structural design of the blade for minimum bending moment at the root of the blade comprise the two technical design disciplines. Scalar metamodels are developed for linking the design variables with the performance metrics associated with the design of the blade geometry. Main characteristics of the wind turbine, namely, the rotor diameter, the rotational speed, the maximum rated power, the hub height, the structural characteristics of the blade, and the geometric characteristics of the blade (distribution of thickness, twist angle, and chord) are employed as design variables for the overall design analysis. The optimization results and the physical insight which can be gained through a sensitivity analysis for the optimal configuration are presented and discussed.",2011
"A New Model for Wind Farm Layout Optimization With Landowner Decisions","Current wind farm layout optimization research focuses on advancing optimization methods. The research includes the assumption that a continuous piece of land is readily available. In reality, wind farm development projects rely on the permission of landowners for success. When a viable wind farm site location is identified, local residents are approached for permission to build turbines on their land, typically in exchange for monetary compensation. Landowners play a crucial role on the development of a wind farm and some land parcels are more important to the success of the project than others. In order to advance the research on wind farm optimization, this paper relaxes the assumption that a continuous piece of land is available, developing a novel approach that includes landowners’ decisions on whether or not to participate in the project. The optimization results of this new approach show that, for a specific wind farm layout case, we can identify the most crucial landowners and the optimal positions of turbines prior to the negotiation process with landowners. Using this approach, a site developer can spend more resources on persuading these most-important landowners to take part in the project, or approach them in a personalized manner. This will ultimately increase the efficiency of wind farm projects, saving time and money in the development stages.",2011
"Optimal Hybridization of Battery, Engine and Motor for PHEV20","A plug-in hybrid electric vehicle (PHEV) relies on relatively larger storage batteries than conventional hybrid electric vehicles. The characteristics of PHEV batteries, as well as hybridization of the PHEV battery with the engine and electric motor, play an important role in the design and potential adoption of PHEVs. To exhaustively evaluate all the possible combinations of available types of batteries, motors and engines, the total computational time is prohibitive. This work proposed an integrated optimal design strategy to address this problem. The recently developed Pareto set pursuing (PSP) multi-objective optimization approach is employed to perform optimal hybridization. Each PHEV with chosen battery, motor and engine is designed for optimal component sizing using the Powertrain System Analysis Toolkit (PSAT) software. The methodology is demonstrated with the Toyota Prius PHEV20: PHEV version sized for 20 miles (32.1 km) of all electric range (AER). Fuel economy, operating cost, and green house gases emissions are simultaneously optimized from 4,480 possible combinations of design parameters: 20 batteries, 14 motors, and 16 engines. The hybridization optimization is performed on two different drive cycles—Urban dynamometer driving schedule (UDDS) and Winnipeg weekday duty cycle (WWDC). It was found that battery, motor, and engine work collectively to define an optimal hybridization scheme and the optimal hybridization scheme varies with each driving cycle. The proposed method and software platform could be applied to optimize other powertrain designs.",2011
"Developing Innovative Energy Harvesting Approaches for Infrastructure Health Monitoring Systems","Many factors must be addressed when designing infrastructure health monitoring systems. Structures in remote locations or with limited accessibility make the requirements for these systems unique and challenging. For locations where connection to the power grid is difficult or impossible, monitoring system life is severely limited by battery technology. Alternatively, an energy harvesting power supply can make the monitoring system independent of the grid while increasing capabilities and lifetime beyond what is possible with current battery technology. This paper discusses a design and development methodology for developing energy harvesting aspects of a health monitoring system. The system comprises a sensor module that monitors the health of the structure, an on-site processing module that analyzes the data, and a wireless communication module that transmits the data. The method is demonstrated by examples of energy harvesting systems for a bridge monitoring application, using solar, wind, and vibration energy harvesters to provide power to a wireless network, local data processors, and strain gauges. Theoretical feasibility of energy harvesting in these domains has been previously demonstrated. The examples described in this paper validate the feasibility previously calculated as well as illustrate shortcomings in the current technology that inhibit potential implementation. The examples also show areas where innovation is needed to continue to advance the technology of energy harvesting in this application on infrastructure.",2011
"Optimal Component Sizing and Forward-Looking Dispatch of an Electrical Microgrid for Energy Storage Planning","Optimal design of an electrical microgrid and sizing of its components seeks to balance capital investment with expected operational cost while meeting performance requirements. Calculating operational cost requires scheduling each microgrid component over some time period (dispatching) for each design evaluated. Heuristic or rule-based dispatch strategies typically consider only single time instances and are computationally efficient but do not include scheduling energy storage for future time periods. In this paper, we propose to optimize microgrid designs using forward-looking optimal dispatch for future energy storage planning. We present a case study of an ‘islanded’ military base microgrid with renewable and non-renewable electricity generation, battery storage, and plug-in vehicles with electrical export power capability. The optimal design and forward-looking dispatch strategy are compared to results obtained using the publicly available rule-based dispatch strategy in HOMER Energy software. Results show that the forward-looking strategy uses storage batteries to plan for future energy shortfalls rather than simply as a buffer for variable renewable energy supply, resulting in a 7.8% reduction in predicted fuel use. For the given cost assumptions, sensitivity analysis of the optimal design with respect to fuel price shows that investment in renewable energy technology is justified at prices greater than $5 per gallon ($1.32/liter) with an attendant reduction in fuel use of 3–30%.",2011
"Optimal Scheduling of Parabolic Heliostats Aim Targets in a Mini-Tower Solar Concentrator System","Solar tower with heliostat mirrors is an established technology for utility-scale solar energy harvesting. The setup has several advantages such as the capability to reach high temperature, modularity and ease of maintenance of the heliostats, containment of the high temperature zone, as well as overall low cost per harvested energy. Downscaling to medium and small scale applications is a desirable goal in order to attract more users of the technology. However, the downscaling often does not turn out economically feasible while using flat mirror heliostats, which are the norm in utility-scale systems. This is mainly due to the need to preserve the ",2011
"Optimization of Parabolic Heliostat Focal Lengths in a Mini-Tower Solar Concentrator System","Solar tower with heliostat mirrors is one of the established setups for utility-scale solar energy harvesting. Advantages of the setup include the capability to reach high temperature, modularity and ease of maintenance for the heliostats, containment of the high temperature zone atop the tower, as well as overall low cost per unit energy. However, downscaling to medium or small scale applications often does not turn out economically feasible with flat mirror heliostats that are the norm in utility-scale systems. This is mainly due to the need to preserve the solar concentration ratio, which in turn means the ",2011
"Characterizing the Influence of Land Configuration on the Optimal Wind Farm Performance","The development of large scale wind farms that can produce energy at a cost comparable to that of other conventional energy resources presents significant challenges to today’s wind energy industry. The consideration of the key design and environmental factors that influence the performance of a wind farm is a crucial part of the solution to this challenge. In this paper, we develop a methodology to account for the configuration of the farm land (length-to-breadth ratio and North-South-East-West orientation) within the scope of wind farm optimization. This approach appropriately captures the correlation between the (i) land configuration, (ii) the farm layout, and (iii) the selection of turbines-types. Simultaneous optimization of the farm layout and turbine selection is performed to minimize the Cost of Energy (COE), for a set of sample land configurations. The optimized COE and farm efficiency are then represented as functions of the land aspect ratio and the land orientation. To this end, we apply a recently developed response surface method known as the Reliability-Based Hybrid Functions. The overall wind farm design methodology is applied to design a 25MW farm in North Dakota. This case study provides helpful insights into the influence of the land configuration on the optimum farm performance that can be obtained for a particular site.",2011
"Customer Driven Optimal Design for Convergence Products","Convergence products are multifunctional designs which are changing the way consumers use existing functionalities. Manufacturers’ ventures in developing convergence products abound in the marketplace. Smartphones, tablet computers, internet TV, are just a few examples. The complexity of designing a convergence product can differ significantly from that of single function products which most research in “Design for Market Systems” aims at. In this paper, a new customer-driven approach for designing convergence products is proposed to address the following issues: (i) a design representation scheme that considers information from design solutions used in existing products. The representation facilitates the coupling of and combining multiple functionalities; (ii) a hierarchical Bayes model that evaluates consumers’ heterogeneous choices while revealing how usage of multiple functionalities impacts consumers’ preferences; and (iii) design metrics which help evaluate profitability of design alternatives and account for future market penetration given evolving consumer preferences. An example problem for designing a tablet computer is used to demonstrate the proposed approach. The data for the example is collected by conducting a choice-based conjoint survey which yielded 92 responses. The proposed approach is demonstrated with three scenarios differentiated by the consideration of consumer heterogeneity and future market penetration, while comparing how the resulting optimal design solutions for the convergence product differ.",2011
"Defining Technology-Adoption Indifference Curves for Residential Solar Electricity Generation Using Stated Preference Experiments","Success in achieving environmental goals is intrinsically dependent on policy decisions, firm decisions, and consumer decisions. Understanding how consumer product adoption jointly depends on policy incentives and firm design decisions is necessary for both firms and governments to make optimal decisions. This paper demonstrates a methodology for assessing the linkage between policy incentives and firm decisions on the level of consumer adoption of a particular technology. A policy optimization is formulated and technology-adoption indifference curves are constructed to allow firms to identify the most profitable direction for product development given the policy environment, and similarly to allow government organizations to set policies that maximize technology adoption given firm decisions. As an example we use the residential solar electricity industry in New South Wales, Australia. Consumer choice is modeled using a mixed logit choice model estimated with hierarchical Bayes techniques from stated preference experiment data.",2011
"Customer-Driven Product Design Selection Using Web Based User-Generated Content","Acquisition of the customer data for product design selection using conventional customer survey techniques can be a time-consuming and costly undertaking. The aim of this paper is to overcome this limitation by using web based User-Generated Content (UGC) as an alternative to the conventional customer survey techniques. UGC refers to various public media contents created by web users including contents in online customer reviews, blogs, and social networking interactions. So far, there has not been any systematic effort in using UGC in design selection for a customer durable product. Using UGC in product design selection is not an easy task because UGC can be freely expressed and written by customers with little constraints, structure and bounds. As a result, UGC can contain a lot of noise, variability in content and even bias induced by the customers. In order to make use of UGC, this paper develops a systematic methodology for eliciting product attributes from UGC, constructing customer preference models and using these models in design selection. To demonstrate the proposed method, design selection of a smartphone using UGC is considered as an example. It is shown in the example that the proposed method can provide a reasonable estimation of customer preferences while being useful for product design selection.",2011
"Robust Design for Profit Maximization Under Uncertainty of Consumer Choice Model Parameters Using the Delta Method","In new product design, risk averse firms must consider downside risk in addition to expected profitability, since some designs are associated with greater market uncertainty than others. We propose an approach to robust optimal product design for profit maximization by introducing an α-profit metric to manage expected profitability vs. downside risk due to uncertainty in market share predictions. Our goal is to maximize profit at a firm-specified level of risk tolerance. Specifically, we find the design that maximizes the α-profit: the value that the firm has a (1−α) chance of exceeding, given the distribution of possible outcomes. The parameter α∊[0,1] is set by the firm to reflect sensitivity to downside risk (or upside gain), and parametric study of α reveals the sensitivity of optimal design choices to firm risk preference. We account here only for uncertainty of choice model parameter estimates due to finite data sampling when the choice model is assumed to be correctly specified (no misspecification error). We apply the delta method to estimate the mapping from uncertainty in discrete choice model parameters to uncertainty of profit outcomes and identify the estimated α-profit as a closed form function of design decision variables. This process is described for the multinomial logit model, and a case study demonstrates implementation of the method to find the optimal design characteristics of a midsize consumer automobile.",2011
"A Method for Designing Collaborative Products With Application to Poverty Alleviation","Collaborative products are created when physical components from two or more products are temporarily recombined to form another product capable of performing additional tasks. In this paper, a method for designing collaborative products is introduced. The method identifies a set of products capable of being recombined into a collaborative product. These products are then designed to allow for this recombination. Collaborative products are particularly useful in reducing the cost, weight, and size of poverty-alleviating products—reductions that are valued in the developing world. A simple example of a cabinet maker’s tool shows that a collaborative block plane created from a chisel and sanding block can account for reductions in cost, weight, and size of 44%, 38%, and 44% respectively, when compared to a typical wooden block plane, chisel, and sanding block. Additionally, an example of a collaborative apple peeler is provided to demonstrate scalability of the method. The authors conclude that the method introduced herein provides a new and useful tool to design collaborative products and to assist in engineering-based poverty alleviation.",2011
"Principles of Mechanical Design for the Developing World: A Case Study Approach","Designing appropriate technology is becoming more prevalent as engineers have begun to focus more of their attention on the developing world. However, many efforts have failed or been relatively unsuccessful due to design processes that do not focus on sustainability. One author has provided a solid framework by outlining mechanical design basic principles including design for simplicity, analysis of load paths, and use of prototypes. Yet these principles were not presented in a way that makes them applicable to sustainable projects in the developing world. In this paper, these three principles are investigated through two design case studies. The goal was to analyze how well these principles apply to the developing world and whether several hypothesized changes would also be useful. With the principles in mind, a bicycle trailer and taxi were designed for an impoverished community in rural Africa. Based on these designs, it was determined that the three principles analyzed apply to the developing world but should be refocused and presented differently in order to be utilized effectively for sustainability.",2011
"A Method for Identifying Design Principles for the Developing World","Growing awareness of the unique needs and challenges in the developing world has resulted in the development of products for those in poverty. Successful product design focuses development efforts on design principles that are important to a target market. Consequently, the better these principles are understood, the higher the probability is that resulting products will be successful. Recognizing that the identification of these principles is a major challenge, this paper presents a method for identifying them for any target market, but especially for the developing world. The presented methodology uses characteristics of products within the target market to extract information about the underlying design decisions resulting in these characteristics. This information is then used to identify the design principles. To verify the ability of the method to identify these principles, the method is applied to best selling products in the US and then applied to products created for the developing world. The resulting principles from the two markets are then analyzed and compared to highlight the similarities and differences between the identified principles. The authors conclude that the resulting list of principles will enable designers to better design and develop products for the developing world.",2011
"Understanding Rural Village Energy Needs and Design Constraints","Today the primary challenge confronting engineers is to develop clean, sustainable technologies that can meet the needs of all of the world’s people. Traditionally this effort has focused on meeting the needs of the developed world. It is generally assumed that products needed for the developing world already exist or are relatively simple and hence do not require significant engineering design effort. As a consequence, many of the products intended to meet the needs of the poor miss the mark and do not meet their needs. This is particularly true in the design of products and processes intended to address the energy needs of the rural poor. Too often a set of standard assumptions is used, resulting in poor problem definition. And, because the design problem is not well defined, the resulting products and processes fail. Throughout the developing world it is common to find village water and energy projects that have failed. To design products and processes that meet the energy needs of the rural poor, the critical first step in the design process is a detailed in-village study of energy production and consumption dynamics. Quantifying village energy dynamics provides insight into the unfulfilled or unsatisfied needs of the consumer, establishes the design constraints, aids the engineer and the community members in prioritizing needs, and builds trust with the local community. This paper presents a field methodology developed to understand the energy needs of a rural sub-Saharan village of 700 people and discusses how this field methodology was used to establish the design constraints needed for a comprehensive energy solution.",2011
"Managing Uncertainty in Multiscale Systems via Simulation Model Refinement","The motivating question for this article is: ‘How should a system level designer allocate resources for auxiliary simulation model refinement while satisfying system level design objectives and ensuring robust process requirements in multiscale systems? Our approach consists of integrating: (i) a robust design method for multiscale systems (ii) an information economics based approach for quantifying the cost-benefit trade-off for mitigating uncertainty in simulation models. Specifically, the focus is on allocating resources for reducing model parameter uncertainty arising due to insufficient data from simulation models. A comprehensive multiscale design problem, the concurrent design of material and product is used for validation. The multiscale system is simulated with models at multiple length and time scales. The accuracy of the simulated performance is determined by the trade-off between computational cost for model refinement and the benefits of mitigated uncertainty from the refined models. System level designers can efficiently allocate resources for sequential simulation model refinement in multiscale systems using this approach.",2011
"Loci Surface Guided Crystal Phase Transition Pathway Search","Recently a periodic surface model was developed to assist geometric construction in computer-aided nano-design. This implicit surface model helps create super-porous nano structures parametrically and support crystal packing. In this paper, we propose a new approach for pathway search in phase transition simulation of crystal structures. The approach relies on the interpolation of periodic loci surface models. Respective periodic plane models are reconstructed from the positions of individual atoms at the initial and final states, and surface correspondence are found. With geometric constraints imposed based on physical and chemical properties of crystals, two surface interpolation methods are used to approximate the intermediate atom positions on the transition pathway in the full search of the minimum energy path. This hybrid approach integrates geometry information in configuration space and physics information to allow for efficient transition pathway search. The methods are demonstrated by examples of FeTi and VO2 .",2011
"Microstructure Reconstruction for Stochastic Multiscale Material Design","There are two critical components of connecting material and structural design in a multiscale design process: (1) relate material processing parameters to the microstructure that arises after mixing, and (2) stochastically characterize and subsequently reconstruct the microstructure to enable automation of material design that scales upward to the structural domain. This work proposes a data-driven framework to address both above components for two-phase materials and presents the algorithmic backbone to such a framework. In line with the two components above, a set of numerical algorithms is presented for characterization and reconstruction of two-phase materials from microscopic images: these include grayscale image binarization, point-correlation and cluster-correlation characterization, and simulated annealing algorithm for microstructure reconstruction. Another set of algorithms is proposed to connect the material processing parameters with the resulting microstructure by mapping nonlinear, nonphysical regression parameters in microstructure correlation functions to a physically based, simple regression model of key material characteristic parameters. This methodology, that relates material design variables to material structure, is crucial for stochastic multiscale design.",2011
"Identifying Product Scaling Principles: A Tool for Bioinspired Design and Beyond","There are countless products that perform the same function but are engineered to suit a different scale. Designers are often faced with the problem of taking a solution at one scale and mapping it to another. This frequently happens with design-by-analogy and bioinspired design. Despite various scaling laws for specific systems, there are no global principles for scaling systems, for example from a biological nano-scale to macro-scale. This is likely one of the reasons bioinspired design is difficult. Very often scaling laws assume the same physical principles are being used, but this study of products indicates that a variety of changes occur as scale changes, including changing the physical principles to meet a particular function. Empirical product research was used to determine a set of principles by observing and understanding numerous products to unearth new generalizations. The function a product performs is examined in various scales to view subtle and blatant differences. Principles are then determined. This study provides an initial step in creating new innovative designs based on existing solutions in nature or other products that occur at very different scales. Much further work is needed by studying additional products and bioinspired examples.",2011
"Study of the Sequential Constraint-Handling Technique for Evolutionary Optimization With Application to Structural Problems","Engineering design problems are most frequently characterized by constraints that make them hard to solve and time-consuming. When evolutionary algorithms are used to solve these problems, constraints are often handled with the generic weighted sum method or with techniques specific to the problem at hand. Most commonly, all constraints are evaluated at each generation, and it is also necessary to fine-tune different parameters in order to receive good results, which requires in-depth knowledge of the algorithm. The sequential constraint-handling techniques seem to be a promising alternative, because they do not require all constraints to be evaluated at each iteration and they are easy to implement. They nevertheless require the user to determine the ordering in which those constraints shall be evaluated. Therefore two heuristics that allow finding a satisfying constraint sequence have been developed. Two sequential constraint-handling techniques using the heuristics have been tested against the weighted sum technique with the ten-bar structure benchmark. They both performed better than the weighted sum technique and can therefore be easy to implement, and powerful alternatives for solving engineering design problems.",2011
"Comparison of Gene Expression Programming and Common Metamodeling Techniques in Engineering Design","To reduce the tremendous computational expense of implementing complex simulation and analysis in engineering design, more and more researchers pay attention to the construction of approximation models. The approximation models, also called surrogate models and metamodels, can be utilized to replace simulation and analysis codes for design and optimization. Commonly used metamodeling techniques include response surface methodology (RSM), kriging and radial basis functions (RBF). In this paper, gene expression programming (GEP) algorithm in evolutionary computing is investigated as an alternative technique for approximation. The performance of GEP is examined by its innovative applications to the approximation of mathematical functions and engineering analyses. Compared to RSM, kriging and RBF, GEP is demonstrated to be more accurate for the small sample size. For large sample sets, GEP also shows good approximation accuracy. Additionally, GEP has the best transparency since it can provide explicit and compact function relationships and clear factor contributions. Overall, as a novel metamodeling technique, GEP exhibits great capabilities to provide the accurate approximation of a design space and will have wide applications in engineering design, especially when only a few sample points are selected for approximation.",2011
"Methodology for Pipeline Route Selection Using the NSGA II and Distance Transform Algorithms","The objective of this study is to develop a methodology for use in geothermal pipeline route selection. Special emphasis is placed on finding the shortest route and minimizing the visual affects of the pipeline. Two different approaches are taken to solving the problem. In the first method a distance transform algorithm is used both for visual effects ranking and to obtain the optimal path. Subsequently a genetic algorithm is used to modify the route with regards to necessary expansion units. Included in the tool is site selection for separators and pipeline gathering points based on visual effects, incline, inaccessible areas and total distance to boreholes. The second method uses the Non-dominated sorting genetic algorithm II (NSGA II) to obtain the optimal path with regards visual effects, route length and pipeline gradient. This method uses the distance transform ranking method along with constraints on route length to generate the initial population for the genetic algorithm. The methods are implemented for the Hverahlið geothermal area.",2011
"Interactive Modular Optimization Strategy for Layout Problems","Layout design optimization has a significant impact in the design and use of many engineering products and systems. Real-world layout problems are usually considered as complex problems because of the geometry of components, the problem density and the great number of designer’s requirements. Solving these optimization problems is a hard and time consuming task. This paper proposes an interactive modular optimization strategy which allows the designer to find optimal solutions in a short period of calculation time. This generic strategy is based on a genetic algorithm, combined with specific optimization modules. These modules improve the global performances of the algorithm. This approach is adapted to multi-objective optimization problems and interactivity between the designer and the optimization process is used to make a final choice among design alternatives. This optimization strategy is tested on a real-world application which deals with the search of an optimal spatial arrangement of a shelter.",2011
"Assessing the Effectiveness of Using Graveyard Data for Generating Design Alternatives","Modeling to Generate Alternatives (MGA) is a technique used to identify variant designs that maximize design space distance from an initial point while satisfying performance loss constraints. Recent work has explored the application of this technique to nonlinear design problems, where the design space was investigated using an exhaustive sampling procedure. While computational cost concerns were noted, the main focus was determining how scaling and distance metric selection influenced alternative discovery. To increase the viability of MGA for engineering design problems, this work looks to reduce the computational overhead needed to identify design alternatives. This paper investigates and quantifies the effectiveness of using previously sampled designs, i.e. a graveyard, from a multiobjective genetic algorithm as a means of reducing computational expense. Computational savings and the expected error are quantified to assess the effectiveness of this approach. These results are compared to other more common “search” techniques; namely Latin hypercube samplings, grid search, and the Nelder-Mead simplex method. The performance of these “search” techniques are subsequently explored in two case study problems — the design of a two bar truss, and an I-beam — to find the most unique alternative design over a range of different thresholds. Results from this work show the graveyard can be used as a way of inexpensively generating alternatives that are close to ideal, especially nearer to the starting design. Additionally, this paper demonstrates that graveyard information can be used to increase the performance of the Nelder-Mead simplex method when searching for alternative designs.",2011
"Global Product Family Design: A Mathematical Model for Simultaneous Decision of Module Commonalization and Supply Chain Configuration","Today’s manufacturing has become global at all aspects of marketing, design, production, distribution, etc. While product family design has been an essential viewpoint for meeting with the demand for product variety, its meaning is becoming more broad and complicated with linking product design with issues on market systems, supply chain, etc. This paper calls such a design situation ‘global product family design,’ and firstly characterizes its components and complexity. Following them, this paper develops a mathematical model for the simultaneous decision problem of module commonalization strategies under the given product architecture and supply chain configuration through selection of manufacturing sites for module production, assembly and final distribution as an instance of the problems. This paper demonstrates some numerical case studies for ascertaining the validity and promise of the developed mathematical model with an optimization method configured with a genetic algorithm and a simplex method. Finally, it concludes with some discussion on future works.",2011
"Design Preference Elicitation Using Efficient Global Optimization","We seek to elicit individual design preferences through user-computer interaction. During an iteration of the interactive session, the computer presents a set of designs to the user who then picks any preferred designs from the set. The computer learns from this feedback and creates the next set of designs using its accumulated knowledge to minimize a merit function. Under the hypothesis that user responses are deterministic, we show that an effective query scheme is akin to the Efficient Global Optimization (EGO) algorithm. Using simulated interactions, we discuss how the merit function form and user preference sensitivity can affect search efficiency and hence the time to complete an interactive session. We demonstrate the proposed algorithm in the design of vehicle exteriors.",2011
"Learning Stylistic Desires and Generating Preferred Designs of Consumers Using Neural Networks and Genetic Algorithms","Consumers have different ideas of what makes a design stylish. Some consumers may want a sporty looking car, while others may want a rugged looking or a fuel-efficient looking car. Can computers learn what it means to satisfy those style-based goals and use this knowledge to generate designs that target style-based goals in design? An experiment was conducted where participants were asked to rate computer generated car profiles for sportiness, ruggedness, beauty, and fuel efficiency. This survey data is used as an indicator of consumer stylistic form preferences, and was used to train Artificial Neural Networks (ANN) for each of the four rating categories. The resulting ANNs were then inverted using a Genetic Algorithm (GA) in order to generate new designs that elicit targeted style goals from consumers.",2011
"Exploring Marketing to Engineering Information Mapping in Mass Customization: A Presentation of Ideas, Challenges and Resulting Questions","The paradigm of mass customization strives to minimize the tradeoffs between an ‘ideal’ product and products that are currently available. However, the lack of information relation mechanisms that connect the domains of marketing, engineering, and distribution have caused significant challenges when designing products for mass customization. For example, the bridge connecting the marketing and engineering domains is complicated by the lack of proven tools and methodologies that allow customer needs and preferences to be understood at a level discrete enough to support true mass customization. Discrete choice models have recently gained significant attention in engineering design literature as a way of expressing customer preferences. This paper explores how information from choice-based conjoint surveys might be used to assist the development of a mass customizable MP3 player, starting from 140 student surveys. The authors investigate the challenges of fielding discrete choice surveys for the purpose of mass customization, and explore how hierarchical Bayes mixed logit and latent class multinomial logit models might be used to understand the market for customizable attributes. The potential of using discrete choice models as a foundation for mathematically formulating mass customization problems is evaluated through an investigation of strengths and limitations.",2011
"Prospect of Design for Mass Customization and Personalization","The prevailing practice of design for mass customization manifests itself through a configure-to-order paradigm, which means to satisfy explicit customer needs and built upon legacy design. With pervasive connectivity and interactivity of the Internet and sensor networks, personalization has been witnessed in a number of industry sectors as a promising strategy that makes the market of one a reality. This positioning paper envisions an extension to design for mass customization and personalization (DFMCP). By exploiting implicit market demand information and revealing latent customer needs, DFMCP aspires to assist customers in making better informed decisions, and to the largest extent, to anticipate customer satisfaction and adapt to customer delight. Based on a multiverse of user experience, product differentiation and co-creation, the key issues of DFMCP are discussed.",2011
"Structural Topology Optimization for Forced Vibration Problem Using Level Set Method","For the purpose of structure vibration reduction, a structural topology optimization for forced vibration problem is proposed based on the level set method. The objective of present study is to minimize the frequency response at the specified points or surfaces on the structure with an excitation frequency or a frequency range, subject to the given amount of the material over the admissible design domain. The sensitivity analysis with respect to the structural boundaries is carried out, while the X-FEM is employed for solving the state equation and the adjoint equation. The optimal structure with smooth boundaries is obtained by the level set evolution with advection velocity, derived from the sensitivity analysis and the optimization algorithm. A number of two-dimensional numerical examples are presented to demonstrate the feasibility and effectiveness of the proposed approach.",2011
"Modeling of Geometric Variations for Line-Profiles","The geometric variations in a tolerance-zone can be modeled with hypothetical point-spaces called Tolerance-Maps (T-Maps) for purposes of automating the assignment of tolerances during design. The objective of this paper is to extend this model to represent tolerances on line-profiles. Such tolerances limit geometric manufacturing variations to a specified two-dimensional tolerance-zone, i.e. an area, the boundaries to which are curves parallel to the true profile. The single profile tolerance may be used to control position, orientation, and form of the profile. In this paper, the Tolerance-Map (Patent No. 6963824) is a hypothetical volume of points that captures all the positions for the true profile, and those curves parallel to it, which can reside in the tolerance-zone. The model is compatible with the ASME/ANSI/ISO Standards for geometric tolerances. T-Maps have been generated for other classes of geometric tolerances in which the variation of the feature are represented with a plane, line or circle, and these have been incorporated into testbed software for aiding designers when assigning tolerances for assemblies. In this paper the T-Map for line-profiles is created and, for the first time in this model, features may be either symmetrical or non-symmetrical simple planar curves, typically closed. To economize on length of the paper, and yet to introduce a method whereby T-Maps may be ",2011
"An Approach to Automated Conversion From Design Feature Model to Analysis Feature Model","An approach to automatically converting a design feature model to an analysis feature model for downstream finite element analysis is proposed. The analysis feature model is a mixed-dimensional model with shell features representing thin regions and solid features representing thick regions. In the approach, the design feature model is first decomposed into a set of remnants of additive features, each of which represents part of an additive feature’s volume that remain in the final volume of the design model. The remnant of each additive feature is then decomposed into swept bodies and non-swept bodies. After that, the thin regions of each swept body are effectively recognized based on its sketch information. These detected thin regions can be wrongly recognized, which, together with potentially missing ones are thus detected and corrected by a synthesization process. Finally, the analysis features and their relative interfaces are generated, which ultimately gives the corresponding analysis feature model to the input design feature model. Experimental results are also shown to demonstrate the proposed method’s effectiveness.",2011
"Generic Visual Simulation of Manufacturing Equipment","Simulation of a machine is very important before laser metal deposition is performed, as a tool to check collision detection and validate deposition result. There are several kinds of machines that are used for laser deposition and hence there is a need for a generalized concept for visual simulation of various kinds of machines. This paper presents the research conducted on describing each machine configuration in a generic format. A parent–child list and a dependency list obtained from the machine configuration are utilized to form the generic format. Such a format can be used to describe linear and rotational motion of the machines parts. This method has been tested on various examples to demonstrate its robustness and efficiency.",2011
"Conceptual Design of Freeform Surfaces From Unstructured Point Sets Using Neural Network Regression","This paper presents a new point set surfacing method that employs neural networks for regression. Our technique takes as input unstructured and possibly noisy point sets representing two-manifolds in R 3  . To facilitate parametrization, the set is first embedded in R 2   using neighborhood preserving locally linear embedding. A neural network is then constructed and trained that learns a mapping between the embedded 2D parametric coordinates and the corresponding 3D space coordinates. The trained network is then used to generate a tessellation that spans the parametric space, thereby producing a surface in the original space. This approach enables the surfacing of noisy and non-uniformly distributed point sets, and can be applied to open or closed surfaces. We show the utility of the proposed method on a number of test models, as well as its application to freeform surface creation in virtual reality environments.",2011
"Shape and Topology Optimization With Medial Zones","Shape optimization (with topological changes) has become a ",2011
"Evaluating Genetic Algorithms on Welding Sequence Optimization With Respect to Dimensional Variation and Cycle Time","Spot welding is the predominant joining method in car body assembly. Spot welding sequences have a significant influence on the dimensional variation of resulting assemblies and ultimately on overall product quality. It also has a significant influence on welding robot cycle time and thus ultimately on manufacturing cost. In this work we evaluate the performance of Genetic Algorithms, GAs, on multi-criteria optimization of welding sequence with respect to dimensional assembly variation and welding robot cycle time. Reference assemblies are fully modelled in 3D including detailed fixtures, welding robots and weld guns. Dimensional variation is obtained using variation simulation and part measurement data. Cycle time is obtained using automatic robot path planning. GAs are not guaranteed to find the global optimum. Besides exhaustive calculations, there is no way to determine how close to the actual optimum a GA trial has reached. Furthermore, sequence fitness evaluations constitute the absolute majority of optimization computation running time and do thus need to be kept to a minimum. Therefore, for two industrial reference assemblies we investigate the number of fitness evaluations that is required to find a sequence that is optimal or a near-optimal with respect to the fitness function. The fitness function in this work is a single criterion based on a weighted and normalized combination of dimensional variation and cycle time. Both reference assemblies involves 7 spot welds which entails 7!=5040 possible welding sequences. For both reference assemblies, dimensional variation and cycle time is exhaustively calculated for all 5040 possible sequences, determining the optimal sequence, with respect to the fitness function, for a fact. Then a GA that utilizes Random Key Encoding is applied on both cases and the performance is recorded. It is found that in searching through about 1% of the possible sequences, optimum is reached in about half of the trials and 80–90% of the trials reach the ten best sequences. Furthermore the optimum of the single criterion fitness function entails dimensional variation and cycle time fairly close to their respective optimum. In conclusion, this work indicates that genetic algorithms are highly effective in optimizing welding sequence with respect to dimensional variation and cycle time.",2011
"Approximate Surfacing of Curve Clouds for Conceptual Shape Creation and Evaluation","In product design, designers often create a multitude of concept sketches as part of the ideation process. Transforming such sketches to 3D digital models usually require special expertise and effort due to a lack of suitable Computer Aided Design (CAD) tools. Although recent advances in sketch-based user interfaces and immersive environments (such as augmented/virtual reality) have introduced novel curve design tools, rapid surfacing of such data remains an open challenge. To this end, we propose a new method that enables a quick construction of approximate surfaces from a cloud of 3D curves that need not be connected to one another. Our method first calculates a vector field by discretizing the space in which the curve cloud appears into a voxel image. This vector field drives a deformable surface onto the 3D curve cloud thus producing a closed surface. The surface smoothness is achieved through a set of surface smoothing and subdivision operations. Our studies show that the proposed technique can be particularly useful for early visualization and assessment of design ideas.",2011
"APIX: Analysis From Pixellated Inputs in Early Design Using a Pen-Based Interface","Product development is seeing a paradigm shift in the form of a simulation-driven approach. Recently, companies and designers have started to realize that simulation has the biggest impact when used as a concept verification tool in early stages of design. Early stage simulation tools like ANSYS™ Design Space and SIMULIA™ DesignSight Structure help to overcome the limitations in traditional product development processes where analyses are carried out by a separate group and not the designers. Most of these commercial tools still require well defined solid models as input and do not support freehand sketches, an integral part of the early design stage of product development. To this extent, we present APIX (acronym for A nalysis from Pix ellated Inputs), a tool for quick analysis of two dimensional mechanical sketches and parts from their static images using a pen-based interface. The input to the system can be offline (paper) sketches and diagrams, which include scanned legacy drawings and freehand sketches. In addition, images of two-dimensional projections of three dimensional mechanical parts can also be input. We have developed an approach to extract a set of boundary contours to represent a pixellated image using known image processing algorithms. The idea is to convert the input images to online sketches and use existing stroke-based recognition techniques for further processing. The converted sketch can now be edited, segmented, recognized, merged, solved for geometric constraints, beautified and used as input for finite element analysis. Finally, we demonstrate the effectiveness of our approach in the early design process with examples.",2011
"A Rational Design Approach to Gaussian Process Modeling for Variable Fidelity Models","Computer models and simulations are essential system design tools that allow for improved decision making and cost reductions during all phases of the design process. However, the most accurate models tend to be computationally expensive and can therefore only be used sporadically. Consequently, designers are often forced to choose between exploring many design alternatives with less accurate, inexpensive models and evaluating fewer alternatives with the most accurate models. To achieve both broad exploration of the design space and accurate determination of the best alternatives, surrogate modeling and variable accuracy modeling are gaining in popularity. A surrogate model is a mathematically tractable approximation of a more expensive model based on a limited sampling of that model. Variable accuracy modeling involves a collection of different models of the same system with different accuracies and computational costs. We hypothesize that designers can determine the best solutions more efficiently using surrogate and variable accuracy models. This hypothesis is based on the observation that very poor solutions can be eliminated inexpensively by using only less accurate models. The most accurate models are then reserved for discerning the best solution from the set of good solutions. In this paper, a new approach for global optimization is introduced, which uses variable accuracy models in conjuction with a kriging surrogate model and a sequential sampling strategy based on a Value of Information (VOI) metric. There are two main contributions. The first is a novel surrogate modeling method that accommodates data from any number of different models of varying accuracy and cost. The proposed surrogate model is Gaussian process-based, much like classic kriging modeling approaches. However, in this new approach, the error between the model output and the unknown truth (the real world process) is explicitly accounted for. When variable accuracy data is used, the resulting response surface does not interpolate the data points but provides an approximate fit giving the most weight to the most accurate data. The second contribution is a new method for sequential sampling. Information from the current surrogate model is combined with the underlying variable accuracy models’ cost and accuracy to determine where best to sample next using the VOI metric. This metric is used to mathematically determine where next to sample and with which model. In this manner, the cost of further analysis is explicitly taken into account during the optimization process.",2011
"An Improved Support Vector Domain Description Method for Modeling Valid Search Domains in Engineering Design Problems","Predictive modeling is an important tool in engineering design and optimization. Designers can develop a predictive model to replace a computationally-intensive physics-based model (a practice referred to as meta-modeling or response-surface modeling) or to model systems based on empirically-obtained data. However, such models typically have a limited domain of validity—that is, only certain combinations of model inputs yield predictions that are trustable. Consequently, designers must take care to bound the search space of optimization algorithms that otherwise would be unable to distinguish between valid and invalid predictions. Prior research has found that the valid input domain of a model can be shaped irregularly and difficult to model using simple bounds on input variables. The Support Vector Domain Description (SVDD) method was shown to be an effective approach for modeling such boundaries. However, the method used previously for generating the domain description is slow and scales poorly as the size of the training data set grows. This paper describes a new incremental method for generating a SVDD using a point-by-point comparison in place of considering all data points at once. This method is observed to be over 1000 times faster than the original method. This makes the overall approach attractive on problems of practical scale. We describe the new method, explore its characteristics, and demonstrate it on a design example for the selection of component concepts for a commercial power generation plant.",2011
"Convex Estimators for Optimization of Kriging Model Problems","This paper presents a framework for identification of the global optimum of Kriging models. The framework is based on a branch and bound scheme for sub-division of the search space into hypercubes while constructing convex under-estimators of the Kriging models. The convex under-estimators, which are a key development in this paper, provide a relaxation of the original problem. The relaxed problem has two key features: i) convex optimization algorithms such as sequential quadratic programming (SQP) are guaranteed to find the global optimum of the relaxed problem, and ii) objective value of the relaxed problem is a lower bound on the best attainable solution within a hypercube for the original (Kriging model) problem. The convex under-estimators improve in accuracy as the size of a hypercube gets smaller via the branching search. Termination of a hypercube branch is done when either: i) solution of the relaxed problem within the hypercube is no better than current best solution of the original problem, or ii) best solution of the original problem and that of the relaxed problem are within tolerance limits. To assess the significance of the proposed framework, comparison studies against genetic algorithm (GA) are conducted using Kriging models that approximate standard nonlinear test functions, as well as application problems of water desalination and vehicle crashworthiness. Results of the studies show the proposed framework deterministically providing a solution within tolerance limits from the global optimum, while GA is observed to not reliably discover the best solutions in problems with larger number of design variables.",2011
"On Using Kriging Models for Complex Design","The design of most modern systems requires the tight integration of multiple disciplines. In practice, these multiple disciplines are often optimized independently, given only fixed values or targets for their interactions with other disciplines. The result is a system that may not represent the optimal system-level design. It may also not be a robust design in the sense that small changes in each subsystem’s performance may have a large impact on the system-level performance. The use of kriging models to represent the response surfaces of subsystems that are then combined to estimate system-level performance can be used as a method to provide collaboration between design teams. The difficulty with this method is the creation of the models given potentially large number of dimensions or observations. This paper presents a method to reduce the dimensionality of the input space for kriging models used for designing of complex systems. The input dimensionality of the kriging model is reduced to only includes the most important factors needed for the prediction of the observed output. A result of using these reduced dimensionality models is the need to no longer force interpolation of all of the observations used to create the models.",2011
"Surrogate Modeling of Complex Systems Using Adaptive Hybrid Functions","This paper explores the effectiveness of the recently developed surrogate modeling method, the Adaptive Hybrid Functions (AHF), through its application to complex engineered systems design. The AHF is a hybrid surrogate modeling method that seeks to exploit the advantages of each component surrogate. In this paper, the AHF integrates three component surrogate models: (i) the Radial Basis Functions (RBF), (ii) the Extended Radial Basis Functions (E-RBF), and (iii) the Kriging model, by characterizing and evaluating the local measure of accuracy of each model. The AHF is applied to model complex engineering systems and an economic system, namely: (i) wind farm design; (ii) product family design (for universal electric motors); (iii) three-pane window design; and (iv) onshore wind farm cost estimation. We use three differing sampling techniques to investigate their influence on the quality of the resulting surrogates. These sampling techniques are (i) Latin Hypercube Sampling (LHS), (ii) Sobol’s quasirandom sequence, and (iii) Hammersley Sequence Sampling (HSS). Cross-validation is used to evaluate the accuracy of the resulting surrogate models. As expected, the accuracy of the surrogate model was found to improve with increase in the sample size. We also observed that, the Sobol’s and the LHS sampling techniques performed better in the case of high-dimensional problems, whereas the HSS sampling technique performed better in the case of low-dimensional problems. Overall, the AHF method was observed to provide acceptable-to-high accuracy in representing complex design systems.",2011
"Fundamental Concepts for Product Designs Based on Pareto Optimum Solutions","This paper proposes fundamental concepts for goal-defined product designs, and practical methodologies for achieving optimal designs based these concepts. Also emphasized are the functions and significance of Pareto optimum solution sets in multi-objective optimizations during the execution of the proposed methodologies. Three main concepts for product design optimization are presented. First, the goal of the product design optimization is specified to obtain the best harmony of related (and often conflicting) characteristics, where Pareto optimum solution sets represent this harmony and more preferable degrees of harmony cause an increase in social profit. Second, to obtain design solutions that maximize the desired harmony, deeper level characteristics in the design optimization problem are derived based on simplification or decomposition of the usual surface level characteristics, and optimizations are initiated from these deeper levels where the most important and influential aspects of the design problems are easiest to recognize. The third concept entails the use of collaboration with specialist experts concerning the product characteristics, focusing on Pareto optimum solution sets obtained in deeper level optimizations, so that these experts can facilitate the development of more preferable results based on their own ideas and knowledge. The interrelationships between the second and third concepts are described and used to obtain globally optimal design solutions that have the highest degree of harmony for the required product design objectives. The proposed concepts and methodologies for product design optimizations are demonstrated using certain designs for articulated robots.",2011
"Tracing the Envelope of the Objective-Space in Multi-Objective Topology Optimization","In multi-objective problems, one is often interested in generating the envelope of the objective-space, where the envelope is, in general, a superset of pareto-optimal solutions. In this paper, we propose a method for tracing the envelope of multi-objective ",2011
"Simultaneous Requirement and Design Optimization of an Industrial Robot Family Using Multi-Objective Optimization","Simultaneous development of an industrial robot family, consisting typically of 2–10 robots, has been an engineering practice in robotics industry. In this process, significant scenario studies on defining product requirement specifications and associated design change are conducted. This implies that understanding the relation between product requirements and design of the robot family is of critical importance. However, in the current engineering practice, any change in requirement specification results in tremendous efforts in the re-design of the robot family. This discloses the need for efficient methodology and tools for simultaneously optimizing product requirements and design of an industrial robot family. In this work, methodology and tools have been successfully developed for simultaneously optimizing product requirements and design of an industrial robot family in a fully automated way. This problem is formulated to a multi-objective optimization problem and solved using multi-objective genetic algorithm (MOGA). Results of this work have demonstrated clearly the efficiency of this approach and the insight obtained on the relation between product requirement and product design. The developed methodology and results of simultaneous requirement specification and design optimization will be detailed in this paper. In addition, research experience and future work will also be discussed. To our best knowledge, the simultaneous optimization of product requirement and product design has not been widely investigated and explored in academia. The trade-off information explored by such approach is crucial in product development in industrial practice. Such approach will further increase the complexity of traditional design optimization approach where product requirement is normally pre-defined and used as constraint. It is certain that discussions of the addressed problem and developed methodology will contribute to promoting the significance of efforts in the research society of multi-objective design optimization, multi-objective design optimization of product families, and design automation.",2011
"LIVE: A Work-Centered Approach to Support Visual Analytics of Multi-Dimensional Engineering Design Data With Interactive Visualization and Data-Mining","During the process of trade space exploration, information overload has become a notable problem. To find the best design, designers need more efficient tools to analyze the data, explore possible hidden patterns, and identify preferable solutions. When dealing with large-scale, multi-dimensional, continuous data sets (e.g., design alternatives and potential solutions), designers can be easily overwhelmed by the volume and complexity of the data. Traditional information visualization tools have some limits to support the analysis and knowledge exploration of such data, largely because they usually emphasize the visual presentation of and user interaction with data sets, and lack the capacity to identify hidden data patterns that are critical to in-depth analysis. There is a need for the integration of user-centered visualization designs and data-oriented data analysis algorithms in support of complex data analysis. In this paper, we present a work-centered approach to support visual analytics of multi-dimensional engineering design data by combining visualization, user interaction, and computational algorithms. We describe a system, Learning-based Interactive Visualization for Engineering design (LIVE), that allows designer to interactively examine large design input data and performance output data analysis simultaneously through visualization. We expect that our approach can help designers analyze complex design data more efficiently and effectively. We report our preliminary evaluation on the use of our system in analyzing a design problem related to aircraft wing sizing.",2011
"Multidisciplinary Design Optimization for Complex Engineered Systems Design: State of the Research and State of the Practice—Report From a National Science Foundation Workshop","Multidisciplinary design optimization (MDO) has evolved remarkably since its inception 25 years ago. Despite these advances, the design of complex engineered systems remains a challenge, and many large-scale engineering projects are routinely plagued by exorbitant cost overruns and delays. To gain insight into these challenges, 48 people gathered from industry, academia, and government agencies to examine MDO’s current and future role in designing complex engineered systems. This paper summarizes the views of five distinguished speakers on the “state of the research” along with the discussions from an industry panel of representatives from Boeing, Caterpillar, Ford, NASA Glenn Research Center, and United Technologies Research Center on the “state of the practice”. This paper also summarizes the future research topics identified by breakout groups in five key areas: (1) modeling and the design space; (2) metrics, objectives, and requirements; (3) coupling in complex engineered systems; (4) dealing with uncertainty; and (5) people and workflow. Finally, five over-arching themes are offered to advance MDO. First, we need to engage more disciplines outside of engineering and look for opportunities to use MDO outside of its traditional areas. Second, MDO problem formulations must evolve to encompass a wider range of design criteria. Third, we need effective strategies for putting designers “back in the loop” during MDO. Fourth, we need to do a better job of publicizing the successful examples of MDO so that we can improve the “buy in” that is needed to advance MDO in academia, industry, and government agencies. Fifth, we need to better educate our students and practitioners on systems design, optimization, and MDO along with their benefits and drawbacks.",2011
"Multidisciplinary Design Optimization of Ship Hull Forms Using Metamodels","Multidisciplinary optimization is a highly iterative process that requires a large number of function evaluations to evaluate objective functions and constraints. Metamodels for computationally expensive functions or simulations can be employed in the multidisciplinary optimization instead of the actual solvers resulting in significant computational savings. In this paper, metamodeling is applied to the multidisciplinary design optimization of a ship hull with resistance, seakeeping, and maneuvering performance analyses. At the top system level, a simple cost metric is defined to drive the overall design optimization process. Changes to the hull shape are reflected in the numerical model for resistance computations and in the simulations associated with the seakeeping and maneuvering disciplines. An automated process has been developed for propagating changes to the numerical (CFD) model for the resistance computations; this expedites the computations at the sample points used for developing the metamodels. The validity of employing metamodels instead of the actual solvers during the optimization is demonstrated by comparing the values of the objective functions and constraints at the optimum point when using the actual solvers and when using the metamodels.",2011
"Visually Exploring a Design Space Through the Use of Multiple Contextual Self-Organizing Maps","Understanding relationships amongst n-dimensional design spaces has long been a problem in the engineering community. Many visual methods previously developed, although useful, are limited to comparing three design variables at a time. Work described in this paper builds off the idea of a self-organizing map in order to visualize n-dimensional data on a two dimensional map. By using the contextual self-organizing map, current work shows that more design space information can be gleaned from map nodes themselves. By breaking the final visualization up into three maps containing separate contextual information, an investigator can quickly obtain information about the overall behavior of a design space. Tests run on well-known optimization functions show that information such as modality and curvature may be quickly suggested by these maps, and that they may provide enough information for a designer to choose a function to proceed with formal optimization of a given data set.",2011
"Multidisciplinary Design Optimization of Modular Industrial Robots","This paper presents a multidisciplinary design optimization framework for modular industrial robots. An automated design framework, containing physics based high fidelity models for dynamic simulation and structural strength analyses are utilized and seamlessly integrated with a geometry model. The proposed framework utilizes well-established methods such as metamodeling and multi-level optimization in order to speed up the design optimization process. The contribution of the paper is to show that by applying a merger of well-established methods, the computational cost can be cut significantly, enabling search for truly novel concepts.",2011
"Sequential Sampling With Kernel-Based Bayesian Network Classifiers","Complex design problems are typically decomposed into smaller design problems that are solved by domain-specific experts who must then coordinate their solutions into a satisfactory system-wide solution. In set-based collaborative design, collaborating engineers coordinate themselves by communicating multiple design alternatives at each step of the design process. Previous research has demonstrated that classifiers can be a communication medium for facilitating set-based collaborative design because of their ability to divide a design space into satisfactory and unsatisfactory regions. The proposed kernel-based Bayesian network (KBN) classifier uses a set of example designs of known acceptability, called the training set, to create a map of the satisfactory region of the design space. However, previous implementations used deterministic space-filling sampling sequences to choose the training set of designs. The shortcoming of deterministic space-filling sampling schemes is that they do not adapt to focus the samples on regions of interest to the design team (exploitation) or, alternatively, on regions in which little information is known (exploration). In this paper, we introduce the use of KBN classifiers as the basis for sequential sampling strategies that can be exploitive, exploratory, or any combination thereof.",2011
"Quantifying the Convergence Time of Distributed Design Processes","Time is an asset of critical importance in the design process and it is desirable to reduce the amount of time spent developing products and systems. Design is an iterative activity and a significant portion of time spent in the product development process is consumed by design engineers iterating towards a mutually acceptable solution. Therefore, the amount of time necessary to complete a design can be shortened by reducing the time required for design iterations or by reducing the number of iterations. The focus of this paper is on reducing the number of iterations required to converge to a mutually acceptable solution in distributed design processes. In distributed design, large systems are decomposed into smaller, coupled design problems where individual designers have control over local design decisions and seek to satisfy their own individual objectives. The number of iterations required to reach equilibrium solutions in distributed design processes can vary depending on the starting location and the chosen process architecture. We investigate the influence of process architecture on the convergence behavior of distributed design systems. This investigation leverages concepts from game theory, classical controls and discrete systems theory to develop a transient response model. As a result, we are able to evaluate process architectures without carrying out any solution iterations.",2011
"Co-Design of an Active Suspension Using Simultaneous Dynamic Optimization","Design of physical systems and associated control systems are coupled tasks; design methods that manage this interaction explicitly can produce system-optimal designs, whereas conventional sequential processes may not. Here we explore a new technique for combined physical system and control design (co-design) based on a simultaneous dynamic optimization approach known as direct transcription, which transforms infinite-dimensional control design problems into finite dimensional nonlinear programming problems. While direct transcription problem dimension is often large, sparse problem structures and fine-grained parallelism (among other advantageous properties) can be exploited to yield computationally efficient implementations. Extension of direct transcription to co-design gives rise to a new problem structures and new challenges. Here we illustrate direct transcription for co-design using a new automotive active suspension design example developed specifically for testing co-design methods. This example builds on prior active suspension problems by incorporating a more realistic physical design component that includes independent design variables and a broad set of physical design constraints, while maintaining linearity of the associated differential equations.",2011
"Bounded Target Cascading in Hierarchical Design Optimization","Analytical Target Cascading method has been widely developed to solve hierarchical design optimization problems. In the Analytical Target Cascading method, a weighted-sum formulation has been commonly used to coordinate the inconsistency between design points and assigned targets in each level while minimizing the cost function. However, the choice of the weighting coefficients is very problem dependent and improper selections of the weights will lead to incorrect solutions. To avoid the problems associated with the weights, single objective functions in the hierarchical design optimization are formulated by a new Bounded Target Cascading method. Instead of point targets assigned for design variables in the Analytical Target Cascading method, bounded targets are introduced in the new method. The target bounds are obtained from the optimal solutions in each level while the response bounds are updated back to the system level. If the common variables exist, they are coordinated based on their sensitivities with respect to design variables. Finally, comparisons of the results from the proposed method and the weighted-sum Analytical Target Cascading are presented and discussed.",2011
"Structural Optimization of Lattice Materials","Lattice materials are characterized at the microscopic level by a regular pattern of voids confined by walls. Recent rapid prototyping techniques allow their manufacturing from a wide range of solid materials, ensuring high degrees of accuracy and limited costs. The microstructure of lattice material permits to obtain macroscopic properties and structural performance, such as very high stiffness to weight ratios, highly anisotropy, high specific energy dissipation capability and an extended elastic range, which cannot be attained by uniform materials. Among several applications, lattice materials are of special interest for the design of morphing structures, energy absorbing components and hard tissue scaffold for biomedical prostheses. Their macroscopic mechanical properties can be finely tuned by properly selecting the lattice topology and the material of the walls. Nevertheless, since the number of the design parameters involved is very high, and their correlation to the final macroscopic properties of the material is quite complex, reliable and robust multiscale mechanics analysis and design optimization tools are a necessary aid for their practical application. In this paper, the optimization of lattice materials parameters is illustrated with reference to the design of a bracket subjected to a point load. Given the geometric shape and the boundary conditions of the component, the parameters of four selected topologies have been optimized to concurrently maximize the component stiffness and minimize its mass.",2011
"Multiscale Design and Multiobjective Optimization of Orthopaedic Cellular Hip Implants","A multiscale design and multiobjective optimization procedure is developed to design a new type of graded cellular hip implant. We assume that the prosthesis design domain is occupied by a unit cell representing the building block of the implant. An optimization strategy seeks the best geometric parameters of the unit cell to minimize bone resorption and interface failure, two conflicting objective functions. Using the asymptotic homogenization method, the microstructure of the implant is replaced by a homogeneous medium with an effective constitutive tensor. This tensor is used to construct the stiffness matrix for the finite element modeling (FEM) solver that calculates the value of each objective function at each iteration. As an example, a 2D finite element model of a left implanted femur is developed. The relative density of the lattice material is the variable of the multiobjective optimization, which is solved through the non-dominated sorting genetic algorithm II (NSGA-II). The set of optimum relative density distributions is determined to minimize concurrently interface stress distribution and bone loss mass. The results show that the amount of bone resorption and the maximum value of interface stress can be reduced by over 70% and 50%, respectively, when compared to current fully dense titanium stem.",2011
"Shape Design of Periodic Cellular Materials Under Cyclic Loading","This paper presents a method to improve the fatigue strength of 2D periodic cellular materials under a fully-reversed loading condition. For a given cell topology, the shape of the unit cell is synthesized to minimize any stress concentration caused by discontinuities in the cell geometry. We propose to reduce abrupt geometric changes emerging in the periodic microstructure through the synthesis of a cell shape defined by curved boundaries with continuous curvature, i.e. G2 -continous curves. The bending moments caused by curved cell elements are reduced by minimizing the curvature of G2 -continuous cell elements so as to make them as straight as possible. The asymptotic homogenization technique is used to obtain the homogenized stiffness matrix and the fatigue strength of the synthesized cellular material. The proposed methodology is applied to synthesize a unit cell topology described by smooth boundary curves. Numeric simulations are performed to compare the performance of the synthesized cellular solid with that of common two dimensional lattice materials having hexagonal, circular, square, and Kagome shape of the unit cell. The results show that the methodology enables to obtain a cellular material with improved fatigue strength. Finally, a parametric study is performed to examine the effect of different geometric parameters on the performance of the proposed cellular geometries.",2011
"Optimization of Honeycomb Cellular Meso-Structures for High Speed Impact Energy Absorption","This paper presents the energy absorption properties of hexagonal honeycomb structures of varying cellular geometries to high speed in-plane impact. While the impact responses in terms of energy absorption and densification strains have been extensively researched and reported, a gap is identified in the generalization of honeycombs with controlled and varying geometric parameters. This paper attempts to address this gap through a series of finite element (FE) simulations where cell angle and angled wall thickness are varied while maintaining a constant mass of the honeycomb structure. A randomly filled, non-repeating Design of Experiments (DOE) is generated to determine the effects of these geometric parameters on the output of energy absorbed, and a statistical sensitivity analysis is used to determine the parameters significant for optimization. A high degree of variation in the impact response of varying cellular geometries has shown the potential for the forward design into lightweight crushing regions in many applications, particularly the automotive and aerospace industries. It is found that while an increase in angled wall thickness enhances the energy absorption of the structure, increases in either the cell angle or ratio of cell angle to angled wall thickness have adverse effects on the output. Finally, optimization results present that a slightly auxetic cellular geometry with maximum angled wall thickness provides for maximum energy absorption, which is verified with an 8% error when compared to a final FE simulation.",2011
"Shear Compliant Hexagonal Cellular Solids With a Shape Memory Alloy","In this study, hexagonal honeycombs with a shape memory alloy (SMA) are explored for super-compliant meso-structural design. A nitianol (NiTi) SMA based shear compliant hexagonal cellular materials are introduced and their elastic properties in shear are investigated. The constitutive relation of SMA and Cellular Materials Theory (CMT) are used to develop analytical constitutive equations of SMA honeycombs under isothermal shear loading. A fixed volume based SMA honeycombs are designed with a target shear modulus, (",2011
"Return on Investment Analysis for Implementing Barriers to Reverse Engineering","Reverse engineering (extracting information about a product from the product itself) is a competitive strategy for many firms and is often costly to innovators. Recent research has proven metrics for estimating the reverse engineering time and barrier and has shown that products can strategically be made more difficult to reverse engineer, thus protecting the innovator. Reverse engineering, however, is only the first phase of attempting to duplicate a product. Imitating — the process of discovering how to physically reproduce the performance of the reverse engineered product in one or more of its performance areas — is the second and final phase. This paper presents metrics for the time and barrier to imitating and shows how they can be joined with reverse engineering metrics to estimate a total time and total barrier to duplicate a product. As there is a cost associated with the design of barriers to reverse engineering and imitating it is important that a return on investment analysis be performed to ensure a profitable endeavor. Details of such an analysis are presented here.",2011
"A Novel Search Algorithm for Interactive Automated Conceptual Design Generator (ACDG)","Automated concept generation is non-trivial task. The complexity of this problem is mainly due to lack of formal representation frameworks that lend themselves easily to a computational approach. Generative grammar has emerged as a potential solution to this problem and presents a number of different possibilities for conceptual design automation. This paper presents a novel search method that has been developed specifically for search trees defined by a special class of generative grammar in which rules of the grammar have parameters associated with them. A novel feature of the proposed search is ‘Human in the loop’ approach in which learning about the search space is achieved by querying the user. The user fatigue restricts the maximum number of comparisons of candidate solutions (30–50). From the data gathered from the comparisons, a stochastic decision making process proposed in this paper quickly converges to a region of design space which best meet the user’s preference. The method is implemented and applied to a grammar for shampoo bottle concept generation. It is shown through multiple user-guided and automated experiments that the method has ability to learn and adopt through human computer interaction process. The implications of the proposed search method for automated conceptual design are expounded on in the conclusions.",2011
"Object-Oriented Modeling of Industrial Manipulators With Application to Energy Optimal Trajectory Scaling","The development of safe, energy efficient mechatronic systems is currently changing standard paradigms in the design and control of industrial manipulators. In particular, most optimization strategies require the improvement or the substitution of different system components. On the other hand, from an industry point of view, it would be desirable to develop energy saving methods applicable also to established manufacturing systems being liable of small possibilities for adjustments. Within this scenario, an engineering method is reported for optimizing the energy consumption of serial manipulators for a given operation. An object-oriented modeling technique, based on bond graph, is used to derive the robot electromechanical dynamics. The system power flow is then highlighted and parameterized as a function of the total execution times. Finally, a case study is reported showing the possibility to reduce the operation energy consumption when allowed by scheduling or manufacturing constraints.",2011
"Analysis and Design of an In-Pipe System for Water Leak Detection","In most cases the deleterious effects associated with the occurrence of leaks may present serious problems and therefore, leaks must be quickly detected, located and repaired. The problem of leakage becomes even more serious when it is concerned with the vital supply of fresh water to the community. In addition to waste of resources, contaminants may infiltrate into the water supply. The possibility of environmental health disasters due to delay in detection of water pipeline leaks has spurred research into the development of methods for pipeline leak and contamination detection. Leaking in water networks has been a very significant problem worldwide, especially in developing countries, where water is sparse. Many different techniques have been developed to detect leaks, either from the inside or from the outside of the pipe; each one of them with their advantages, complexities but also limitations. To overcome those limitations we focus our work on the development of an in-pipe-floating sensor. The present paper discusses the design considerations of a novel autonomous system for in-pipe water leak detection. The system is carefully designed to be minimally invasive to the flow within the pipe and thus not to affect the delicate leak signal. One of its characteristics is the controllable motion inside the pipe. The system is capable of pinpointing leaks in pipes while operating in real network conditions, i.e. pressurized pipes and high water flow rates, which are major challenges.",2011
"A Design Exploration of Genetically Engineered Myosin Motors","As technology advances, there is an increasing need to reliably output mechanical work at smaller scales. At the nanoscale, one of the most promising routes is utilizing biomolecular motors such as myosin proteins commonly found in cells. Myosins convert chemical energy into mechanical energy and are strong candidates for use as components of artificial nanodevices and multi-scale systems. Isoforms of the myosin superfamily of proteins are fine-tuned for specific cellular tasks such as intracellular transport, cell division, and muscle contraction. The modular structure that all myosins share makes it possible to genetically engineer them for fine-tuned performance in specific applications. In this study, a parametric analysis is conducted in order to explore the design space of Myosin II isoforms. The crossbridge model for myosin mechanics is used as a basis for a parametric study. The study sweeps commonly manipulated myosin performance variables and explores novel ways of tuning their performance. The analysis demonstrates the extent that myosin designs are alterable. Additionally, the study informs the biological community of gaps in experimentally tabulated myosin design parameters. The study lays the foundation for further progressing the design and optimization of individual myosins, a pivotal step in the eventual utilization of custom-built biomotors for a broad range of innovative nanotechnological devices.",2011
"Identification of Product Family Platforms Using Pattern Recognition","In product family design the goal is to generate a set of lowest cost products that target specific market niches. Sharing components, called platforms, between different products can minimize duplication of effort, thereby lowering family costs. However, if the products’ requirements are too dissimilar, sharing components may compromise the end product; such variance will lead to lower end products being overdesigned and/or higher end products being underdesigned. It is important to identify which components are similar enough, so that sharing does not compromise the individual products’ performances. Most existing product family design methods make decisions a priori about platforms; constraining platforms to be used by every product in the family, or not at all. Methods that simultaneously optimize component sharing and design variable settings have the potential to find better families because product subsets may be more similar to each other than to other subsets of products. Allowing components to be shared between any subset of family members leads to a very large combinatorial problem, and considering large product families can be computationally prohibitive. This paper proposes a method to identify possible sets of product family platforms by using the pattern recognition technique of fuzzy c-means clustering on component subspaces. Component subspaces are taken from a database of generated design points for the whole family. If components from different products are similar enough to be grouped into the same cluster, then those components could possibly become the same platform. Fuzzy equivalence relations can be extracted from the cluster membership functions that show the binary relationship from one products’ component to a different products’ component. Ultimately, this method can be used as a platform identification heuristic in a larger product family design methodology. This method is demonstrated by applying it to find possible common components in a family of universal electric motors.",2011
"Recent Advancements in Product Family Design and Platform-Based Product Development: A Literature Review","Increase of demand on product variety has pushed companies to think about offering more and more product variants in order to take more market shares. However, product variation can lead to cost increase for design and production, as well as the lead time for new variants. As a result, a proper tradeoff is required between cost-effectiveness of manufacturing and satisfying diverse demands. Such tradeoff has been shown to be manageable effectively by exploiting product family design (PFD) and platform-based product development. These strategies have been widely studied during the past decades, and a large number of approaches have been proposed for covering different issues and steps related to design and development of product families and platforms. Verification and performance of such approaches have also been traced through practical case studies applied to several industries. This paper focuses on a review of the research in this field and efforts to classify the recent advancements relevant to product family design and platform development issues. A comprehensive review on the state-of-the-art research in this field was done by Jiao et al. in 2007; therefore the main focus of this paper is on the research activities from 2006 to present. Mainly, the effort of this paper is to identify new achievements in regard with different aspects of product family design such as customer involvement in design, market driven studies, new indices and metrics for assessing families and developing the desired platforms, issues relevant to product family optimization (i.e., new algorithms and optimization approaches applied to different PFD problems along with their benefits and limitations in comparison to previously developed approaches), issues relevant to development of platforms (i.e., platform configuration approaches, joint platform design and optimization, and factors effective on forming proper platform types), and issues relevant to knowledge management and modeling of families and platforms for facilitating and supporting future design efforts. Through a comparison with previous research, new achievements are discussed and the remaining challenges and potential new research areas in this field are addressed.",2011
"Platform Strategy for Product Family Design Using Particle Swarm Optimization","Product family design allows innovative companies to create customized product roadmaps, to manage designers and component partners, and to develop the next generation of products based on platform strategies. In product family design, problems for determining a design strategy or the degree of commonality for a platform can be considered as a multidisciplinary optimization problem with respect to design variables, production cost, company’s revenue, and customers’ satisfaction. In this paper, we investigate strategic module-based platform design to identify an optimal platform strategy in a product family. The objective of this paper is to introduce a multi-objective particle swarm optimization (MOPSO) approach to select the best platform design strategy from a set of Pareto-optimal solutions based on commonality and design variation within the product family. We describe modifications to apply the proposed MOPSO to the multi-objective problem of product family design and allow designers to evaluate varying levels of platform strategies. To demonstrate the effectiveness of the proposed approach, we use a case study involving a family of General Aviation Aircraft. The limitations of the approach and future work are also discussed.",2011
"A Platform Selection Approach Based on Product Family Ontology Modeling","Product family design is probably the most widely adopted strategy for product realization in mass customization paradigm. With the ever-increasing product offerings in consumer market, current product representation schemes are restricted by their limited capability in handling multiple conceptual relationships amongst product components and rich semantic annotations associated with different design concepts. Previously, we have studied and proposed an ontology-based information representation scheme for product family design, which offers a promising solution to address the aforementioned challenges. In this study, we suggest a new commonality metric and a faceted platform selection approach, which are both created for ontology-based product family representation models. Utilizing this metric and faceted search, we discuss the advantages of our approach compared to existing modeling possibilities. We also exemplify the applications of our proposal towards an optimal configuration of product variants using a case study of four laptop computer families. Finally, we conclude this paper with some indications for future work.",2011
"An Importance Sampling Approach for Time-Dependent Reliability","Reliability is an important engineering requirement for consistently delivering acceptable product performance through time. The reliability usually degrades with time increasing the lifecycle cost due to potential warranty costs, repairs and loss of market share. Reliability is the probability that the system will perform its intended function successfully for a specified time. In this article, we consider the first-passage reliability which accounts for the first time failure of non-repairable systems. Methods are available which provide an upper bound to the true reliability which may overestimate the true value considerably. The traditional Monte-Carlo simulation is accurate but computationally expensive. A computationally efficient importance sampling technique is presented to calculate the cumulative probability of failure for random dynamic systems excited by a stationary input random process. Time series modeling is used to characterize the input random process. A detailed example demonstrates the accuracy and efficiency of the proposed importance sampling method over the traditional Monte Carlo simulation.",2011
"A Variable-Size Local Domain Approach for Concurrent Design Optimization and Model Validation Using Parametric Bootstrap","A common approach to the validation of simulation models focuses on validation throughout the entire design space. In a more recent methodology, we proposed to validate designs as they are generated during a simulation-based optimization process, relying on validating the simulation model through calibration in a sequence of local domains. In that work, the size of the local domains was held fixed and not linked to uncertainty, and the confidence in designs was quantified using Bayesian hypothesis testing. In this article, we present an improved methodology where the size and shape of the local domain at each stage of a sequential design optimization process, are determined from a parametric bootstrap methodology involving maximum likelihood estimators of unknown model parameters. Validation through calibration is carried out in the local domain at each stage. The sequential process continues until the local domain does not change from stage to stage during the design optimization process, ensuring convergence to an optimal design. The proposed methodology is illustrated with the design of a thermal insulator using one-dimensional, linear heat conduction in a solid slab with heat flux boundary conditions.",2011
"Addressing Limitations of Pareto Front in Design Under Uncertainty","Engineering design reconciles design constraints with decision maker preferences. The task of eliciting and encoding decision maker preferences is, however, extremely difficult. A Pareto front representing the locus of the non-dominated designs is therefore, often generated to help a decision maker select the best design. In this paper, we show that this method has a shortcoming. We show that when there is uncertainty in both the decision problem variables and in the decision maker’s preferences, this methodology is inconsistent with multi-attribute utility theory, unless the decision maker trades off attributes or some functions of them linearly. This is a strong restriction. To account for this, we propose a methodology that enables a decision maker to select the best design on a modified Pareto front which is acquired using envelopes of a set of certainty equivalent surfaces. This methodology does not require separability of the multi-attribute utility function into single attribute utilities, nor does it require the decision maker to trade the attributes (or any function of them) linearly. We demonstrate this methodology on a simple optimization problem and in design of a reduction gear.",2011
"Robustness of Residual Stresses in Brake Discs by Metamodeling","During casting residual stresses are developed due to the solidification and cooling. In this work the robustness of residual stresses in casted brake discs with respect to variations in four parameters is evaluated. The parameters are Young’s modulus, yield strength and hardening, time of breaking the mould and the thickness of the brake disc. The robustness analysis is performed by Monte Carlo simulations of metamodels which are surrogates to a finite element model. Quadratic response surfaces and Kriging approximations are considered. Those are based on finite element analyses defined by a Latin hypercube sampled design of experiments. In the finite element analyses an un-coupled approach is utilized where a thermal analysis generates a temperature history of the solidification and cooling. Then follows a structural analysis which is driven by the temperature history. After casting the machining of the brake disc is analyzed by gradually removing elements in the finite element model. The results show that the variation in the studied parameters yield large variation in residual stresses. The thickness of the brake disc is the parameter that has largest influence to the variation in residual stresses. Furthermore, the level of the residual stresses are in general high and might influence the fatigue life of the brake disc.",2011
"Equivalent Standard Deviation to Convert High-Reliability Model to Low-Reliability Model for Efficiency of Sampling-Based RBDO","This study presents a methodology to convert an RBDO problem requiring very high reliability to an RBDO problem requiring relatively low reliability by increasing input standard deviations for efficient computation in sampling-based RBDO. First, for linear performance functions with independent normal random inputs, an exact probability of failure is derived in terms of the ratio of the input standard deviation, which is denoted by ",2011
"Adaptive Virtual Support Vector Machine for the Reliability Analysis of High-Dimensional Problems","In this study, an efficient classification methodology is developed for reliability analysis while maintaining the accuracy level similar to or better than existing response surface methods. The sampling-based reliability analysis requires only the classification information — a success or a failure – but the response surface methods provide real function values as their output, which requires more computational effort. The problem is even more challenging to deal with high-dimensional problems due to the curse of dimensionality. In the newly proposed virtual support vector machine (VSVM), virtual samples are generated near the limit state function by using linear or Kriging-based approximations. The exact function values are used for approximations of virtual samples to improve accuracy of the resulting VSVM decision function. By introducing the virtual samples, VSVM can overcome the deficiency in existing classification methods where only classified function values are used as their input. The universal Kriging method is used to obtain virtual samples to improve the accuracy of the decision function for highly nonlinear problems. A sequential sampling strategy that chooses a new sample near the true limit state function is integrated with VSVM to maximize the accuracy. Examples show the proposed adaptive VSVM yields better efficiency in terms of the modeling time and the number of required samples while maintaining similar level or better accuracy especially for high-dimensional problems.",2011
"Robustness Metrics for Time-Dependent Quality Characteristics","Quality characteristics (QC’s) are often treated static in robust design optimization while many of them are time dependent in reality. It is therefore desirable to define new robustness metrics for time-dependent QC’s. This work shows that using the robustness metrics of static QC’s for those of time-dependent QC’s may lead to erroneous design results. To this end, we propose the criteria of establishing new robustness metrics for time-dependent QC’s and then define new robustness metrics. Instead of using a point expected quality loss over the time period of interest, we use the expectation of the maximal quality loss over the time period to quantify the robustness for time-dependent QC’s. Through a four-bar function generator mechanism analysis, we demonstrate that the new robustness metrics can capture the full information of robustness of a time-dependent QC over a time interval. The new robustness metrics can then be used as objective functions for time-dependent robust design optimization.",2011
"Quantifying Model Uncertainty Using Measurement Uncertainty Standards","There is always a deviation between a model prediction and the reality that the model intends to represent. The deviation is largely caused by the model uncertainty due to ignorance, assumptions, simplification, and other sources of lack of knowledge. Quantifying model uncertainty is a vital task and requires the comparison between model prediction and observation. This exercise is generally computationally intensive on the prediction side and costly on the experimentation side. In this work, a new methodology is proposed to provide an alternative implementation of model uncertainty quantification. With the new methodology, the experimental results are reported with expanded uncertainty terms around the experimental results for both model input and output. In other words, the experimental results are expressed as intervals. Then the model takes the experimental results of the input intervals and produces an interval prediction. The model uncertainty is then quantified by the difference between the model prediction and experimental observation, represented by an interval as well. By employing the standards for measurement uncertainty, the new methodology is easy to implement and could serve as a common framework for both model builders and experimenters.",2011
"Multi-Stage Uncertainty Quantification for Verifying the Correctness of Complex System Designs","Designing complex systems for mission-critical applications requires a design process with focus upon maximizing the probability of meeting design requirements. Typically the design process for these systems consists of filtering and refining an initial set of conceptual designs to produce a final set of detailed designs. This final set of designs is presented to the system contracting agency or management team for design selection. In this work, a framework is presented for a multi-stage design process in which the Probability of Correctness (PoC) is utilized as a metric to sequentially filter designs from the abstract conceptual phase through the detailed design phase. This framework utilizes methods for uncertainty propagation (UP) from reliability engineering, which are organized within the framework to match the UP method with the model fidelity and data type available at each stage of the process. A case study using the Advanced Diagnostic and Prognostic Testbed (ADAPT) Electric Power System (EPS) is presented to illustrate both the verification process utilizing multiple UP methods, and also the use of the OpenModelica environment for system design. A discussion presents a generalization of the framework and the future work needed to realize the comprehensive framework for system design.",2011
"Resilience-Driven System Design of Complex Engineered Systems","Most engineered systems are designed with a passive and fixed design capacity and, therefore, may become unreliable in the presence of adverse events. Currently, most engineered systems are designed with system redundancies to ensure required system reliability under adverse events. However, a high level of system redundancy increases a system’s life-cycle cost (LCC). Recently, proactive maintenance decisions have been enabled through the development of prognostics and health management (PHM) methods that detect, diagnose, and predict the effects of adverse events. Capitalizing on PHM technology at an early design stage can transform passively reliable (or vulnerable) systems into adaptively reliable (or resilient) systems while considerably reducing their LCC. In this paper, we propose a resilience-driven system design (RDSD) framework with the goal of designing complex engineered systems with resilience characteristics. This design framework is composed of three hierarchical tasks: (i) the resilience allocation problem (RAP) as a top-level design problem to define a resilience measure as a function of reliability and PHM efficiency in an engineering context, (ii) the system reliability-based design optimization (RBDO) as the first bottom-level design problem for the detailed design of components, and (iii) the system PHM design as the second bottom-level design problem for the detailed design of PHM units. The proposed RDSD framework is demonstrated using a simplified aircraft control actuator design problem resulting in a highly resilient actuator with optimized reliability, PHM efficiency and redundancy for the given parameter settings.",2011
"Reliable Kinetic Monte Carlo Simulation Based on Random Set Sampling","To simulate the dynamic behaviors of large molecular systems, approaches that solve ordinary differential equations such as molecular dynamics (MD) simulation may become inefficient. The kinetic Monte Carlo (KMC) method as the alternative has been widely used in simulating rare events such as chemical reactions or phase transitions. Yet lack of complete knowledge of transitions and the associated rates is one major challenge for accurate KMC predictions. In this paper, a reliable KMC (R-KMC) mechanism is proposed to improve the robustness of KMC results, where propensities are interval estimates instead of precise numbers and sampling is based on random sets instead of random numbers. A multi-event algorithm is developed and implemented. The weak convergence of the multi-event algorithm towards traditional KMC is demonstrated with a proposed generalized Chapman-Kolmogorov Equation.",2011
"Hybrid Power Generation System Design Optimization Based on a Markovian Reliability Analysis Approach","The optimal design of hybrid power generation systems (HPGS) can significantly improve the economic and technical performance of power supply. Due to the intermittent nature of renewable energy sources, as well as the application of energy storage techniques, the efficacy and efficiency of reliability assessment have become vital for successful HPGS design optimization. This paper proposes a sizing optimization method for HPGS based on a Markovian approach for long term reliability assessment. A multi-scenario formulation is considered to minimize the system cost while guaranteeing acceptable reliability across all the representative scenarios. The presented reliability analysis approach employs a Markov chain to model the state of charge of the energy storage based on probabilistic resource and load models. With this treatment, the loss of load probability of the HPGS can be tracked with relatively low computation, making it suitable for optimization applications. The effectiveness of the reliability analysis approach is tested through a comparison with Monte Carlo simulation; then the optimization approach is demonstrated with a numerical case study.",2011
"Improving Identifiability in Model Calibration Using Multiple Responses","The use of complex computer simulations to design, improve, optimize, or simply to better understand complex systems in many fields of science and engineering is now ubiquitous. However, simulation models are never a perfect representation of physical reality. Two general sources of uncertainty that account for the differences between simulations and experiments are parameter uncertainty and model uncertainty. The former derives from unknown model parameters, while the latter is caused by underlying missing physics, numerical approximations, and other inaccuracies of the computer simulation that exist even if all of the parameters are known. To obtain knowledge of these two sources of uncertainty, data from computer simulations (usually abundant) and data from physical experiments (typically more limited) are often combined using statistical methods. Statistical adjustment of the computer simulation model to account for the two sources of uncertainty is referred to as calibration. We argue that calibration as it is typically implemented, using only a single response variable, is challenging in that it is often extremely difficult to distinguish between the effects of parameter and model uncertainty. However, many different responses (distinct responses and/or the same response measured at different spatial and temporal locations) are automatically calculated in simulations. As multiple responses generally share a mutual dependence on the unknown parameters, they provide valuable information that can improve identifiability of parameter and model uncertainty in calibration, if they are also measured experimentally. In this paper, we explore the use of multiple responses for calibration.",2011
"Robust Structural Design Optimization Under Non-Probabilistic Uncertainties","Robust structural design optimization with non-probabilistic uncertainties is often formulated as a two-level optimization problem. The top level optimization problem is simply to minimize a specified objective function while the optimized solution at the second level solution is within bounds. The second level optimization problem is to find the worst case design under non-probabilistic uncertainty. Although the second level optimization problem is a non-convex problem, the global optimal solution must be assured in order to guarantee the solution robustness at the first level. In this paper, a new approach is proposed to solve the robust structural optimization problems with non-probabilistic uncertainties. The WCDO problems at the second level are solved directly by the monotonocity analysis and the global optimality is assured. Then, the robust structural optimization problem is reduced to a single level problem and can be easily solved by any gradient based method. To illustrate the proposed approach, truss examples with non-probabilistic uncertainties on stiffness and loading are presented.",2011
