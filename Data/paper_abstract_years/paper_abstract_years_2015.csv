title,Abstract,year
"Multiobjective Optimization Under Uncertainty in Advanced Abrasive Machining Processes via a Fuzzy-Evolutionary Approach","This paper considers multiobjective optimization under uncertainty (MOOUC) for the selection of optimal cutting conditions in advanced abrasive machining processes. Processes considered are water-jet machining, abrasive water-jet machining and ultra-sonic machining. Decisions regarding the cutting conditions can involve optimization for multiple competing goals; such as surface finish, machining time and power consumption. In practice, there is also an issue of variations in the ability to attain the performance goals. This can be due to limitations in machine accuracy or variations in material properties of the workpiece and/or abrasive particles. The approach adopted in this work relies on a Strength Pareto Evolutionary Algorithm (SPEA2) framework, with specially tailored dominance operators to account for probabilistic aspects in the considered multiobjective problem. Deterministic benchmark problems in the literature for the considered machining processes are extended to include performance uncertainty, and then used in testing the performance of the proposed approach. Results of the study show that accounting for process variations through a simple penalty term may be detrimental for the multiobjective optimization. On the other hand, a proposed Fuzzy-tournament dominance operator appears to produce favorable results.",2015
"Multi-Objective Optimization of Gas Blend Alternative Refrigerants for Vapor-Compression Refrigeration Systems","This paper presents a theoretical study on optimizing the mixing ratios of hydrocarbon blends to be used as refrigerants in existing refrigeration equipment. The primary objective is to maximize the coefficient of performance. The gas blending optimization problem is posed in a multi-objective framework, where the optimization seeks to generate Pareto optimal solutions that span the trade-off frontier between coefficient of performance versus deviation from a desired volumetric refrigeration capacity, while adhering to a maximum compression ratio. Design variables in the optimization are the mass fractions of hydrocarbon gases in the blend. A domain reduction scheme is introduced, which allows for efficient conduction of exhaustive search, with up to three hydrocarbon gases in the blend. While exhaustive search guarantees that the obtained solutions are global optima, the computational resources it requires scale poorly as the number of design variables increase. Two alternative approaches, (multi-start SQP) and (NSGA-II) are also tested for solving the optimization problem. Numerical simulation case studies for replacement of R12, R22 and R134a with hydrocarbon blends of isobutane, propane and propylene show agreement between solution methods that good compromises are possible to achieve, but a small loss in coefficient of performance is inevitable.",2015
"Optimization of Cutting Conditions in Vibration Assisted Drilling of Composites via a Multi-Objective EGO Implementation","A recent and promising technique to overcome the challenges of conventional drilling is vibration-assisted drilling (VAD) whereby a controlled harmonic motion is superimposed over the principal drilling feed motion in order to create an intermittent cutting state. Two additional variables other than the feed and the speed are introduced, namely the frequency and the amplitude of the imposed vibrations. Optimum selection of cutting conditions in VAD operations of composite materials is a challenging task due to several reasons; such as the increase in the number of controllable variables, the need for costly experimentation, and the limitation on the number of experiments that can be performed before tool degradation becomes an issue in the reliability of measurements. Additionally, there are often several objectives to consider, some of which may be conflicting, while others may be somewhat correlated. Pareto-optimality analysis is needed for conflicting objectives; however the existence of several objectives (high-dimension Pareto space) makes the generation and interpretation of Pareto solutions difficult. An attractive approach to the optimization task is thus to employ Kriging meta-models in a multi-objective efficient global optimization (m-EGO) framework for incremental experimentation of optimal setting of the cutting parameters. Additional challenge posed by constraints on machine capabilities is accounted for through domain transformation of the design variables prior to the construction of the Kriging models. Study results using a baseline exhaustive experimental data shows opportunity for employing m-EGO for the generation of well distributed Pareto-frontiers with fewer experiments.",2015
"Towards Automated Design of Mechanically Functional Molecules","Metal Organic Responsive Frameworks (MORFs) are a proposed new class of smart materials consisting of a Metal Organic Framework (MOF) with photoisomerizing beams (also known as linkers) that fold in response to light. Within a device these new light responsive materials could provide the capabilities such as photo-actuation, photo-tunable rigidity, and photo-tunable porosity. However, conventional MOF architectures are too rigid to allow isomerization of photoactive sub-molecules. We propose a new computational approach for designing MOF linkers to have the required mechanical properties to allow the photoisomer to fold by borrowing concepts from de novo molecular design and graph synthesis. Here we show how this approach can be used to design compliant linkers with the necessary flexibility to be actuated by photoisomerization and used to design MORFs with desired functionality.",2015
"A Prediction Modeling Framework: Toward Integration of Noisy Manufacturing Data and Product Design","In many design and manufacturing applications, data inconsistency or noise is common. These data can be used to create opportunities and/or support critical decisions in many applications, for example, welding quality prediction for material selection and quality monitoring applications. Typical approaches to deal with these data issues are to remove or alter them before constructing any model or conducting any analysis to draw decisions. However, these approaches are limited especially when each data carries important value to extract additional information about the nature of the given problem. In the literature, with the presence of noise in data, bootstrap aggregating has shown an improvement in the prediction accuracy. In order to achieve such an improvement, a bagging model has to be carefully constructed. The base learning algorithm, number of base learning algorithms, and parameters for the base learning algorithms are crucial design parameters in that aspect. Evolutionary algorithms such as genetic algorithm and particle swarm optimization have shown promising results in determining good parameters for different learning algorithms such as multilayer perceptron neural network and support vector regression. However, the computational cost of an evolutionary computation algorithm is usually high as they require a large number of candidate solution evaluations. This requirement even more increases when bagging is involved rather than a single learning algorithm. To reduce such high computational cost, a metamodeling approach is introduced to particle swarm optimization. The meta-modeling approach reduces the number of fitness function evaluations in the particle swarm optimization process and therefore the overall computational cost can be reduced. In this paper, we propose a prediction modeling framework whose aim is to construct a bagging model to improve the prediction accuracy on noisy data. The proposed framework is tested on an artificially generated noisy dataset. The quality of final solutions obtained by the proposed framework is reasonable compared to particle swarm optimization without meta-modeling. In addition, using the proposed framework, the largest improvement in the computational time is about 42 percent.",2015
"Automatically Synthesizing Principle Solutions in Multi-Disciplinary Conceptual Design With Functional and Structural Knowledge","Synthesizing principle solutions (PSs) in various disciplines together is a common practice in multi-disciplinary conceptual design (MDCD), which generates the combination of PSs to meet the desired functional requirement. Different from structure- and function-based synthesis methods, a hybrid PS synthesis (HPSS) method through integrating functional and structural knowledge is proposed in this paper, which not only achieves the automated synthesis of multi-disciplinary PSs, but also resolves the undesired physical conflicts during the synthesis process. It comprises of united representation approach for modeling functional and structural knowledge of multi-disciplinary PSs, adapted agent-based approach for chaining the specified functional flows of PSs, and the extension conflict resolve approach for handling the partial design conflicts among selected PSs. An industrial case study on the emergency cutting off (ECO) device design was given to validate the applicability of HPSS, and it indicates that HPSS can not only get multi-disciplinary design result of ECO device, but also further resolve the design conflict (i.e., vibration impact) to optimize the functional structure of ECO device.",2015
"A Method for Visualizing the Relations Between Grammar Rules, Performance Objectives and Search Space Exploration in Grammar-Based Computational Design Synthesis","Design grammars have been successfully applied in numerous engineering disciplines, e.g. in electrical engineering, architecture and mechanical engineering. A successful application of design grammars in Computational Design Synthesis (CDS) requires a) a meaningful representation of designs and the design task at hand, b) a careful formulation of grammar rules to synthesize new designs, c) problem specific design evaluations, and d) the selection of an appropriate algorithm to guide the synthesis process. Managing these different aspects of CDS requires not only a detailed understanding of each individual part, but also of the interdependencies between them. In this paper, a new method is presented to analyze the exploration of design spaces in CDS. The method analyzes the designs generated during the synthesis process and visualizes how the design space is explored with respect to a) design characteristics, and b) objectives. The selected algorithm as well as the grammar rules can be analyzed with this approach to support the human designer in successfully understanding and applying a CDS method. The case study demonstrates how the method is used to analyze the synthesis of bicycle frames. Two algorithms are compared for this task. Results demonstrate how the method increases the understanding of the different components in CDS. The presented research can be useful for both novices to CDS to help them gain a deeper understanding of the interplay between grammar rules and guidance of the synthesis process, as well as for experts aiming to further improve their CDS application by improving parameter settings of their search algorithms, or by further refining their design grammar. Additionally, the presented method constitutes a novel approach to interactively visualize design space exploration considering not only designs objectives, but also the characteristics and interdependencies of different designs.",2015
"Automated Concept Generation Based on Function-Form Synthesis","This work hypothesizes that enhancing next generation products’ distinctiveness through function-form synthesis results in feasible design concepts for designers. A data mining driven methodology that searches for novel function and form candidates suitable to include in next generation product design is introduced in this work. The methodology employs a topic modeling algorithm to search for functional relationships between the current product design and designs from related/unrelated domains. Combining the current product design and candidate products’ form and function, which is acquired from related/unrelated domains, generates next generation design concepts. These resulting design concepts are not only distinct from their parent designs but are also likely to be implemented in the real world by containing novel functions and form features. A hybrid marine model, which is differentiated from both the current design and candidate products in related/unrelated domains, is introduced in the case study in order to demonstrate the proposed methodology’s potential to develop concepts for novel product domains. By comparing the form and function similarity values between generated design concepts, an existing hybrid marine model (Wing In Ground effect ship: WIG), and source products, this research verifies the feasibility of these design concepts.",2015
"ecoRacer: Game-Based Optimal Electric Vehicle Design and Driver Control Using Human Players","We investigate the cost and benefit of crowdsourcing solutions to an NP-complete powertrain design and control problem. Specifically, we cast this optimization problem as an online competition, and received 2391 game plays by 124 anonymous players during the first week from the launch. We compare the performance of human players against that of the Efficient Global Optimization (EGO) algorithm. We show that while only a small portion of human players can outperform the algorithm in long term, players tend to formulate good heuristics early on, from where good solutions can be extracted and used to constrain the solution space. Incorporating this constraint into the search enhances the efficiency of the algorithm, even for problem settings different from the game. These findings indicate that human computation is promising in solving comprehensible and computationally hard optimal design and control problems.",2015
"Predictive Modeling of Product Returns for Remanufacturing","As awareness of environmental issues increases, the pressures from the public and policy makers have forced OEMs to consider remanufacturing as the key product design option. In order to make the remanufacturing operations more profitable, forecasting product returns is critical with regards to the uncertainty in quantity and timing. This paper proposes a predictive model selection algorithm to deal with the uncertainty by identifying better predictive models. Unlike other major approaches in literature (distributed lag model or DLM), the predictive model selection algorithm focuses on the predictive power over new or future returns. The proposed algorithm extends the set of candidate models that should be considered: autoregressive integrated moving average or ARIMA (previous returns for future returns), DLM (previous sales for future returns), and mixed model (both previous sales and returns for future returns). The prediction performance measure from holdout samples is used to find a better model among them. The case study of reusable bottles shows that one of the candidate models, ARIMA, can predict better than the DLM depending on the relationships between returns and sales. The univariate model is widely unexplored due to the criticism that the model cannot utilize the previous sales. Another candidate model, mixed model, can provide a chance to find a better predictive model by combining the ARIMA and DLM. The case study also shows that the DLM in the predictive model selection algorithm can provide a good predictive performance when there are relatively strong and static relationships between returns and sales.",2015
"Decision Support for Performance Arts Using Support Vector Regression With Genetic and Particle Swarm Algorithms","Different from typical mechanical products, tickets for movies and performing arts can be considered as a special type of consumer products. Compared to widely known box-office receipts prediction with single-output in movie industry, estimating the market share and price for performing arts is still a challenging problem due to high dimensional datasets yet limited number of samples. This paper describes a data-driven decision support system to help arts managers make strategic decisions, especially on session-determination and price-setting, considering price discrimination and prediction on the corresponding sales volume. Eight different attributes from the database, with multiple labels in each attribute, are used to accurately and comprehensively represent and classify the characteristics of performing arts in each genre. A web-based influence factor is also defined to quantify the popularity and publicity of performing arts. For this multi-input and multi-output problem, support vector regression (SVR) is employed and its optimal parameters are determined using genetic algorithm (GA) and particle swarm optimization (PSO) respectively. Price utility axiom with the law of demand is applied to maximize the receipts. Compared to artificial neural networks (ANN), those two optimization based SVR methods perform much better, in terms of effectiveness and reliability.",2015
"A Novel Application of Gamification for Collecting High-Level Design Information","This paper presents a novel application of gamification for collecting high-level design descriptions of objects. High-level design descriptions entail not only superficial characteristics of an object, but also function, behavior, and requirement information of the object. Such information is difficult to obtain with traditional data mining techniques. For acquisition of high-level design information, we investigated a multiplayer game, “Who is the Pretender?” in an offline context. Through a user study, we demonstrate that the game offers a more fun, enjoyable, and engaging experience for providing descriptions of objects than simply asking people to list them. We also show that the game elicits more high-level, problem-oriented requirement descriptions and less low-level, solution-oriented structure descriptions due to the unique game mechanics that encourage players to describe objects at an abstract level. Finally, we present how crowdsourcing can be used to generate game content that facilitates the gameplay. Our work contributes towards acquiring high-level design knowledge that is essential for developing knowledge-based CAD systems.",2015
"Automatic Extraction of Function Knowledge From Text","This paper presents a method to automatically extract function knowledge from natural language text. Our method uses syntactic rules to extract subject-verb-object triplets from parsed text. We then leverage the Functional Basis taxonomy, WordNet, and word2vec to classify the triplets as artifact-function-energy flow knowledge. For evaluation, we compare the function definitions associated with 30 most frequent artifacts compiled in a human-constructed knowledge base, Oregon State University’s Design Repository (DR), to those extracted using our method from 4953 Wikipedia pages classified under the category “Machines”. Our method found function definitions for 66% of the test artifacts. For those artifacts found, our method identified 50% of the function definitions compiled in DR. In addition, 75% of the most frequent function definitions found by our method were also defined in DR. The results demonstrate the promising potential of our method in automatic extraction of function knowledge.",2015
"Predicting the Benefits of Topology Optimization","Topology optimization is a systematic method of generating designs that maximize specific objectives. While it offers significant benefits over traditional shape optimization, topology optimization can be computationally demanding and laborious. Even a simple 3D compliance optimization can take several hours. Further, the optimized topology must typically be manually interpreted and translated into a CAD-friendly and manufacturing friendly design.",2015
"Solution Space Exploration in Model-Based Realization of Engineered Systems","George Box a British mathematician and professor of statistics wrote that “essentially, all models are wrong, but some are useful.” In keeping with George Box’s observation we suggest that in the model-based realization of complex systems, the decision maker must be able to work constructively with decision models of varying fidelity, completeness and accuracy in order to make defendable decisions under uncertainty. The models, and the search algorithms that use these models, will never be perfect and the inherent inaccuracy and incompleteness of analysis models and solvers manifest as uncertainties in the projected outcomes. Therefore, a significant and desirable step in any model-based application is to find stable and robust solutions in which variation of the (input) variables and parameters within manageable tolerances has the minimum effect on delivering favorable, system performance. In this paper we present a method for visualizing and exploring the solution space using the compromise Decision Support Problem (cDSP) as a decision model to aid a decision maker in finding these stable and robust solutions.",2015
"Non Prismatic Air-Breathing Fuel Cells: Concept, Design and Manufacturing","This paper details the research into Axis Symmetric Architecture (ASA) Proton Exchange Membrane (PEM) fuel cells, which are fuel cells that possess a non-prismatic cylindrical architecture as compared to the traditional flat plate designs. The paper elucidates the advantages on the ASA designs versus flat plate designs, including increased fuel flow characteristics, reduced sealing area, reduced weight characteristics and increased power densities (power/weight ratios). Finite element analysis will show improvements to flow characteristics on both the cathode and anode side along with a study of the flow channel cross-sections. The ASA design facilitates natural convective flow to promote improved reactant availability and the prototypes created show the ease of manufacture and assembly. ASA’s unlike traditional fuel cells do not require bulky clamping plates and extensive fastening mechanisms and hence lead to prototypes with reduced size, weight and cost.",2015
"Understanding the Utilization of Information Stimuli in Design Decision Making Using Eye Gaze Data","Research on decision making in engineering design has focused primarily on how to make decisions using normative models given certain information. However, there exists a research gap on how diverse information stimuli are combined by designers in decision making. In this paper, we address the following question: ",2015
"Impact of Technology Infusion on System Complexity and Modularity","Complex systems often have long life cycles with requirements that are likely to change over time. Therefore, it is important to be able to adapt the system accordingly over time. This is often accomplished by infusing new technologies into the host system in order to update or improve overall system performance. However, technology infusion often results in a disruption in the host system. This can take the form of a system redesign or a change in the inherent attributes of the system. In this study, we analyzed the impact of technology infusion on system attributes, specifically the complexity and modularity. Two different systems that were infused with new technologies were analyzed for changes in complexity and modularity.",2015
"Modeling Consumer Decisions on Returning End-of-Use Products Considering Design Features and Consumer Interactions: An Agent Based Simulation Approach","As electronic waste (e-waste) becomes one of the fastest growing environmental concerns, remanufacturing is considered as a promising solution. However, the profitability of take back systems is hampered by several factors including the lack of information on the quantity and timing of to-be-returned used products to a remanufacturing facility. Product design features, consumers’ awareness of recycling opportunities, socio-demographic information, peer pressure, and the tendency of customer to keep used items in storage are among contributing factors in increasing uncertainties in the waste stream. Predicting customer choice decisions on returning back used products, including both the time in which the customer will stop using the product and the end-of-use decisions (e.g. storage, resell, through away, and return to the waste stream) could help manufacturers have a better estimation of the return trend. The objective of this paper is to develop an Agent Based Simulation (ABS) model integrated with Discrete Choice Analysis (DCA) technique to predict consumer decisions on the End-of-Use (EOU) products. The proposed simulation tool aims at investigating the impact of design features, interaction among individual consumers and socio-demographic characteristics of end users on the number of returns. A numerical example of cellphone take-back system has been provided to show the application of the model.",2015
"Design as a Sequential Decision Process: A Method for Reducing Design Set Space Using Models to Bound Objectives","Design is a sequential decision process that increases the detail of modeling and analysis while simultaneously decreasing the space of alternatives considered. In a decision theoretic framework, low-fidelity models help decision-makers identify regions of interest in the tradespace and cull others prior to constructing more computationally expensive models of higher fidelity. The method presented herein demonstrates design as a sequence of finite decision epochs through a search space defined by the extent of the set of designs under consideration, and the level of analytic fidelity subjected to each design. Previous work has shown that multi-fidelity modeling can aid in rapid optimization of the design space when high-fidelity models are coupled with low-fidelity models. This paper offers two contributions to the design community: (1) a model of design as a sequential decision process of refinement using progressively more accurate and expensive models, and (2) a connected approach for how conceptual models couple with detailed models. Formal definitions of the process are provided, and a simple one-dimensional example is presented to demonstrate the use of sequential multi-fidelity modeling in determining an optimal modeling selection policy.",2015
"A Mathematical Model for Pressure Compensating Emitters","This paper presents a mathematical model investigating the physics behind pressure-compensating (PC) drip irrigation emitters. A network of PC emitters, commonly known as drip irrigation, is an efficient way to deliver water to crops while increasing yield. Irrigation can provide a means for farmer to grow more sensitive, and profitable crops and help billions of small-holder farmers lift themselves out of poverty. Making drip irrigation accessible and economically viable is important for developing farmers as most face the challenges of water scarcity, declining water tables and lack of access to an electrical grid. One of the main reasons for the low adoption rate of drip irrigation in the developing world is the relatively high cost of the pumping power. It is possible to reduce this cost by reducing the required activation pressure of the emitters, while maintaining the PC behavior. The work presented here provides a guide of how design changes in the emitter could allow for a reduction in the activation pressure from 1 bar to approximately 0.1 bar. This decrease in the activation pressure of each emitter in turn decreases the system driving pressure. This reduction of driving pressure will decrease the energy need of pumping, making a solar-powered system affordable for small-acreage farmers.",2015
"Simulating Variability of Rework Cost and Market Performance Estimates in Product Redesign","When considering the redesign of an existing product, designers must consider possible engineering and marketing ramifications. Ideal changes capture a large portion of the market and have a low risk of change propagation that results in reduced cost to the manufacturer. Engineering change tools such as the Change Prediction Method and market research models such as Hierarchical Bayes Mixed Logit allow designers to estimate the cost of the redesign process and market shares of preference. Variability in the inputs of the Change Prediction Method (impact and likelihood values) results in a range of redesign cost values. Assumptions regarding model form and the randomness used in model fitting also lead to variations when estimating market performance. When the variability associated with these techniques is considered, focus should shift from a point-estimate to a region-estimate. This paper explores the region-estimate produced for proposed redesigns when considering rework cost and market share of preference.",2015
"Changing Subsystem Information Strategies Using Weighted Objectives: Increasing Robustness to Biased Information Passing","Complex system design requires managing competing objectives between many subsystems. Previous field research has demonstrated that subsystem designers may use biased information passing as a negotiation tactic and thereby reach sub-optimal system-level results due to local optimization behavior. One strategy to combat the focus on local optimization is an incentive structure that promotes system-level optimization. This paper presents a new subsystem incentive structure based on Multi-disciplinary Optimization (MDO) techniques for improving robustness of the design process to such biased information passing strategies. Results from simulations of different utility functions for a test suite of multi-objective problems quantify the system robustness to biased information passing strategies. Results show that incentivizing subsystems with this new weighted structure may decrease the error resulting from biased information passing.",2015
"The Importance of Contextual Factors in Determining the Greenhouse Gas Emission Impacts of Solar Photovoltaic Systems","Small-scale residential solar photovoltaic (PV) systems are becoming increasingly common. In some cases, governments or individual homeowners promote PV technology because of concerns about climate change and a desire to reduce global greenhouse gas emissions (GHGs). While solar PV directly emits no GHGs during use, the panels are associated with a significant amount of embedded GHG emissions, resulting from the manufacturing of the panels, for instance. A review of relevant literature reveals that the life cycle GHG emissions of solar PV panels are significantly influenced by contextual factors, such as the location of the panels during use. The purpose of this paper is to illustrate the many ways context could affect the GHG emissions associated with solar PV systems and to demonstrate — via calculations from a simple analytical model — the potential magnitude of the GHG emissions differences associated with using PV panels in different contexts.",2015
"Wave Energy Converter Array Optimization: A Review of Current Work and Preliminary Results of a Genetic Algorithm Approach Introducing Cost Factors","Currently, ocean wave energy is a novel means of electricity generation that is projected to potentially serve as a primary energy source in coastal areas. However, for wave energy converters (WECs) to be applicable on a scale that allows for grid implementation, these devices will need to be placed in close relative proximity to each other. From what’s been learned in the wind industry of the U.S., the placement of these devices will require optimization considering both cost and power. However, current research regarding optimized WEC layouts only considers the power produced. This work explores the development of a genetic algorithm (GA) that will create optimized WEC layouts where the objective function considers both the economics involved in the array’s development as well as the power generated. The WEC optimization algorithm enables the user to either constrain the number of WECs to be included in the array, or allow the algorithm to define this number. To calculate the objective function, potential arrays are evaluated using cost information from Sandia National Labs Reference Model Project, and power development is calculated such that WEC interaction affects are considered. Results are presented for multiple test scenarios and are compared to previous literature, and the implications of ",2015
"Cost Optimization of a Solar Humidification-Dehumidification Desalination System Augmented by Thermal Energy Storage","Solar-powered water desalination is one of the promising approaches for addressing fresh water scarcity in the Middle-East, North Africa, and areas of similar climate around the world. Humidification-dehumidification (HDH) is a scalable, commercially-viable technology that primarily utilizes thermal energy in order to extract fresh water from a high salinity water source. Because of inherent variability and uncertainty in solar energy availability due to daily and seasonal cycles, solar-powered HDH desalination systems may benefit from installing thermal energy storage (TES). TES can allow higher utilization of the installed system components and thus reduce the overall lifecycle cost of fresh water production. This work presents a configuration for a HDH desalination system augmented by TES. The system is optimized using Genetic Algorithms (GA) for minimum total annual cost (TAC) per unit volume of produced potable water while satisfying a preset potable water demand. The optimum results for the same location and cost function are compared with results from a previous system which does not have TES. The comparison shows a considerable reduction in potable water production cost when TES is utilized in addition to the benefit of smaller variation in water production across the day.",2015
"Solar Power Ramp Events Detection Using an Optimized Swinging Door Algorithm","Solar power ramp events (SPREs) significantly influence the integration of solar power on non-clear days and threaten the reliable and economic operation of power systems. Accurately extracting solar power ramps becomes more important with increasing levels of solar power penetrations in power systems. In this paper, we develop an optimized swinging door algorithm (OpSDA) to enhance the state of the art in SPRE detection. First, the swinging door algorithm (SDA) is utilized to segregate measured solar power generation into consecutive segments in a piecewise linear fashion. Then we use a dynamic programming approach to combine adjacent segments into significant ramps when the decision thresholds are met. In addition, the expected SPREs occurring in clear-sky solar power conditions are removed. Measured solar power data from Tucson Electric Power is used to assess the performance of the proposed methodology. OpSDA is compared to two other ramp detection methods: the SDA and the L1-Ramp Detect with Sliding Window (L1-SW) method. The statistical results show the validity and effectiveness of the proposed method. OpSDA can significantly improve the performance of the SDA, and it can perform as well as or better than L1-SW with substantially less computation time.",2015
"A Mixed Integer Linear Programming Formulation for Unrestricted Wind Farm Layout Optimization","This paper proposes a mixed integer linear programming formulation of the unrestricted wind farm layout optimization problem. The formulation is achieved in part by changing the objective from power generation maximization to the maximization of the smallest penalized downstream distance between any pair of turbines, where downstream distance is defined as the distance between any pair of turbines with overlapping wake cones. The proposed formulation also linearizes other non-linear characteristics of the unrestricted layout optimization problem such as proximity constraints and wake-cone membership. The main advantage of the proposed approach is that an optimal solution to the proposed formulation is guaranteed to be globally optimal. This is in contrast to previous approaches with non-linear formulations that do not come with such guarantees.",2015
"Adaptive Operation Decisions in Net Zero Building Clusters","In light of the growing strain on the energy grid and the increased awareness of the significant role buildings play within the energy ecosystem, the need for building operational strategies which minimize energy consumption has never been greater. One of the major hurdles impeding this realization primarily lies not in the lack of decision strategies, but in their inherent lack of adaptability. With most operational strategies partly dictated by a dynamic trio of social, economic and environmental factors which include occupant preference, energy price and weather conditions, it is important to realize and capitalize on this dynamism to open up new avenues for energy savings. This paper extends this idea by developing a dynamic optimization mechanism for Net-zero building clusters. A bi-level operation framework is presented to study the energy tradeoffs resulting from the adaptive measures adopted in response to hourly variations in energy price, energy consumption and indoor occupant comfort preferences. The experimental results verify the need for adaptive decision frameworks and demonstrate, through Pareto analysis, that the approach is capable of exploiting the energy saving opportunities made available through fluctuations in energy price and occupant comfort preferences.",2015
"Optimization of Renewable Energy Power Systems for Remote Communities","Many remote communities rely on diesel generators as their primary power source, which is expensive and harmful to the environment. Renewable energy systems, based on photovoltaics and wind turbines, present a more sustainable and potentially cost-effective option for remote communities with abundant sun and wind. Designing and implementing community-owned and operated renewable power generation alternatives for critical infrastructure such as hospitals, water sanitation, and schools is one approach towards community autonomy and resiliency. However, configuring a cost-effective and reliable renewable power system is challenging due to the many design choices to be made, the large variations in the renewable power sources, and the location specific renewable power source availability. This paper presents an optimization-based approach to aid the configuration of a solar photovoltaic (PV), wind turbine generator and lead-acid battery storage hybrid power system. The approach, implemented in MATLAB, uses a detailed time-series system model to analyze system Loss of Load Probability (LOLP) and a lifetime system cost model to analyze system cost. These models are coupled to a genetic algorithm to perform a multi-objective optimization of system reliability and cost.",2015
"Constrained Multi-Objective Wind Farm Layout Optimization: Introducing a Novel Constraint Handling Approach Based on Constraint Programming","Recently, land has been exploited extensively for onshore wind farms and turbines are frequently located in proximity to human dwellings, natural habitats, and infrastructure. This proximity has made land use constraints and noise generation and propagation matters of increasing concern for all stakeholders. Hence, wind farm layout optimization approaches should be able to consider and address these concerns. In this study, we perform a constrained multi-objective wind farm layout optimization considering energy and noise as objective functions, and considering land use constraints arising from landowner participation, environmental setbacks and proximity to existing infrastructure. The optimization problem is solved with the NSGA-II algorithm, a multi-objective, continuous variable Genetic Algorithm. A novel hybrid constraint handling tool that uses penalty functions together with Constraint Programming algorithms is introduced. This constraint handling tool performs a combination of local and global searches to find feasible solutions. After verifying the performance of the proposed constraint handling approach with a suite of test functions, it is used together with NSGA-II to optimize a set of wind farm layout optimization test cases with different number of turbines and under different levels of land availability (constraint severity). The optimization results illustrate the potential of the new constraint handling approach to outperform existing constraint handling approaches, leading to better solutions with fewer evaluations of the objective functions and constraints.",2015
"Wind Farm Layout Optimization in Complex Terrains Using Computational Fluid Dynamics","The aim of wind farm design is to maximize energy production and minimize cost. In particular, optimizing the placement of turbines in a wind farm is crucial to minimize the wake effects that impact energy production. Most work on wind farm layout optimization has focused on flat terrains and spatially uniform wind regimes. In complex terrains, however, the lack of accurate analytical wake models makes it difficult to evaluate the performance of layouts quickly and accurately as needed for optimization purposes. This paper proposes an algorithm that couples computational fluid dynamics (CFD) with mixed-integer programming (MIP) to optimize layouts in complex terrains. High-fidelity CFD simulations of wake propagation are utilized in the proposed algorithm to constantly improve the accuracy of the predicted wake effects from upstream turbines in complex terrains. By exploiting the deterministic nature of MIP layout solutions, the number of expensive CFD simulations can be reduced significantly. The proposed algorithm is demonstrated on the layout design of a wind farm domain in Carleton-sur-Mer, Quebec, Canada. Results show that the algorithm is capable of producing good wind farm layouts in complex terrains while minimizing the number of computationally expensive wake simulations.",2015
"High Penetration Residential Solar Photovoltaics and the Effects of Dust Storms on System Net Load","Residential solar photovoltaic (PV) systems are becoming increasingly common around the world. Much of this growth is attributed to a decreasing cost of solar PV modules, reduction in the cost of installation and other “soft costs,” along with net-metering, financial incentives, and the growing societal interest in low-carbon energy. Yet this steep rise in distributed, uncontrolled solar PV capacity is being met with growing concern in maintaining electric grid stability when solar PV reaches higher penetration levels. Rapid reductions in solar PV output create an immediate and direct rise in the net system load. Demand response and storage technologies can offset these fluctuations in the net system load, but their potential has yet to be realized through wide-scale commercial dissemination. In the interim these fluctuations will continue to cause technical and economic challenges to the utility and the end-user. Late-afternoon peak demands are of particular concern as solar PV drops off and household demand rises as residents return home. Transient environmental factors such as clouding, rain, and dust storms pose additional uncertainties and challenges. This study analyzes such complex cases by simulating residential loads, rooftop solar PV output, and dust storm effects on solar PV output to examine transients in the net system load. The Phoenix, Arizona metropolitan area is used as a case study that experiences dust storms several times per year. A dust storm is simulated progressing over the Phoenix metro in various directions and intensities. Various solar PV penetration rates are also simulated to allow insight into resulting net loads as PV penetration grows in future years.",2015
"Autonomous Electric Vehicle Sharing System Design","Car-sharing services promise “green” transportation systems. Two vehicle technologies offer marketable, sustainable sharing: Autonomous vehicles eliminate customer requirements for car pick-up and return, and battery electric vehicles entail zero-emissions. Designing an Autonomous Electric Vehicle (AEV) fleet must account for the relationships among fleet operations, charging station operations, electric powertrain performance, and consumer demand. This paper presents a system design optimization framework integrating four sub-system problems: Fleet size and assignment schedule; number and locations of charging stations; vehicle powertrain requirements; and service fees. A case study for an autonomous fleet operating in Ann Arbor, Michigan, is used to examine AEV sharing system profitability and feasibility for a variety of market scenarios.",2015
"A Simplified Mathematical Model for Two-Sided Market Systems With an Intervening Engineered Platform","A two-sided market involves two different user groups whose interactions are enabled over a platform that provides a distinct set of values to either side. In such market systems, one side’s participation depends on the value created by presence of the other side over the platform. Two-sided market platforms must acquire enough users on both sides in appropriate proportions to generate value to either side of the user market. In this paper, we present a simplified, generic mathematical model for two-sided markets with an intervening platform that enables interaction between the two different sets of users with distinct value propositions. The proposed model captures both the same side as well as cross-side effects (i.e., network externalities) and can capture any behavioral asymmetry between the different sides of the two-sided market system. The cross-side effects are captured using the notion of affinity curves while same side effects are captured using four rate parameters. We demonstrate the methodology on canonical affinity curves and comment on the attainment of stability at the equilibrium points of two-sided market systems. Subsequently a stochastic choice-based model of consumers and developers is described to simulate a two-sided market from grounds-up and the observed affinity curves are documented. Finally we discuss how the two-sided market model links with and impacts the engineering characteristics of the platform.",2015
"Modeling Noncompensatory Choices With a Compensatory Model for a Product Design Search","Market-based product design has typically used compensatory models that assume a simple additive part-worth rule. However, marketing literature has demonstrated that consumers use various heuristics called noncompensatory choices to simplify their choice decisions. This study aims to explore the suitability of compensatory modeling of these noncompensatory choices for the product design search. This is motivated by the limitations of the existing Bayesian-based noncompensatory mode, such as the screening rule assumptions, probabilistic representation of noncompensatory choices, and discontinuous choice probability functions in the Bayesian-based noncompensatory model. Results from using compensatory models show that noncompensatory choices can lead to distinct segments with extreme part-worths. In addition, the product design search problem suggests that the compensatory model would be preferred due to small design errors and inexpensive computational burden.",2015
"Data Driven Prognostics With Lack of Training Data Sets","Data-driven prognostics typically requires sufficient offline training data sets for accurate remaining useful life (RUL) prediction of engineering products. This paper investigates performances of typical data-driven methodologies when the amount of training data sets is insufficient. The purpose is to better understand these methodologies especially when offline training datasets are insufficient. The neural network, similarity-based approach, and copula-based sampling approach were investigated when only three run-to-failure training units were available. The example of lithium-ion (Li-ion) battery capacity degradation was employed for the demonstration.",2015
"Diagnostics and Prognostics of Lithium-Ion Batteries","This paper investigates recent research on battery diagnostics and prognostics especially for Lithium-ion (Li-ion) batteries. Battery diagnostics focuses on battery models and diagnosis algorithms for battery state of charge (SOC) and state of health (SOH) estimation. Battery prognostics elaborates data-driven prognosis algorithms for predicting the remaining useful life (RUL) of battery SOC and SOH. Readers will learn not only basics but also very recent research developments on battery diagnostics and prognostics.",2015
"Online Estimation of Lithium-Ion Battery Capacity Using Sparse Bayesian Learning","Lithium-ion (Li-ion) rechargeable batteries are used as one of the major energy storage components for implantable medical devices. Reliability of Li-ion batteries used in these devices has been recognized as of high importance from a broad range of stakeholders, including medical device manufacturers, regulatory agencies, patients and physicians. To ensure a Li-ion battery operates reliably, it is important to develop health monitoring techniques that accurately estimate the capacity of the battery throughout its life-time. This paper presents a sparse Bayesian learning method that utilizes the charge voltage and current measurements to estimate the capacity of a Li-ion battery used in an implantable medical device. Relevance Vector Machine (RVM) is employed as a probabilistic kernel regression method to learn the complex dependency of the battery capacity on the characteristic features that are extracted from the charge voltage and current measurements. Owing to the sparsity property of RVM, the proposed method generates a reduced-scale regression model that consumes only a small fraction of the CPU time required by a full-scale model, which makes online capacity estimation computationally efficient. 10 years’ continuous cycling data and post-explant cycling data obtained from Li-ion prismatic cells are used to verify the performance of the proposed method.",2015
"Resilience Analysis and Allocation for Complex Systems Using Bayesian Network","The concept of resilience has been explored in diverse disciplines. However, there are only a few which focus on how to quantitatively measure engineering resilience and allocate resilience in engineering system design. This paper is dedicated to exploring the gap between quantitative and qualitative assessments of engineering resilience in the domain of designing complex engineered systems, thus optimally allocating resilience into subsystems and components level in industrial applications. A conceptual framework is first proposed for modeling engineering resilience, and then Bayesian Network is employed as a quantitative tool for the assessment and analysis of engineering resilience for complex systems. One industrial-based case study, a supply chain system, is employed to demonstrate the proposed approach. The proposed resilience quantification and allocation approach using Bayesian Networks would empower system designers to have a better grasp of the weakness and strength of their own systems against system disruptions induced by adverse failure events.",2015
"A Generic Fusion Platform of Failure Diagnostics for Resilient Engineering System Design","Effective health diagnostics provides benefits such as improved safety, improved reliability, and reduced costs for the operation and maintenance of complex engineered systems. This paper presents a multi-attribute classification fusion approach which leverages the strengths provided by multiple membership classifiers to form a robust classification model for structural health diagnostics. The developed classification fusion approach conducts the health diagnostics with three primary stages: (i) fusion formulation using a k-fold cross validation model; (ii) diagnostics with multiple multi-attribute classifiers as member algorithms; and (iii) classification fusion through a weighted majority voting with dominance system. State-of-the-art classification techniques from three broad categories (i.e., supervised learning, unsupervised learning, and statistical inference) are employed as the member algorithms. The developed classification fusion approach is demonstrated with the 2008 PHM challenge problem. The developed fusion diagnostics approach outperforms any stand-alone member algorithm with better diagnostic accuracy and robustness.",2015
"Toward Customer Needs Cultural Risk Indicator Insights for Product Development","As the availability and affordability of consumer products continues to increase around the world, consumers — especially those in developing countries and living on less than $10/day — will express more discerning in their tastes and preferences. Design teams have already been operating in design for the developing world contexts for many years and more are moving into the arena on a regular basis. Many designers do not have cultural knowledge of the customers cultures they are designing for. Cultural ignorance can lead to misinterpretation of customer needs that can lead to products that do not satisfy customer needs and results in disappointed customers, low sales figures, and a frustrated design team. The Customer Needs Cultural Risk Indicator (CNCRI) method introduced in this paper provides a method for design teams to rapidly analyze customer needs for “Risk Indicators” in customer needs based upon cultural differences between the customers and the design team. By understanding early on in the design process where a lack of cultural knowledge may be a risk to the design, the design team can make informed decisions on how to satisfy customer needs effectively.",2015
"Justification, Design, and Analysis of a Village-Scale Photovoltaic-Powered Electrodialysis Reversal System for Rural India","This paper presents the merits of village-scale photovoltaic (PV) powered electrodialysis reversal (EDR) systems for rural India and the design and analysis of such a system built by the authors with planned testing to be completed in March 2015 in Alamogordo, New Mexico. The requirements for the system include daily water output of 6–15 m",2015
"Modeling Technology Strategies for Thermal Energy Services in Rural Developing Communities","Approximately 40% of the world’s population lives in energy poverty, lacking basic clean energy to prepare their food, heat water for washing, and provide light in their homes. Access to improved energy services can help to alleviate this poverty and result in significant improvements to health and livelihoods, yet past strategies for meeting the needs of this large and diverse population have often been top-down and focused on single intervention or solution, leading to limited success. Using a systems-based approach to examine residential thermal energy needs, this paper explores five intervention strategies to provide energy services for a remote off-grid village in Mali. The five intervention strategies are (1) general improved biomass cookstoves, (2) advanced biomass cookstoves, (3) communal biomass cookstoves, (4) LPG cookstoves, and (5) solar water heaters. Using a probabilistic multi-objective model that includes technical, environmental, economic, and social objectives, the potential net improvements, critical factors, and sensitivities are investigated. The results show that the factors with the most impact on the outcome of an intervention include the rate of user adoption, value of time, and biomass harvest renewability; in contrast, parameters such as cookstove emission factors have less impact on the outcome. This suggests that the focus of village energy research and development should shift to the design of technologies that have high user adoption rates. That is, the results of this study support the hypothesis that the most effective village energy strategy is one that reinforces the natural user-driven process to move toward efficient and convenient energy services.",2015
"A More Balanced Design Approach for Preserving the Usability of a Peruvian Cookstove","Over the past decade, a large amount of research has been dedicated to improving the efficiency and reducing the emissions of biomass cookstoves. The trade-off from placing such an emphasis on these two objectives is that improved cookstoves are often not as functional or desirable to the end user in comparison to their traditional cookstove. Thus, users often abandon their new improved cookstoves and sustained use is not achieved. In order for improved cookstoves to be more impactful, a different design approach is needed; improved cookstoves must be designed for usability, even at the expense of higher efficiencies or lower emissions. This paper explores the benefits of this alternative approach, which is demonstrated in the design of a replacement biomass cookstove for residents living in the Tambogrande region of Peru. The heavy use of biomass cookstoves in this small collection of villages, has resulted in many health and environmental problems for the residents. Recent field studies revealed that residents were pleased with the functionality of their traditional channel stove, yet also desired to have a stove that cooks faster, consumes less fuel, and emits less smoke. The resulting design includes a set of adaptable, inexpensive pot skirts that can be integrated with their current channel stove. These pot skirts allow for varying sizes and number of pots, as well as allow traditional fuels to be used. Despite a usability focused design approach, the pot skirts still improved the technical performance of the cookstove by improving thermal efficiency by 25.8%, decreasing time to boil by 26.0%, and decreasing fuel consumption by 24.7%. These results demonstrate that a usability focused design can still yield significant performance improvements while achieving a high level of user functionality.",2015
"Feasibility Study of an Electrodialysis System for In-Home Water Desalination and Purification in Urban India","Desalination of high salinity water is an effective way of improving the aesthetic quality of drinking water and has been demonstrated to be a characteristic valued by consumers. Across India, 60% of the groundwater, the primary water source for millions, is brackish or contains a high salt content with total dissolved solids (TDS) ranging from 500 parts per million (ppm) to 3,000ppm. The government does not provide sufficient desalination treatment before the water reaches the tap of a consumer. Therefore consumers have turned to in-home desalination. However, current products are either expensive or have low recovery, product water output per untreated feed water, (∼30%) wasting water resources. Electrodialysis (ED) is a promising technology that desalinates water while maintaining higher recovery (up to 95%) compared to existing consumer reverse osmosis (RO) products. This paper first explores the in-home desalination market to determine critical design requirements for an in-home ED system. A model was then used to evaluate and optimize the performance of an ED stack at this scale and designated salinity range. Additionally, testing was conducted in order to validate the model and demonstrate feasibility. Finally, cost estimates of the proposed in-home ED system and product design concept are presented. The results of this work identified a system design that provides consumers with up to 80% recovery of feed water with cost and size competitive to currently available in-home RO products.",2015
"Development of Biomass Energy Technologies and Business Models for Southern Africa","This study evaluates options for biomass pellet formulations and business models to create a sustainable energy solution for cooking energy in Southern Africa. Various agricultural wastes and agro-processing wastes are investigated to meet industry standards on biomass pellet quality. These fuels are obtained from farms and facilities across a geographic area that affects the end-cost of the pellet through transportation costs and the cost of the biomass. The technical performance of the pellet and cost of the pellet are first contrasted and then optimized in unison to develop sustainable energy options that can provide year-round clean energy for household cooking and heating needs. A market was analyzed using wheat, sugarcane and maize crops as components for the biomass pellet fuel source in the Zululand district of South Africa. Using a target moisture content (MC",2015
"Non-Negative Matrix Factorization Based Uncertainty Quantification Method for Complex Networked Systems","The behavior of large networked systems with underlying complex nonlinear dynamic are hard to predict. With increasing number of states, the problem becomes even harder. Quantifying uncertainty in such systems by conventional methods requires high computational time and the accuracy obtained in estimating the state variables can also be low. This paper presents a novel computational Uncertainty Quantifying (UQ) method for complex networked systems. Our approach is to represent the complex systems as networks (graphs) whose nodes represent the dynamical units, and whose links stand for the interactions between them. First, we apply Non-negative Matrix Factorization (NMF) based decomposition method to partition the domain of the dynamical system into clusters, such that the inter-cluster interaction is minimized and the intra-cluster interaction is maximized. The decomposition method takes into account the dynamics of individual nodes to perform system decomposition. Initial validation results on two well-known dynamical systems have been performed. The validation results show that uncertainty propagation error quantified by RMS errors obtained through our algorithms are competitive or often better, compared to existing methods.",2015
"Autonomous Microgrid Design Using Classifier-Guided Sampling","Identifying high-performance, system-level microgrid designs is a significant challenge due to the overwhelming array of possible configurations. Uncertainty relating to loads, utility outages, renewable generation, and fossil generator reliability further complicates this design problem. In this paper, the performance of a candidate microgrid design is assessed by running a discrete event simulation that includes extended, unplanned utility outages during which microgrid performance statistics are computed. Uncertainty is addressed by simulating long operating times and computing average performance over many stochastic outage scenarios. Classifier-guided sampling, a Bayesian classifier-based optimization algorithm for computationally expensive design problems, is used to search and identify configurations that result in reduced average load not served while not exceeding a predetermined microgrid construction cost. The city of Hoboken, NJ, which sustained a severe outage following Hurricane Sandy in October, 2012, is used as an example of a location in which a well-designed microgrid could be of great benefit during an extended, unplanned utility outage. The optimization results illuminate design trends and provide insights into the traits of high-performance configurations.",2015
"Toward a Dedicated Failure Flow Arrestor Function Methodology","Risk analysis in engineering design is of paramount importance when developing complex systems or upgrading existing systems. In many complex systems, new generations of systems are expected to have decreased risk and increased reliability when compared with previous designs. For instance, within the American civilian nuclear power industry, the Nuclear Regulatory Commission (NRC) has progressively increased requirements for reliability and driven down the chance of radiological release beyond the plant site boundary. However, many ongoing complex system design efforts analyze risk after early major architecture decisions have been made. One promising method of bringing risk considerations earlier into the conceptual stages of the complex system design process is functional failure modeling. Function Failure Identification and Propagation (FFIP) and related methods began the push toward assessing risk using the functional modeling taxonomy. This paper advances the Dedicated Failure Flow Arrestor Function (DFFAF) method which incorporates dedicated Arrestor Functions (AFs) whose purpose is to stop failure flows from propagating along uncoupled failure flow pathways, as defined by Uncoupled Failure Flow State Reasoner (UFFSR). By doing this, DFFAF provides a new tool to the functional failure modeling toolbox for complex system engineers. This paper introduces DFFAF and provides an illustrative simplified civilian Pressurized Water Reactor (PWR) nuclear power plant case study.",2015
"Toward a Functional Failure Modeling Method of Representing Prognostic Systems During the Early Phases of Design","Current methods of functional failure risk analysis do not facilitate explicit modeling of systems equipped with Prognostics and Health Management (PHM) hardware. As PHM systems continue to grow in application and popularity within major complex systems industries (e.g. aerospace, automotive, civilian nuclear power plants), implementation of PHM modeling within the functional failure modeling methodologies will become useful for the early phases of complex system design and for analysis of existing complex systems. Functional failure modeling methods have been developed in recent years to assess risk in the early phases of complex system design. However, the methods of functional modeling have yet to include an explicit method for analyzing the effects of PHM systems on system failure probabilities. It is common practice within the systems health monitoring industry to design the PHM subsystems during the later stages of system design — typically after most major system architecture decisions have been made. This practice lends itself to the omission of considering PHM effects on the system during the early stages of design. This paper proposes a new method for analyzing PHM subsystems’ contribution to risk reduction in the early stages of complex system design. The Prognostic Systems Variable Configuration Comparison (PSVCC) eight-step method developed here expands upon existing methods of functional failure modeling by explicitly representing PHM subsystems. A generic pressurized water nuclear reactor primary coolant loop system is presented as a case study to illustrate the proposed method. The success of the proposed method promises more accurate modeling of complex systems equipped with PHM subsystems in the early phases of design.",2015
"A Multi-Objective Simulated Annealing Approach Towards 3D Packing Problems With Strong Constraints: CMOSA","This research concerns the packing problem frequently encountered in engineering design, where the volume and weight of a number of structural components such as valves and plumbing lines need to be minimized. Since in real applications the constraints are usually complex, the formulation of computationally tractable optimization becomes challenging. In this research, we propose a novel multiobjective simulated annealing (MOSA) approach towards the design optimization, i.e., optimizing the placement of valves under prescribed constraints to minimize the volume occupied, and the estimated plumbing line length. The objectives and constraints are described by analytical expressions. Our case study indicates that the new MOSA algorithm has relatively better performance towards 3D packing with strong constraints and the design can indeed be automated. The outcome of this research may benefit both existing manufacturing practice and future additive manufacturing.",2015
"B-Spline Based Robust Topology Optimization","In this paper, we present an extension of the B-spline based density representation to a robust formulation of topology optimization. In our B-spline based topology optimization approach, we use separate representations for material density distribution and analysis. B-splines are used as a representation of density and the usual finite elements are used for analysis. The density undergoes a Heaviside projection to reduce the grayness in the optimized structures. To ensure minimal length control so the resulting designs are robust with respect to manufacturing imprecision, we adopt a three-structure formulation during the optimization. That is, dilated, intermediate and eroded designs are used in the optimization formulation. We give an analytical description of minimal length of features in optimized designs. Numerical examples have been implemented on three common topology optimization problems: minimal compliance, heat conduction and compliant mechanism. They demonstrate that the proposed approach is effective in generating designs with crisp black/white transition and is accurate in minimal length control.",2015
"Exploring the Design Set Points of Refining Operation in Ladle for Cost Effective Desulfurization and Inclusion Removal","This paper is motivated by a need identified by steel makers, namely, the need to produce steel products with new and often more stringent set of specifications and enhanced performances (such as fatigue life and corrosion behavior) using existing equipment cost-effectively.",2015
"Numerical Methods for the Design of Meso-Structures: A Comparative Review","Over the past decade, there has been an increase in the intentional design of meso-structured materials that are optimized to target desired material properties. This paper reviews and critically compares common numerical methodologies and optimization techniques used to design these meso-structures by analyzing the methods themselves and published applications and results. Most of the reviewed research targets mechanical material properties, including effective stiffness and crushing energy absorption. The numerical methodologies reviewed include topology and size/shape optimization methods such as homogenization, Solid Isotropic Material with Penalization, and level sets. The optimization techniques reviewed include genetic algorithms (GAs), particle swarm optimization (PSO), gradient based, and exhaustive search methods. The research reviewed shows notable patterns. The literature reveals a push to apply topology optimization in an ever-growing number of 3-dimensional applications. Additionally, researchers are beginning to apply topology optimization and size/shape optimization to multiphysics problems. The research also shows notable gaps. Although PSOs are comparable evolutionary algorithms to GAs, the use of GAs dominates over PSOs. These patterns and gaps, along with others, are discussed in terms of possible future research in the design of meso-structured materials.",2015
"Towards Nonlinear Multimaterial Topology Optimization Using Unsupervised Machine Learning and Metamodel-Based Optimization","This work introduces a multimaterial density-based topology optimization method suitable for nonlinear structural problems. The proposed method consists of three stages: continuous density distribution, clustering, and metamodel-based optimization. The initial continuous density distribution is generated following a synthesis strategy without penalization, e.g., the hybrid cellular automaton (HCA) method. In the clustering stage, unsupervised machine learning (e.g., K-means clustering) is used to optimally classify the continuous density distribution into a finite number of clusters based on their similarity. Finally, a metamodel (e.g., Kriging interpolation) is generated and iteratively updated following a global optimization algorithm (e.g., genetic algorithms) to ultimately converge to an optimal material distribution. The proposed methodology is demonstrated with the design of multimaterial stiff (minimum compliance) structures, compliant mechanisms, and a thin-walled S-rail structure for crashworthiness.",2015
"Organizing Cells Within Non-Periodic Microarchitectured Materials That Achieve Graded Thermal Expansions","The aim of this paper is to introduce an approach for optimally organizing a variety of different unit cell designs within a large lattice such that the bulk behavior of the lattice exhibits a desired Young’s modulus with a graded change in thermal expansion over its geometry. This lattice, called a graded microarchitectured material, can be sandwiched between two other materials with different thermal expansion coefficients to accommodate their different expansions or contractions caused by changing temperature while achieving a desired uniform stiffness. First, this paper provides the theory necessary to calculate the thermal expansion and Young’s modulus of large multi-material lattices that consist of periodic (i.e., repeating) unit cells of the same design. Then it introduces the theory for calculating the graded thermal expansions of a large multimaterial lattice that consists of non-periodic unit cells of different designs. An approach is then provided for optimally designing and organizing different unit cells within a lattice such that both of its ends achieve the same thermal expansion as the two materials between which the lattice is sandwiched. A MATLAB tool is used to generate images of the undeformed and deformed lattices to verify their behavior and various examples are provided as case studies. The theory provided is also verified and validated using finite element analysis and experimentation.",2015
"Polytope Sector-Based Synthesis and Analysis of Microarchitectured Materials With Tunable Thermal Conductivity and Expansion","The aim of this paper is to (1) introduce an approach, called Polytope Sector-based Synthesis, for synthesizing 2D or 3D microstructural architectures that exhibit a desired bulk-property directionality (e.g., isotropic, cubic, orthotropic, etc.), and (2) provide general analytical methods that can be used to rapidly optimize the geometric parameters of these architectures such that they achieve a desired combination of bulk thermal conductivity and thermal expansion properties. Although the methods introduced can be applied to general beam-based microstructural architectures, we demonstrate their utility in the context of an architecture that can be tuned to achieve a large range of extreme thermal expansion coefficients — positive, zero, and negative. The material-property-combination region that can be achieved by this architecture is determined within an Ashby-material-property plot of thermal expansion vs. thermal conductivity using the analytical methods introduced. Both 2D and 3D versions of the design have been fabricated using projection microstereolithography.",2015
"A Set-Based Design Method for Material-Geometry Structures by Design Space Mapping","The objective of this work is a multiscale, set-based design method for mechanical components and their manufacturing processes and materials with interactive identification of feasible design regions. A unique aspect of the proposed method is the ability to adjust both material properties, through process planning, and part geometry, through exploration of various cellular structures (e.g., lattices, honeycombs), in order to achieve design goals. More specifically, the proposed design method can effectively explore the achievement of desired mechanical properties by controlling process conditions, material properties, and/or selecting appropriate feature configurations. Material process-property relationships and cellular structure structure-property relationships are modeled to create a comprehensive multiscale design space mapping database. Bayesian network classifiers enable real-time “inverse mapping” that identify feasible regions of the unit cell, feature, and process spaces, thus providing immediate feedback to the designer regarding the feasibility of the current component design. The design method is demonstrated on a class of mechanical components that are manufactured by electron-beam melting (EBM) in Ti-6Al-4V.",2015
"Generalized Viscoelastic Material Design With Integro-Differential Equations and Direct Optimal Control","Rheological material properties are examples of function-valued quantities that depend on frequency (linear viscoelasticity), input amplitude (nonlinear material behavior), or both. This dependence complicates the process of utilizing these systems in engineering design. In this article, we present a methodology to model and optimize design targets for such rheological material functions. We show that for linear viscoelastic systems simple engineering design assumptions can be relaxed from a conventional spring-dashpot model to a more general linear viscoelastic relaxation kernel, K(t). While this approach expands the design space and connects system-level performance with optimal material design functions, it entails significant numerical difficulties. Namely, the associated governing equations involve a convolution integral, thus forming a system of integro-differential equations. This complication has two important consequences: 1) the equations representing the dynamic system cannot be written in a standard state space form as the time derivative function depends on the entire past state history, and 2) the dependence on prior time-history increases time derivative function computational expense. Previous studies simplified this process by incorporating parameterizations of K(t) using viscoelastic models such as Maxwell or critical gel models. While these simplifications support efficient solution, they limit the type of viscoelastic materials that can be designed. This article introduces a more general approach that can explore arbitrary K(t) designs using direct optimal control methods. In this study, we analyze a nested direct optimal control approach to optimize linear viscoelastic systems with no restrictions on K(t). The study provides new insights into efficient optimization of systems modeled using integro-differential equations. The case study is based on a passive vibration isolator design problem. The resulting optimal K(t) functions can be viewed as early-stage design targets that are material agnostic and allow for creative material design solutions. These targets may be used for either material-specific selection or as targets for later-stage design of novel materials.",2015
"Heterogeneous Material Design Using a PCA-Based Microstructure Representing Method","A PCA (Principle Component Analysis)-based microstructure descriptor is proposed in this paper. The PCA process is performed on the microstructures of the material database, and is able to maintain the dominant geometric features using a small number of PCA basis. Moreover, this PCA-based descriptor also gives a closed-form expression between the microstructures within the design space and their physical properties. Previous approaches, for example, based on meta-modeling, involve much more expensive off-line finite element computations. Using this novel PCA-based descriptor, an approach to design a microstructure of optimal physical performance is proposed, taking the concrete case of linear elasticity analysis for demonstration.",2015
"A Geometry Projection Method for the Optimal Distribution of Short Fiber Reinforcements","This paper presents an optimization method for optimally distributing short fibers of variable length for the reinforcement of structural components for stiffness. Unlike standard density-based and level set topology optimization methods that generally render material distributions with variable member size, the proposed method projects an explicit geometry model onto a continuous density field. The proposed method inherits the benefits of density-based topology optimization methods, namely simplified and efficient primal and sensitivity analyses on a fixed grid, fast convergence, robustness, and amenability to standard finite element methods for the analysis and to nonlinear programming algorithms for the optimization. The explicit geometry representation of the fibers provides a suitable description of the short fibers, and therefore the designs produced by the proposed method have potential for manufacturing using, for example, processing methods for the fabrication of micro-architectured materials. Examples of reinforcement distribution design for two-dimensional structures in plane stress demonstrate the method.",2015
"A Structural Equation Modeling Based Approach for Identifying Key Descriptors in Microstructural Materials Design","In design of advanced heterogeneous materials system, microstructures play an important role as a link between processing and material properties. An accurate and efficient representation of material microstructures is necessary. Our prior work applied a supervised ranking algorithm to identify key microstructure descriptors, however the approach falls short in identifying redundancy in descriptors and is not reliable when the training sample size is small. In this paper, we propose a Structural Equation Modeling (SEM) based approach to identify significant microstructure descriptors based on either correlation functions (CF) or material properties, or both. By building a reflective structural model, we are able to deal with high correlations among all candidate descriptors, gain more insights into their relations, and identify latent factors for categorizing microstructure features. The proposed approach begins with an Exploratory Factor Analysis (EFA) for grouping and reducing descriptors to determine the proper structure of microstructure descriptors as indicators of latent factors. The SEM analysis is then applied to identify the key descriptors using the Partial Least Squares (PLS) algorithm. The nanodielectric system with epoxy-nanosilica is used as an example to illustrate and validate the proposed approach. The potential use of identified key microstructure descriptors for optimal design of microstructural materials is discussed.",2015
"On Design of Mechanical Metamaterials Using Level-Set Based Topology Optimization","Recently, engineered metamaterials have aroused considerable interests in many fields, due to their distinctive characteristics different from conventional materials. This paper proposes a level-set based topological optimization method for design of the mechanical metamaterials with negative Poisson’s ratio. The strain energy method is employed to predict the effective properties of the periodic microstructure. The adjoint variable method is applied to calculate the shape sensitivity, which is further used to construct the design velocity field. Finally, the level set method is applied to achieve shape evolution and topological changes of the microstructure until the desired materials properties like negative Poisson’s ratios are achieved.",2015
"Performance Comparison of a Bulk Thermoelectric Cooler With a Hybrid Device Architecture","This paper compares the economic viability and performance outcomes of two different thermoelectric device architectures to determine the advantages and appropriate use of each configuration. Hybrid thermoelectric coolers employ thin-film thermoelectric materials sandwiched between a plastic substrate and formed into a corrugated structure. Roll-to-roll manufacturing and low-cost polymer materials offer a cost advantage to the hybrid architecture at the sacrifice of performance capabilities while conventional bulk devices offer increased performance at a higher cost. Performance characteristics and cost information are developed for both hybrid and conventional bulk single-stage thermoelectric modules. The design variables include device geometry, electrical current input, and thermoelectric material type. The trade-offs between cooling performance and cost will be explored and the thermoelectric system configuration analyzed for both hybrid and conventional bulk thermoelectric coolers.",2015
"Topology Optimization of Cellular Materials With Maximized Energy Absorption","While topology optimization is typically employed for design at the component-level scale, it is increasingly being used to design the topology of high performance cellular materials. The design problem is posed as an optimization problem with governing unit cell and upscaling mechanics embedded in the formulation, and solved with formal mathematical programming. While design for linear elastic properties is generally well-established, this paper will discuss including nonlinear mechanics in the topology optimization formulation, also in the domain of cellular materials. In particular, the problem of maximizing total energy absorption of a cellular Bulk Metallic Glass material is considered and numerical and experimental analyses of the new design show that it has enhanced performance compared to conventional cellular topologies.",2015
"Tensegrity Form-Finding Using Generative Design Synthesis Approach","The aim of this research is to produce large irregular tensegrity structures using a generative design synthesis approach. Unlike most of the form-finding methods, the approach does not require the description of the connectivity of the tensegrity structures to define the shape of the tensegrities. It uses graphs to represent the tensegrity structures, which allows a very fast generation of stable tensegrity solutions for a given design problem. The graph grammar interpreter, GraphSynth, is used to carry out graph transformations, which define different configurations for a given design problem. After generating the graphs, they are transformed into meaningful 3D shapes. The effectiveness and robustness of the proposed method is checked by solving a variety of test problems.",2015
"Hierarchical Primitive Surface Classification From Triangulated Solids for Defining Part-to-Part Degrees of Freedom","Mesh segmentation is the process of organizing a set of data points into connected areas defined by known surface primitives or surface equations. Our approach is a hierarchical method based on clustering and Mamdani’s fuzzification system. First, a clustering algorithm is used to isolate regions of small and irregular oriented triangles, which make up a large portion of the total polygonal faces. Then, using fuzzification rule sets, the remaining triangles are made into meaningful primitives: cylinder, cone, sphere and flat. The end goal of this work is to use the primitives to define the degrees of freedom between mating parts in an assembly when such information is unknown or lost. The result of this process has proven to be accurate with well-defined borders with no need of additional post-processing steps.",2015
"Spiral Toolpaths for High-Speed Machining of 2D Pockets With or Without Islands","We describe new methods for the construction of spiral toolpaths for high-speed machining. In the simplest case, our method takes a polygon as input and a number δ > 0 and returns a spiral starting at a central point in the polygon, going around towards the boundary while morphing to the shape of the polygon. The spiral consists of linear segments and circular arcs, it is G",2015
"Multi-Sensor Reverse Engineering Technique for the Acquisition of Centrifugal Pump Impellers","The geometrical reconstruction of centrifugal pump impellers is a strategic activity for many manufacturing industries. In particular, the digitalization of internal hydraulic shapes represents the most critical task due to the difficulties in accessing the internal parts of impeller disks.",2015
"Dynamic Design Using the Kalman Filter for Flexible Systems With Epistemic Uncertainty","Most engineered systems have to exhibit a high degree of reliability and robustness. They are high in cost and complexity and often incorporate highly sophisticated materials, components, design and other technologies. Therefore, they face uncertainties in categories ranging from technical issues to market changes. This includes a wide range of epistemic uncertainties, such as demand or budget uncertainty; due to increasingly dynamic markets it has become important for systems to cope with these uncertainties. In this paper, a Kalman filter approach is applied to control the design as uncertainties are resolved in a discrete time frame. It is shown how the Kalman filter approach treats the design as a stochastic control problem, in which the design is controlled throughout its lifecycle to compensate for sources of epistemic uncertainty, as the uncertainties are resolved. The proposed method is applicable to flexible systems where changing the design is possible. A design framework is proposed encompassing a set of definitions, metrics, the methodology, and a case study of a spaceborne system.",2015
"Investigating the Heterogeneity of Product Feature Preferences Mined Using Online Product Data Streams","This work investigates the “must have” and “deal breaker” product feature preferences expressed by users of online platforms (e.g., customer review websites or social media networks) in order to inform designers of product features that should be investigated during the next iteration of a product’s launch. Existing design literature highlights the risks of aggregating group preferences, and suggest that design teams should instead, focus on maximizing enterprise value by optimizing the attributes of a product. However, design knowledge about products and product attributes are influenced by market information, which is dynamic and difficult to acquire. The use of online product review platforms has emerged in the design community as a viable source of product data acquisition and demand model prediction. However, as the heterogeneity of product preferences increases, so does the complexity of understanding which product attributes should be optimized by the design team to maximize enterprise value. These challenges are exacerbated in product preference acquisition techniques that rely on mining online data, as the customer is typically unknown to the designer, which limits the amount of follow up data available to be mined. By quantifying the degree of “must have” and “deal breaker” product preferences expressed online, designers will be able to understand what product-features should be omitted from next generation product design optimization models (i.e., “deal breaker” features) and what product features should be considered (i.e., “must have” features). A case study involving customer electronics mined from online customer review websites is used to demonstrate the validity of the proposed methodology.",2015
"Using Multivariate Analysis to Select Accommodation Boundary Manikins From a Population Database","Digital Human Models (DHMs) are a tool that can be used to aid in determining dimensions for human-centered designs. DHMs have the ability to represent the anthropometric extremes of the population and help to determine which dimensions should be used to acquire a certain level of accommodation within a population. It is not possible to use current techniques for selecting manikins that represent a population, like principal component analysis (PCA), the application of design families, or percentiles due to these methods having a lower output accommodation levels than expected. The purpose of this research is to provide a multivariate analysis based on Pareto optimization. This method determines a pool of manikins representing the total target population when comparing up to three anthropometric dimensions within a database. This pool will act as boundary manikins for a given level of accommodation.",2015
"Combining Anthropometric Data and Consumer Review Content to Inform Design for Human Variability","In this paper, we strive to combine concepts from the field of ergonomics with techniques from design analytics in order to inform the specifications of a product, such that a designer can create products that are both universal and well-received by consumers. While the ergonomic guidelines dictated by anthropometric data can provide a broad target range for design features, consumer reviews of previous iterations of similar products can narrow the design further, within that given range, to pinpoint a more effective design. We establish a general, ergonomically-centered set of cue-phrases to identify and extract useful information from online reviews, after which this information can be coupled with anthropometric measurements to produce more accurate product dimensions and features. A new metric, the Frequency and Accuracy Summation (FAS) Number is also introduced, as a means to predict the likelihood that a cue-phrase will yield useful results. The usefulness of these cue-phrases as well as the utility of combining consumer review content with anthropometric data sets are then tested in a case study centered on the design of a new and more universal “ear bud” headphone. Using the established cue-phrases, reviews of an existing product are processed such that problem components could be targeted in the new design. The information that was gleaned from those reviews is then paired with actual human ear measurements to propose a new, hypothetical product design that is more ergonomically sound and universal.",2015
"Metamodel Uncertainty Quantification by Using Bayes’ Theorem","In complex engineering systems, approximation models, also called metamodels, are extensively constructed to replace the computationally expensive simulation and analysis codes. With different sample data and metamodeling methods, different metamodels can be constructed to describe the behavior of an engineering system. Then, metamodel uncertainty will arise from selecting the best metamodel from a set of alternative ones. In this study, a method based on Bayes’ theorem is used to quantify this metamodel uncertainty. With some mathematical examples, metamodels are built by six metamodeling methods, i.e., polynomial response surface, locally weighted polynomials (LWP), k-nearest neighbors (KNN), radial basis functions (RBF), multivariate adaptive regression splines (MARS), and kriging methods, and under four sampling methods, i.e., parameter study (PS), Latin hypercube sampling (LHS), optimal LHS and full factorial design (FFD) methods. The uncertainty of metamodels created by different metamodeling methods and under different sampling methods is quantified to demonstrate the process of implementing the method.",2015
"On Using Adaptive Surrogate Modeling in Design for Efficient Fluid Power","In the last several decades fluid power has been used extensively in diverse industries such as agriculture, construction, marine, offshore resource extraction, and even entertainment. With a vast and ever-increasing spectrum of potential applications, the design of efficient and leak-free components in fluid power systems has become essential. Previous experiments and studies have shown that the use of microtextured surfaces in hydraulic components achieves performance enhancement by reducing friction and leakage. This article aims to build on this recent work through a systematic optimization-based study of performance improvement through microtexture surface design. These studies evaluate the potential of Newtonian fluid properties, coupled with varying surface features, to achieve design objectives for efficiency. This early-stage design strategy aims to find optimal surface features that minimize apparent fluid viscosity (low friction) and the area of the microtexture. The resulting multi-objective optimization (MOO) problem involves a computationally intensive simulation of the system based on computational fluid dynamics (CFD). As a strategy to reduce overall computational expense, this paper describes the development of a new adaptive surrogate modeling strategy for multi-objective optimization. Two case studies are presented: a simple analytical case study illustrating the details of the method and a more sophisticated case study involving the two-dimensional CFD simulation of Newtonian fluids on symmetric surface textures. This design approach embraces the potential of using rheologically complex fluids in engineering system design and optimization. This study can be further extended to a more generalized problem by coupling both fluid and geometrical design decisions.",2015
"Variable-Fidelity Optimization With In-Situ Surrogate Model Refinement","Owing to the typical low fidelity of surrogate models, it is often challenging to accomplish reliable optimum solutions in Surrogate-Based Optimization (SBO) for complex nonlinear problems. This paper addresses this challenge by developing a new model-independent approach to refine the surrogate model during optimization, with the objective to maintain a desired level of fidelity and robustness “where” and “when” needed. The proposed approach, called Adaptive Model Refinement (AMR), is designed to work particularly with population-based optimization algorithms. In AMR, reconstruction of the model is performed by sequentially adding a batch of new samples at any given iteration (of SBO) when a refinement metric is met. This metric is formulated by comparing (1) the uncertainty associated with the outputs of the current model, and (2) the distribution of the latest fitness function improvement over the population of candidate designs. Whenever the model-refinement metric is met, the history of the fitness function improvement is used to determine the desired fidelity for the upcoming iterations of SBO. Predictive Estimation of Model Fidelity (an advanced surrogate model error metric) is applied to determine the model uncertainty and the batch size for the samples to be added. The location of the new samples in the input space is determined based on a hypercube enclosing promising candidate designs, and a distance-based criterion that minimizes the correlation between the current sample points and the new points. The powerful mixed-discrete PSO algorithm is used in conjunction with different surrogate models (e.g., Kriging, RBF, SVR) to apply the new AMR method. The performance of the proposed AMR-based SBO is investigated through three different benchmark functions.",2015
"Optimization on Metamodeling-Supported Iterative Decomposition","The recently developed metamodel-based decomposition strategy relies on quantifying the variable correlations of black-box functions so that high dimensional problems are decomposed to smaller sub-problems, before performing optimization. Such a two-step method may miss the global optimum due to its rigidity or requires extra expensive sample points for ensuring adequate decomposition. This work develops a strategy to iteratively decompose high dimensional problems within the optimization process. The sample points used during the optimization are reused to build a metamodel called PCA-HDMR for quantifying the intensities of variable correlations by sensitivity analysis. At every iteration, the predicted intensities of the correlations are updated based on all the evaluated points and a new decomposition scheme is suggested by omitting the weak correlations. Optimization is performed on the iteratively updated sub-problems from decomposition. The proposed strategy is applied for optimization of different benchmark and engineering problems and results are compared to direct optimization of the undecomposed problems using Trust Region Mode Pursuing Sampling method (TRMPS), Genetic Algorithm (GA), and Dividing RECTangles (DIRECT). The results show that except for the category of un-decomposable problems with all or lots of strong (i. e., important) correlations, the proposed strategy effectively improves the accuracy of the optimization results. The advantages of the new strategy in comparison with the previous methods are also discussed.",2015
"Function Extrapolation at One Inaccessible Point Using Converging Lines","Focus of this paper is on the prediction accuracy of multidimensional functions at an inaccessible point. The paper explores the possibility of extrapolating a high-dimensional function using multiple one-dimensional converging lines. The main idea is to select samples along lines towards the inaccessible point. Multi-dimensional extrapolation is thus transformed into a series of one-dimensional extrapolations that provide multiple estimates at the inaccessible point. We demonstrate the performance of converging lines using Kriging to extrapolate a two-dimensional drag coefficient function. Post-processing of extrapolation results from different lines based on Bayesian theory is proposed to combine the multiple predictions. Selection of lines is also discussed. The method of converging lines proves to be more robust and reliable than two-dimensional Kriging surrogate for the example.",2015
"Multi-Objective Evolutionary Algorithms’ Performance in a Support Role","This paper presents a diagnostic assessment study, evaluating five leading multi-objective evolutionary algorithms (MOEAs) on their effectiveness, efficiency, reliability, and controllability on four different formulations of the same benchmark conceptual design problem, using the same underlying model. This assessment entails a broad sampling of the parameter space of each MOEA, for each problem formulation, requiring millions of optimization runs and trillions of model evaluations. The results of this assessment show the strengths and limitations of these MOEAs, establishing the Borg MOEA as a leading algorithm.",2015
"Improving Multiobjective Multidisciplinary Optimization With a Data Mining-Based Hybrid Method","Multiobjective, multidisciplinary design optimization (MDO) of complex system is challenging due to the long computational time needed for evaluating new designs’ performances. Heuristic optimization algorithms are widely employed to overcome the local optimums, but the inherent randomness of such algorithms leads to another disadvantage: the need for a large number of design evaluations. To accelerate the product design process, a data mining-based hybrid strategy is developed to improve the search efficiency. Based on the historical information of the optimization search, clustering and classification techniques are employed to detect low quality designs and repetitive designs, and which are then replaced by promising designs. In addition, the metamodel with bias correction is integrated into the proposed strategy to further increase the probability of finding promising design regions within a limited number of design evaluations. Two case studies, one mathematical benchmark problem and one vehicle side impact design problem, are conducted to demonstrate the effectiveness of the proposed method in improving the searching efficiency.",2015
"The Effect of Credit Definition and Aggregation Strategies on Multi-Objective Hyper-Heuristics","Heuristics and meta-heuristics are often used to solve complex real-world problems such as the non-linear, non-convex, and multi-objective combinatorial optimization problems that regularly appear in system design and architecture. Unfortunately, the performance of a specific heuristic is largely dependent on the specific problem at hand. Moreover, a heuristic’s performance can vary throughout the optimization process. Hyper-heuristics is one approach that can maintain relatively good performance over the course of an optimization process and across a variety of problems without parameter retuning or major modifications. Given a set of domain-specific and domain-independent heuristics, a hyper-heuristic adapts its search strategy over time by selecting the most promising heuristics to use at a given point.",2015
"A New Sequential Multi-Disciplinary Optimization Method for Bi-Level Decomposed Systems","Multi-discipline optimization (MDO) has been proposed to handle complex optimization problems with multiple disciplines or subsystems. These problems are with global and coupling variables that are shared by more than one subsystem, which is one of the major challenges for these problems. Diverse methods that can be classified as of single- and multi-levels have been developed aiming to solve those problems. Most of these methods do not give optimization autonomies to each subsystem concerning these shared variables. This study proposed that for decomposed bi-level MDO problems, each subsystem is given full optimization autonomy in the initial stage to decide possible best solutions individually. Then all the obtained solutions are given to the system that will process the information and dispatch shared variables to each subsystem. Based on these dispatched variables, subsystems will sequentially perform the corresponding optimization with respect to their local (and global) variables with additional consistency constraints. There is no nested optimization between the system and subsystems. Six numerical and engineering examples are tested and comparisons with other methods are made to demonstrate the efficiency and applicability of the proposed approach.",2015
"Decomposition-Based Design Optimization of Hybrid Electric Powertrain Architectures: Simultaneous Configuration and Sizing Design","Effective electrification of automotive vehicles requires designing the powertrain’s configuration along with sizing its components for a particular vehicle type. Employing planetary gear systems in hybrid electric vehicle powertrain architectures allows various architecture alternatives to be explored, including single-mode architectures that are based on a fixed configuration and multi-mode architectures that allow switching power flow configuration during vehicle operation. Previous studies have addressed the configuration and sizing problems separately. However, the two problems are coupled and must be optimized together to achieve system optimality. An all-in-one system solution approach to the combined problem is not viable due to the high complexity of the resulting optimization problem. In this paper we propose a partitioning and coordination strategy based on Analytical Target Cascading for simultaneous design of powertrain configuration and sizing for given vehicle applications. The capability of the proposed design framework is demonstrated by designing powertrains with one and two planetary gears for a mid-size passenger vehicle.",2015
"Introduction of a Tradeoff Index for Efficient Trade Space Exploration","The development of many-objective evolutionary algorithms has facilitated solving complex design optimization problems, that is, optimization problems with four or more competing objectives. The outcome of many-objective optimization is often a rich set of solutions, including the non-dominated solutions, with varying degrees of tradeoff amongst the objectives, herein referred to as the trade space. As the number of objectives increases, exploring the trade space and identifying acceptable solutions becomes less straightforward. Visual analytic techniques that transform a high-dimensional trade space into two-dimensional (2D) presentations have been developed to overcome the cognitive challenges associated with exploring high-dimensional trade spaces. Existing visual analytic techniques either identify acceptable solutions using algorithms that do not allow preferences to be formed and applied iteratively, or they rely on exhaustive sets of 2D representations to identify tradeoffs from which acceptable solutions are selected. In this paper, an index is introduced to quantify tradeoffs between any two objectives and integrated into a visual analytic technique. The tradeoff index enables efficient trade space exploration by quickly pinpointing those objectives that have tradeoffs for further exploration, thus reducing the number of 2D representations that must be generated and interpreted while allowing preferences to be formed and applied when selecting a solution. Furthermore, the proposed index is scalable to any number of objectives. Finally, to illustrate the utility of the proposed tradeoff index, a visual analytic technique that is based on this index is applied to a Pareto approximate solution set from a design optimization problem with ten objectives.",2015
"Dual Residual for Distributed Augmented Lagrangian Coordination Based on Optimality Conditions","Augmented Lagrangian Coordination (ALC) is one of the more popular coordination strategies for decomposition based optimization. It employs the augmented Lagrangian relaxation approach and has shown great improvements in terms of efficiency and solution accuracy when compared to other methods addressing the same type of problem. Additionally, by offering two variants: the centralized ALC in which an artificial master problem in the upper level is created to coordinate all the sub-problems in the lower level, and the distributed ALC in which coordination can be performed directly between sub-problems without a master problem, ALC provides more flexibility than other methods. However, the initial setting and the update strategy of the penalty weights in ALC still significantly affect its performance and thus are worth further research.",2015
"Towards a Methodology for Multidisciplinary Design Optimization of Haptic Devices","Design of haptic devices requires trade-off between many conflicting requirements, such as high stiffness, large workspace, small inertia, low actuator force/torque, and a small size of the device. With the traditional design and optimization process, it is difficult to effectively fulfill the system requirements by separately treating the different discipline domains. To solve this problem and to avoid sub-optimization, this work proposes a design methodology, based on Multidisciplinary Design Optimization (MDO) methods and tools, for design optimization of six degree-of-freedom (DOF) haptic devices for medical applications, e.g. simulators for surgeon and dentist training or for remote surgery.",2015
"Resource Allocation for Reduction of Epistemic Uncertainty in Simulation-Based Multidisciplinary Design","Epistemic model uncertainty is a significant source of uncertainty that affects a multidisciplinary system. In order to achieve a reliable design, it is critical to ensure that the disciplinary/subsystem simulation models are trustworthy, so that the aggregated uncertainty of system quantities of interest (QOIs) is acceptable. Uncertainty reduction can be achieved by gathering additional experiments and simulations data; however resource allocation for multidisciplinary design optimization (MDO) remains a challenging task due to the complex structure of a multidisciplinary system. In this paper, we develop a novel approach by integrating multidisciplinary uncertainty analysis (MUA) and multidisciplinary statistical sensitivity analysis (MSSA) to answer the questions about where (sampling locations), what (disciplinary responses), and which (simulations versus experiments) for allocating more resources. To manage the complexity in making the above decisions, a sequential procedure is proposed. First, the input space of a multidiscipline system is explored to identify the locations with unacceptable amounts of uncertainty with respect to the system QOIs. Next, these input locations are selected through a correlation check so that they are sparsely located in the input space, and their corresponding critical responses are identified based on MSSA. Finally, using a preposterior analysis, decisions are made about what type of resources (experimental or computational) should be allocated to the critical responses at the chosen input locations. The proposed method is applied to a benchmark electronic packaging problem to demonstrate how epistemic uncertainty is gradually reduced via gathering more data.",2015
"Trade-Off Analysis of System Architecture Modularity Using Design Structure Matrix","Product modularity has been the subject of considerable research and debate in last decade. Various metrics have been proposed in design community to measure the level of modularity and various procedures have been developed to search for ideal modular architectures. These procedures are based on either manual heuristics or computer clustering algorithms. Both approaches are aimed at finding more ideal architectures by optimizing a definition of modularity. However, different desirable criteria are often in conflict with each other and improving one criteria is not feasible without a compromising effect on another. Here, we propose a procedure to find non-dominated optimal architectures where our criteria of interest are intra-cluster and extra-cluster costs. We demonstrate an approach where a designer can consider the architecture that minimizes total cost of interactions, but also allows visualization of the trade-off in increased and decreased costs when considering nearby architectures with more or less modules. An alternative approach has been to consider granularity and hierarchical clustering schemes. We also show through an example that cost optimal architectures for any choice of number of modules are not necessarily obtainable via dividing or aggregating modules, and restricting to hierarchical clustering algorithms produces non-optimal solutions at different numbers of modules.",2015
"Sensitivity and Correlation Analysis and Implications in Scalable Product Family Design for Plug-In Hybrid Electric Vehicles","Plug-in Hybrid Electric Vehicles (PHEVs) bear great promises for increasing fuel economy and decreasing greenhouse gas emissions by the use of advanced battery technologies and green energy resources. The design of a PHEV highly depends on several factors such as the selected powertrain configuration, control strategy, sizes of drivetrain components, expected range for propulsion purely by electric energy, known as AER, and the assumed driving conditions. Accordingly, design of PHEV powertrains for diverse customer segments requires thorough consideration of the market needs and the specific performance expectations of each segment. From the manufacturing perspective, these parameters provide the opportunity of mass customization because of the high degree of freedom, especially when the component sizes and control parameters are simultaneously assessed. Based on a nonconventional sensitivity and correlation analysis performed on a simulation model for power-split PHEVs in this study, the product family design (PFD) concept and its implications will be investigated, and limitations of PFD for such a complex product along with directions for efficient family design of PHEVs will be discussed.",2015
"An Integrated Approach to Product Family Redesign Using Commonality and Variety Metrics","Redesigning a product family entails carefully balancing the tradeoffs between commonality and differentiation that are governed by the underlying platform architecture. Numerous metrics for commonality and variety exist to support product family design; however, rarely are they used in concert to redesign platforms effectively. In this paper, we introduce an integrated approach using multiple product family metrics to establish an effective platform redesign strategy. Specifically, we present a detailed procedure to integrate the Generational Variety Index (GVI), Product Line Commonality Index (PCI), and Design Structure Matrix (DSM) to prioritize components for redesign based on variety and commonality needs. The integrated approach extends to the platform architecture and establishes a redesign strategy for interfaces between components in the platform architecture. To demonstrate the approach, case studies involving two generations of wireless computer mice and two families of dishwashers are presented. Ongoing and future work are also discussed.",2015
"Measurement of Assembly System Complexity Based on the Task Differences Induced From Product Variety","As mass customization becomes a major challenge for manufacturing companies, assembly systems should have the capabilities to accommodate a high variety of products. Since the assembly system operations become much more complex with the increased variety, the firms are in need of a measure which is general and practical enough to correctly estimate the complexity of an assembly system. This paper proposes a new model which measures the complexity based on the modeling of production architecture. Production architecture describes how a product is processed in an assembly system. Then, the complexity is measured by checking which tasks are processed at each station since the differences among tasks are regarded as the main cause of complexity. The model has been applied to a case, a simplified version of real data from an electronics manufacturer. Applying this approach, the complexity induced from task differences can be measured from each station to an entire assembly system, and one can reduce the overall complexity by controlling the tasks which are allocated to stations. Consequently, the firms can get fundamental insights into the operation of complex assembly systems with the help of production architecture from which the complexity measure is derived.",2015
"Assessing and Generating Modules for Product Recovery","As technology pushes customers to buy new released products, especially mobile phone, high product replacement from the customers plays a role in increasing production rate for new products and rate of abandoned products. It accelerates environmental degradation like natural resource usage for the new products and pollutions generated by disposing the abandoned products. In this respect, product recovery is needed to reduce landfill rates, and resource usages, and prolong product lifecycle. Modular drivers such as interface design, material type, and components’ lifespan are applied to design modules for product recovery. The objective of this research is to support designers to assess initial modules and then reorganize modules for product recovery. First, according to conventional modular product design, the initial modules are generated. Then, since it is difficult to estimate how much the modules have negative effects on environment, the environmental impacts of a product are assessed by Eco-Indicator 99 based on used materials. Also, the complexity of the interface design is measured to understand how the modules are easily disassembled for upgrading and maintaining end-of-life products by using weighted-modular complexity score (wMCS). After assessing the product based on the Eco-Indicator 99 and wMCS, we apply new design guidelines to improve sustainability of a product in the end of life stage. Consequently, we compare the extent to design for sustainability before and after redesigning a product based on the design guideline. To demonstrate the effectiveness of the modular product design, we carry out a case study with a coffee maker.",2015
"An Automated Approach to the Design of Small Aerial Systems Using Rapid Manufacturing","Traditional engineering design processes focus on the generation of a completely defined solution for a specific set of design requirements. However, in the modern, rapidly evolving battlespace, Soldiers face the need for situationally specific aerial reconnaissance. Recent advances in automated manufacturing techniques, such as 3-D printing, have enabled the design of small unmanned aerial vehicles in which discrete components can be integrated with parametrically scaled and printed components. This approach enables mission-driven sizing, design, and synthesis of a product family using a small set of components. An integrated requirements and design process that separates the Soldier from any design engineering is presented. Mission requirements, performance models, component attributes, and manufacturing constraints are used to suggest a product architecture capable of fulfilling requirements. The process is executed to design an on-demand solution to specific aerial reconnaissance needs. Assembly takes place in a virtual environment prior to physical integration with off-the-shelf components. The resulting vehicle is then flown in a controlled environment to mimic the mission. A comparison of requirements to actual performance is presented. An assessment is made of the proposed capability and conclusions are drawn about the applicability and scalability of the approach.",2015
"Impact of Gasoline and Natural Gas Prices on Capacity Planning for Automakers and Electricity Generators Under GHG Emission Constraints","Both automakers and electricity generators are facing increasingly more stringent greenhouse gas (GHG) emission targets. With the introduction of plug-in hybrid and electric vehicles, the transportation and electricity generation sectors become connected. This provides an opportunity for both sectors to work jointly to achieve cost efficient reduction of CO",2015
"Value-Driven Modeling of Tactical and Operational Decisions in Support of Aerospace Product-Service Systems Design","Competitive markets and complex business-to-business environments compel manufacturers to provide innovative service offerings along with their products. This necessitates effective methodologires for developing and implementing sucessful new business strategies. This article presents an approach to model tactical and operational decisions to support the design and development of Product-Service Systems (PSSs). A combination of Quality Function Deployment and Design-to-Cost techniques is proposed as the first step of a PSS design framework that aids design engineers to determine the relations among value to customer, functional requirements, design variables and cost. The objective is to identify PSS design alternatives that deliver value to customer while respecting cost targets. An aerospace software case study is conducted to demonstrate the proposed approach.",2015
"Component Lifing Decisions and Maintenance Strategies in the Context of Aeroengine Product-Service Systems Design","Static structural engine components are typically designed for full lifetime operation. Efforts to reduce weight in order to improve performance result in structural designs associated with higher lifing uncertainty: Maintaining reliability levels may necessitate expensive manufacturing and maintenance solutions. In practice, repair techniques for such structures are available; however, they are not planned for during the design process. The objective of the research presented in this paper is to model and optimize component lifecycle costs with respect to lifing decisions, demonstrated by means of an aeroengine component design example. Both technical (failure) and legislative (certification) implications are considered. The impact of maintenance strategies (repair and/or replace) on lifing design decisions is quantified. It is shown that, under different conditions, it may not be prudent to design for full life but rather accept shorter life and then repair or replace the component. This is especially evident if volumetric effects on low cycle fatigue life are taken into account. It is possible that failure rates based on legacy engines do not translate necessarily to weight-optimized components. Such an analysis can play a significant supporting role in engine component design in a product-service system context.",2015
"Study of Current Trade-In Programs Available for Used Consumer Electronics: Investigation of Cellphones Design Features","This paper carries out an analysis of various trade-in programs available for cellphones in the United States. Product trade-in is one of the methods to recover End-of-Life (EoL) products from consumers. Currently, there is a lack of knowledge amongst consumers about such programs. The study aims to determine the factors which influence the product trade-in price. Cell phone trade-in programs of the following types of companies are studied: Phone network operator, online retailer and recycler, and educational institution. Apple’s iPhone was selected to carry out a case study to analyze various features of the trade-in programs. Age of the cell phone model, memory size of the phone, cellphone condition and phone carrier were found to be the most significant factors of a cell phone trade-in program. Newer phone models and higher memory size capacity phones were found to be offered higher price to the consumer. Cellphones of one particular phone carrier and unlocked cell phones were found to obtain the highest price quote. An attempt is made to evaluate and discuss the prospect of trade-in programs as an effective end of life recovery method. Product recovery by trade-in programs and conventional methods is compared based on factors drawn from consumer behavior studies. Improvements in trade-in programs are suggested, followed by a discussion on ways in which data from trade-in programs can benefit product designers.",2015
"In Vivo In Situ","Cities play an essential role in facilitating and supporting the real-world experimentations (for instance in public spaces with real users) of innovative products and services in the field of clean technologies. In this respect, the City of Paris has implemented an experimentation mechanism to help innovative start-ups improving their solutions and robustifying their business models in a multi-stakeholder eco-system. Nonetheless, a primary investigation demonstrated that the efficiency of these in vivo in situ experimentations have means of being improved.",2015
"Reliability-Based Design Optimization in X-Space Using Ensemble of Gaussian Reliability Analyses (EoGRA)","Reliability-Based Design Optimization (RBDO) algorithms have been developed to solve design optimization problems with existence of uncertainties. Traditionally, the original random design space is transformed to the standard normal design space, where the reliability index can be measured in a standardized unit. In the standard normal design space, the Modified Reliability Index Approach (MRIA) measured the minimum distance from the design point to the failure region to represent the reliability index; on the other hand, the Performance Measure Approach (PMA) performed inverse reliability analysis to evaluate the target function performance in a distance of reliability index away from the design point. MRIA was able to provide stable and accurate reliability analysis while PMA showed greater efficiency and was widely used in various engineering applications. However, the existing methods cannot properly perform reliability analysis in the standard normal design space if the transformation to the standard normal space does not exist or is difficult to determine. To this end, a new algorithm, Ensemble of Gaussian Reliability Analyses (EoGRA), was developed to estimate the failure probability using Gaussian-based Kernel Density Estimation (KDE) in the original design space. The probabilistic constraints were formulated based on each kernel reliability analysis for the optimization processes. This paper proposed an efficient way to estimate the constraint gradient and linearly approximate the probabilistic constraints with fewer function evaluations. Some numerical examples with various random distributions are studied to investigate the numerical performances of the proposed method. The results showed EoGRA is capable of finding correct solutions in some problems that cannot be solved by traditional methods.",2015
"Extreme Value Metamodeling for System Reliability With Time-Dependent Functions","The reliability of a system is usually measured by the probability that the system performs its intended function in a given period of time. Estimating such reliability is a challenging task when the probability of failure is rare and the responses are nonlinear and time variant. The evaluation of the system reliability defined in a period of time requires the extreme values of the responses in the predefined period of time during which the system is supposed to function. This work builds surrogate models for the extreme values of responses with the Kriging method. For the sake of computational efficiency, the method creates Kriging models with high accuracy only in the region that has high contributions to the system failure; training points of random variables and time are sampled simultaneously so that their interactions could be considered automatically. The example of a mechanism system shows the effectiveness of the proposed method.",2015
"Reliability Analysis for Multidisciplinary Systems Involving Stationary Stochastic Processes","The response of a component in a multidisciplinary system is affected by not only the discipline to which it belongs, but also by other disciplines of the system. If any components are subject to time-dependent uncertainties, responses of all the components and the system are also time dependent. Thus, time-dependent multidisciplinary reliability analysis is required. To extend the current time-dependent reliability analysis for a single component, this work develops a time-dependent multidisciplinary reliability method for components in a multidisciplinary system under stationary stochastic processes. The method modifies the First and Second Order Reliability Methods (FORM and SORM) so that the Multidisciplinary Analysis (MDA) is incorporated while approximating the limit-state function of the component under consideration. Then Monte Carlo simulation is used to calculate the reliability without calling the original limit-state function. Two examples are used to demonstrate and evaluate the proposed method.",2015
"Flexible Design of Systems Considering Time-Dependent Reliability","Many repairable systems degrade with time and are subjected to time-varying loads. Their characteristics may change over time considerably, making the assessment of their performance and hence their design difficult. To address this issue, we introduce in this paper the concept of flexible design of repairable systems under time-dependent reliability considerations. In flexible design, the system can be modified in the future to accommodate uncertain events. As a result, regardless of how uncertainty resolves itself, a modification is available that will keep the system close to optimal provided failure events have been properly characterized. We discuss how flexible design of repairable systems requires a fundamentally new approach and demonstrate its advantages using the design of a hydrokinetic turbine. Our results show that long-term metrics are improved when time-dependent characteristics and flexibility are considered together.",2015
"Material Characterization of Additively Manufactured Part via Multi-Level Stochastic Upscaling Method","An integrated multiscale modeling framework that incorporates a simulation-based upscaling technique is developed and implemented for the material characterization of additively manufactured cellular structures in this paper. The proposed upscaling procedure enables the determination of homogenized parameters at multiple levels by matching the probabilistic performances between fine and coarse scale models. Polynomial chaos expansion is employed in upscaling procedure to handle the computational burden caused by the input uncertainties. Efficient uncertainty quantification is achieved at the mesocale level by utilizing the developed upscaling technique. The homogenized parameters of mesostructures are utilized again at the macroscale level in the upscaling procedure to accurately obtain the overall material properties of the target cellular structure. Actual experimental results of additively manufactured parts are integrated into the developed procedure to demonstrate the efficacy of the method.",2015
"Time-Dependent Reliability Using Metamodels With Transformed Random Inputs","We have recently proposed a method for time-dependent reliability based on metamodels with random inputs. In that method, we employed multiple sets of inputs sampled from the input distribution to construct a new metamodel as a mixture of classical metamodels. Because the sampled inputs may cluster around a mode of the input distribution, they may result in a metamodel of reduced quality. We address this issue in this paper by using a transformation to de-cluster the sample inputs and then use our previously developed metamodel with random inputs. We first obtain the output of the computer model for a limited number of transformed input draws which do not cluster in high probability regions of the input space. Then, conditioned on these transformed sampled inputs, we construct a classical Kriging surrogate and obtain the distribution of the new surrogate as the marginal of the joint distribution between the classical surrogate and the transformed sampled inputs. The proposed method is illustrated with a corroding beam example. A more accurate time-dependent reliability estimation is obtained compared with our previously developed metamodel method.",2015
"Time-Dependent Reliability Analysis of Vibratory Systems With Random Parameters","The field of random vibrations of large-scale systems with millions of degrees of freedom is of significant importance in many engineering disciplines. In this paper, we propose a method to calculate the time-dependent reliability of linear vibratory systems with random parameters excited by non-stationary Gaussian processes. The approach combines principles of random vibrations, the total probability theorem and recent advances in time-dependent reliability using an integral equation involving the up-crossing and joint up-crossing rates. A space-filling design, such as optimal symmetric Latin hypercube sampling, is first used to sample the input parameter space. For each design point, the corresponding conditional time-dependent probability of failure is calculated efficiently using random vibrations principles to obtain the statistics of the output process and an efficient numerical estimation of the up-crossing and joint up-crossing rates. A time-dependent metamodel is then created between the input parameters and the output conditional probabilities allowing us to estimate the conditional probabilities for any set of input parameters. The total probability theorem is finally applied to calculate the time-dependent probability of failure. The proposed method is demonstrated using a vibratory beam example.",2015
"Topology Optimization With Load Uncertainty as an Inhomogeneous Eigenvalue Problem","In this paper, a non-probabilistic based topology optimization method under an external load uncertainty is presented. In traditional topology optimization problems, external loadings that apply to structures are always assumed as deterministic, but an external loading with uncertainties is very common in many practical engineering applications. In this paper, load uncertainty is described as an unknown-but-bounded model and the maximum possible strain energy based topology optimization formulation under an uncertain load is solved for the worst case condition. This optimization problem can be rewritten as a two-level optimization problem: the upper level optimization problem is a deterministic topology optimization under a critical loading of the worst structure response, and the lower level optimization problem is to determine the critical loading corresponding to the worst structure response.",2015
"Stochastic Model Bias Correction of Dynamic System Responses for Simulation-Based Reliability Analysis","Reliability analysis based on the simulation model could be wrong if the simulation model were not validated. Various model bias correction approaches have been developed to improve the model credibility by adding the identified model bias to the baseline simulation model. However, little research has been conducted for simulation models with dynamic system responses. This paper presents such a framework for model bias correction of dynamic system responses for reliability analysis by addressing three technical components including: i) a validation metric for dynamic system responses, ii) an effective approach for dynamic model bias calibration and approximation, and iii) reliability analysis considering the dynamic model bias. Two case studies including a thermal problem and a corroded beam problem are employed to demonstrate the proposed approaches for simulation-based reliability analysis.",2015
"Development of a Conservative Model Validation Approach for Reliable Analysis","Simulation models are approximations of real-world physical systems. Therefore, simulation model validation is necessary for the simulation-based design process to provide reliable products. However, due to the cost of product testing, experimental data in the context of model validation is limited for a given design. When the experimental data is limited, a true output PDF cannot be correctly obtained. Therefore, reliable target output PDF needs to be used to update the simulation model. In this paper, a new model validation approach is proposed to obtain a conservative estimation of the target output PDF for validation of the simulation model in reliability analysis. The proposed method considers the uncertainty induced by insufficient experimental data in estimation of predicted output PDFs by using Bayesian analysis. Then, a target output PDF and a probability of failure are selected from these predicted output PDFs at a user-specified conservativeness level for validation. For validation, the calibration parameter and model bias are optimized to minimize a validation measure of the simulation output PDF and the conservative target output PDF subject to the conservative probability of failure. For the optimization, accurate sensitivity of the validation measure is obtained using the complex variable method (CVM) for sensitivity analysis. As the target output PDF satisfies the user-specified conservativeness level, the validated simulation model provides a conservative representation of the experimental data. A simply supported beam is used to carry out the convergence study and demonstrate that the proposed method establishes a conservatively reliable simulation model.",2015
"Reliability Analysis and Design Considering Disjointed Active Failure Regions","Failure of practical engineering systems could be induced by several correlated failure modes, and consequently reliability analysis are conducted with multiple disjointed failure regions in the system random input space. Problems with disjointed failure regions create a great challenge for existing reliability analysis approaches due to the discontinuity of the system performance function between these regions. This paper presents a new enhanced Monte Carlo simulation (EMCS) approach for reliability analysis and design considering disjointed failure regions. The ordinary Kriging method is adopted to construct surrogate model for the performance function so that Monte Carlo simulation (MCS) can be used to estimate the reliability. A maximum failure potential based sampling scheme is developed to iteratively search failure samples and update the Kriging model. Two case studies are used to demonstrate the efficacy of the proposed methodology.",2015
"Design Sensitivity Method for Sampling-Based RBDO With Fixed COV","Conventional reliability-based design optimization (RBDO) uses the means of input random variables as its design variables; and the standard deviations (STDEVs) of the random variables are fixed constants. However, the fixed STDEVs may not correctly represent certain RBDO problems well, especially when a specified tolerance of the input random variable is presented as a percentage of the mean value. For this kind of design problem, the coefficients of variations (COVs) of the input random variables should be fixed, which means STDEVs are not fixed. In this paper, a method to calculate the design sensitivity of probability of failure for RBDO with fixed COV is developed. For sampling-based RBDO, which uses Monte Carlo simulation for reliability analysis, the design sensitivity of the probability of failure is derived using a first-order score function. The score function contains the effect of the change in the STDEV in addition to the change in the mean. As copulas are used for the design sensitivity, correlated input random variables also can be used for RBDO with fixed COV. Moreover, the design sensitivity can be calculated efficiently during the evaluation of the probability of failure. Using a mathematical example, the accuracy and efficiency of the developed method are verified. The RBDO result for mathematical and physical problems indicates that the developed method provides accurate design sensitivity in the optimization process.",2015
"Modified Bayesian Kriging for Noisy Response Problems for Reliability Analysis","This paper develops a new modified Bayesian Kriging (MBKG) surrogate modeling method for problems in which simulation analyses are inherently noisy and thus standard Kriging approaches fail to properly represent the responses. The purpose is to develop a method that can be used to carry out reliability analysis to predict probability of failure. The formulation of the MBKG surrogate modeling method is presented, and the full conditional distributions of the unknown MBKG parameters are presented. Using the full conditional distributions with a Gibbs sampling algorithm, Markov chain Monte Carlo is used to fit the MBKG surrogate model. A sequential sampling method that uses the posterior credible sets for inserting new design of experiment (DoE) sample points is proposed. The sequential sampling method is developed in such a way that the newly added DoE sample points will provide the maximum amount of information possible to the MBKG surrogate model, making it an efficient and effective way to reduce the number of DoE sample points needed. Therefore, the proposed method improves the posterior distribution of the probability of failure efficiently. To demonstrate the developed MBKG and sequential sampling methods, a 2-D mathematical example with added random noise is used. It is shown how, with the use of the sequential sample method, the posterior distribution of the probability of failure converges to capture the true probability of failure. A 3-D multibody dynamics (MBD) engineering block-car example illustrates an application of the new method to a simple engineering example for which standard Kriging methods fail.",2015
"Sequential Cooperative Robust Optimization (SCRO) for Multi-Objective Design Under Uncertainty","The optimal design of systems under uncertainty is a critical challenge faced by design engineers. Robust optimization is a well-studied and widely used technique for the design of engineering systems that possess uncertainty, and numerous robust optimization techniques have been presented in recent years. The majority of the robust optimization techniques presented in the literature suffer from a computational efficiency challenge, either due to the expense of obtaining objective or constraint function uncertainty information, or due to the fact that many robust optimization approaches (with a few notable exceptions) require that a potentially expensive uncertainty analysis calculation (e.g. Monte-Carlo simulation) be nested within an already potentially expensive optimization solver (e.g. a genetic algorithm). Additionally, many robust optimization approaches focus solely on design problems that possess a single design objective, and the robust techniques that do consider problems with multiple design objectives often require various simplifying assumptions or are even more computationally expensive to implement. Clearly there are opportunities for improvement in the area of robust optimization, and this paper presents a new robust design Optimization approach called Sequential Cooperative Robust Optimization (SCRO), which uses both a sequential approach and multi-objective optimization techniques in an effort to decouple the deterministic system optimization problem from the associated uncertainty analysis problem. The SCRO approach first fits surrogate models to the system objective and constraint functions, in addition to system sensitivity functions, using as few function calls as possible in order to improve computational efficiency. The approach then performs a series of sequential multi-objective optimizations using the developed surrogate models. These optimizations work to find points in the design space that are optimal with respect to deterministic performance and both objective and feasibility robustness metrics based on predicted system sensitivities. The SCRO approach has the potential to find solutions not available to other robust optimization approaches, and can be more efficient than other more traditional robust optimization techniques due to its use of surrogate approximation and a sequential framework.",2015
"Uncertainty Quantification in Time-Dependent Reliability Analysis","One of the essential steps in time-dependent reliability analysis is the characterization of stochastic load processes and system random variables based on experimental or historical data. Limited data results in uncertainty in the modeling of random variables and stochastic loadings. The uncertainty in random variable and stochastic load models later causes uncertainty in the results of reliability analysis. An uncertainty quantification framework is developed in this paper for time-dependent reliability analysis. The effects of two kinds of uncertainty sources, namely data uncertainty and model uncertainty on the results of time-dependent reliability analysis are investigated. The Bayesian approach is employed to model the epistemic uncertainty sources in random variables and stochastic processes. A straightforward formulation of uncertainty quantification in time-dependent reliability analysis results in a double-loop implementation, which is computationally expensive. Therefore, this paper builds a surrogate model for the conditional reliability index in terms of variables with imprecise parameters. Since the conditional reliability index is independent of the epistemic uncertainty, the surrogate model is applicable for any realizations of the epistemic uncertainty. Based on the surrogate model, the uncertainty in time-dependent reliability analysis is quantified without evaluating the original limit-state function, which increases the efficiency of uncertainty quantification. The effectiveness of the proposed method is demonstrated using a mathematical example and an engineering application example.",2015
