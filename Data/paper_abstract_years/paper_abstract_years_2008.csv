title,Abstract,year
"Automatically Generating Form Concepts According to Consumer Preference: A Shape Grammar Implementation With Software Agents","In new product development, quickly generating many concepts that a potential consumer prefers is a challenge. This paper presents the inaugural application of software agents implementing a shape grammar to generate product designs according to a utility function that represents consumer preference. The method is composed of three sub-processes: a shape grammar interpreter, an agent interpreter, and a utility investigator. These work together to explore the design space and can constrain product form designs according to a utility function that represents consumer design preference.",2008
"Evolutive Design of Car Silhouettes","An Interactive Genetic Algorithm system is proposed for designing a car silhouette while involving the style designer in the evaluation process of a population of individuals. This IGA is based on the principle of an indirect encoding of a closed curve genotype using a primary Fourier decomposition. A crossing over operator is proposed for mixing the parents’ genes by a random weighted average into a new child’s genotype. A perceived similarity index between two genotypes is built to check that our IGA is able to converge toward a target individual starting from the genes of an initial population.",2008
"Generative Design and CNC Fabrication Using Shape Grammars","Generative design and fabrication refers to the ability to autonomously generate designs while simultaneously generating all information to directly fabricate them. This technique is driven by the increasing need to rapidly and flexibly fabricate customized parts and individually designed products. For the automation of the design-to-fabrication process chain, intensive and dynamically updated knowledge from the domains of design and fabrication must be provided. To allow for a flexible, autonomous fabrication, the knowledge modeled must dynamically reflect the state of the fabrication system and its capabilities. This paper presents an approach to unify knowledge for generative design and generative fabrication using shape grammars. With shape grammars, the geometry of designs and their mapping to removal volumes corresponding to fabrication processes on CNC machine tools are represented. The process instructions for fabrication are included by augmenting the removal volume shapes with labels. A new shape grammar approach to represent designs and fabrication processes is presented and validated on an example functional part as a proof-of-concept. The approach enables pushing knowledge downstream, from design and process planning directly to the fabrication system itself providing a stepping stone towards awareness of machine capabilities in fabrication systems and autonomous process planning for customized parts.",2008
"Pultrusion Process Analysis Using Knowledge-Based System","This paper describes the development and validation of analytical and numerical procedures for the simulation of temperature and cure profiles for the pultrusion process. The governing equations for heat transfer and the resin cure reaction during pultrusion are presented and finite-difference procedure is developed to solve the governing equations. Then analytical and numerical procedures are integrated into the ACES (Automated Concurrent Engineering System) software environment. It is shown that the procedures are numerically stable and predict the temperature and cure profiles, which are in good agreement with those published by other researchers. This analysis leads to the prediction of the temperature and cure distribution along the length and through the thickness of the composite. The prediction of the temperature and cure distribution enables proper selection of process variables, which affects the production rate and quality of the parts.",2008
"Causal Design Knowledge Acquisition by Constructing BBN Through FCM","Managing design knowledge is an important concern for industry, including engineering. Engineering firms are facing pressures to increase the quality of their products, to have even shorter lead times and reduced costs. There is also a trend towards globalization resulting in complex supply chains and the need to manage teams that are not necessarily co-located. Design knowledge needs to be exchanged and accessed efficiently. Other motivations for managing design knowledge are to provide a trail for product liability legislation and to retain design knowledge and experience as engineering designers retire. Fuzzy Cognitive Map (FCM) is one of the main formalisms for modeling, representing and reasoning about causal knowledge. Despite the fact that FCM has been used extensively in causal knowledge engineering, there is a lack of methodology for the systematic construction of FCM. Although some techniques were used in the individual construction processes, these techniques were either not systematically documented or too specific to the problem at hand. FCM and Bayesian Belief Network (BBN) are two major frameworks for modeling, representing and reasoning about causal design knowledge. Despite their extensive use in causal design knowledge engineering, there is no reported work which compares their respective roles. This paper deals with three topics, which are systematic constructing FCM, a methodology for FCM-BBN conversion, and comparison FCM and BBN. BBN has a sound mathematical foundation and reasoning capabilities, also it has an efficient evidence propagation mechanism and a proven track record in industryscale applications. However, BBN is less friendly and flexible, and often very time-consuming to generate appropriate conditional probabilities. Thus, Fuzzy Cognitive Map (FCM) is used for the indirect knowledge acquisition, and the causal knowledge in FCM is systematically converted to BBN. Finally, we compare BBNs directly generated by domain experts and generated from FCM, with a realistic industrial example, a fuel nozzle for an aerospace engine.",2008
"Rough Set-Based Design Rule Selection for Collaborative Assembly Design","While the modern product development requires more knowledge-intensive and collaborative environment, the capture, retrieval, accessibility, and reusability of that design knowledge are increasing critical. In this paper, a rough set theory generates demanded rules and selects the appropriate minimal rules among the demanded design rules associated to the assembly design knowledge. The design rules are infrequently captured and often ignored due to its complexity. Rough set theory synthesizes approximation of concepts, analyzes data by discovering patterns, and classifies into certain decision classes. Such patterns can be extracted from data by means of methods based on Boolean reasoning and discernibility. This paper shows the feasibility of rough-set based rule selection considering complex design data objects in order to obtain efficient assembly design decision.",2008
"Agent Based Variation Propagation for Collaborative Top-Down Assembly Design","The design of a complex mechanical product is usually a top-down process carried out by different teams or designers that are geographically distributed. A systematical variation propagation mechanism is very important to fully support such a design process. In this paper, based on the framework for collaborative top-down assembly design previously proposed by the authors, an agent based approach is presented for addressing variation propagation for collaborative top-down assembly design. The approach achieves variation propagation during the collaborative top-down assembly design through the interaction and cooperation of the agents located at the clients and server. To make the variation propagation automated and intelligent, four kinds of variation reasoning including hierarchical variation reasoning, engineering constraint variation reasoning, feature variation reasoning, and assembly constraint variation reasoning are identified, and the corresponding algorithms are developed and utilized. Meanwhile, a distributed assembly model is put forward to effectively support the design variation propagation for the collaborative top-down assembly design. The approach is implemented and a variation propagation example is given.",2008
"Integrating Assembly Design, Sequence Optimization, and Advanced Path Planning","Assembling a product is a delicate process at the borderline between design and manufacturing. Shortening the time between them plays a central role both for quality assurance and fast time to market. In this paper, we describe an automatic tool based on a new method, integrating assembly design, sequence optimization, and advanced rigid body path planning. First, we introduce a greedy algorithm for assembling a product, part by part, based on state-of-the-art path planning. We exploit all the six degrees of freedom of a rigid body to search for collision-free paths, instead of limited motions. Then, we use assembly design in order to limit the search for an optimal assembling sequence and to guarantee geometrical quality among the sequences examined. Disassembly path planning is used here to further cut the state space and to give a quality measure to the sequences. Eventually, we present results for an industrial test case, which has been successfully solved by applying our method.",2008
"Assembly and Variety Considerations During Conceptual Design","Foremost step in the development of any electromechanical product is its design, and conceptual design is the most ambiguous and creative phase of design. There exist only a few computational tools that aid designers at conceptual design stage, and mostly designers rely on personal experience or experience of co-workers to generate quality designs. The proposed framework aims at generating robust computerized conceptual designs by incorporating Modularity, Design for Assembly (DFA) and Design for Variety (DFV) principles at the conceptual stage. Conceptual design alternatives obtained from the proposed framework are ranked based on minimum assembly time, and are composed of modules in a way that future changes in customer needs are satisfied only by replacing certain modules. The framework involves searching a design repository of components by using functional-basis and pre-defined graph grammar rules, to generate all possible conceptual design alternatives. These design alternatives are ranked and filtered using a DFA index, and top two alternatives are selected. Selected designs are modularized and filtered using a DFV index to obtain the best design alternative. This paper provides a detailed discussion of the proposed framework, and its working is illustrated through the design of a mounting system for holding a Variable Message Sign (VMS).",2008
"A Review of Recent Phase Transition Simulation Methods: Transition Path Search","In this paper, we give a review of recent transition path search methods for nanoscale phase transition simulation A potential energy surface (PES) characterizes detailed information about phase transitions where the transition path is related to a minimum energy path on the PES. The minimum energy path connects reactant to product via saddle point(s) on the PES. Once the minimum energy path is generated, the activation energy required for transitions can be determined. Using transition state theory, one can estimate the rate constant of the transition. The rate constant is critical to accurately simulate the transition process with sampling algorithms such as kinetic Monte Carlo.",2008
"A Review of Recent Phase Transition Simulation Methods: Saddle Point Search","A review of saddle point search methods on a potential energy surface is presented in this paper. Finding saddle points on a complex potential energy surface is the major challenge in modeling and simulating the kinetics of first-order phase transitions. Once the saddle points have been identified and the activation energy for the transition is known, one can apply the kinetic Monte Carlo method to simulate the transition process. We consider some factors while reviewing the methods, such as whether the solution is global, the knowledge of the Hessian during the search, the capability to locate multiple saddle points and higher order saddle points, the kind of approximations used for potential energy surface, if any; and the convergence of the methods.",2008
"A Multiscale Design Approach With Random Field Representation of Material Uncertainty","A multiscale design approach is proposed in this paper considering the impacts of product manufacturing process and material on product performance. A framework is established to integrate designs of manufacturing process, material and product based on the information flow across these three domains. Random field is employed to realistically model the uncertainty existing in material microstructure which spatially varies in a product inherited from the manufacturing process. An efficient procedure for uncertainty propagation from the material random field to the end product performance is established. To reduce the dimensionality of random field representation, a reduced order Karhunen-Loeve expansion is used with a discretization scheme applied to finite element meshes. The univariate dimension reduction method and the Gaussian quadrature formula are used to efficiently quantify the uncertainties in product performance in terms of its statistical moments, which are critical information for design under uncertainty. A control arm example is used to demonstrate the proposed approach. The impact of the initial microscale porosity random field produced during a casting process on the product damage is studied and a reliability-based design of the control arm is performed.",2008
"Feature-Based Crystal Construction in Computer-Aided Nano-Design","Providing nano engineers and scientists efficient and easy-to-use tools to create geometry conformations that have minimum energies is highly desirable in material design. Recently we developed a periodic surface model to assist the construction of nano structures parametrically for computeraided nano-design. In this paper, we present a feature-based approach for crystal construction. The proposed approach starts to create models of basic features by the aide of periodic surfaces followed by operations between basic features. The goal is to introduce a rapid construction method for complex crystal structures.",2008
"An Approach to Automate Concept Generation of Sheet Metal Parts Based on Manufacturing Operations","This paper describes an approach to automate the design for sheet metal parts that are not only novel and manufacturable but also satisfies multiple objective functions such as material cost and manufacturability. Unlike commercial software tools such as Pro/SHEETMETAL which aids the user in finalizing and determining the sequence of manufacturing operations for a specified component, our approach starts with spatial constraints in order to create the component geometries and helps the designer design. While there is an enormous set of parts that can feasibly be generated with sheet metal, it is difficult to define this space systematically. To solve this problem, we currently have 88 design rules that have been developed for four basic sheet metal operations: slitting, notching, shearing, and bending. A recipe of the operations for a final optimal design is then presented to the manufacturing engineers thus saving them time and cost. The technique revealed in this paper represents candidate solutions as a graph of nodes and arcs where each node is a rectangular patch of sheet metal, and modifications are progressively made to the sheet to maintain the parts manufacturability. They are presented in the form of Standard Tessellation Language files (.stl) that can be transferred into available modeling software for further analysis. The overall purpose of this research is to provide creative designs to the designer granting him/her a new perspective and to check all the solutions for manufacturability in the early stage of design process. An example sheet metal design problem is shown in this paper with some of the preliminary designs that our approach created.",2008
"Energy Characteristic State Model for Automated Conceptual Design of Hydraulic System","A new approach for automated conceptual design of hydraulic system is developed in this paper based on the energy characteristic state model. The energy characteristics of hydraulic systems are firstly represented by qualitative vectors, and the integral hydraulic systems are visualized as a set of subsystems with single-action for synthesis. Then, two categories of basic transformation units are defined from general hydraulic components, and the qualitative matrix representations for these basic transformation units are also established according to the function analysis. Finally, the conceptual design model for hydraulic system is established, in which the design processes of single-action subsystems are transformed into the decomposition of system-level matrices and the matching of component-level matrices. A design example is also given to illustrate the proposed approach.",2008
"A Growth Design Approach for Tolerancing","Tolerancing is an essential part of a product design process. It is traditionally performed after the product structure has been determined. The product precision requirements may not be fully utilized for the decision making for selecting the best structure in the early stage of product design — the conceptual design phase. A growth design method has been proposed for product structure design and concurrent tolerancing, in which the product form and tolerance can be generated in the same way as an organism grows. The basic feature of biology, cell division, has been borrowed as the design guiding principle. Product conceptual model has been built as the basis of form and tolerance growth design. The relationship of product function, structure and tolerance was analysed to explain on what and how they act in product design process. Tolerance mathematic model expressed by screw parameters and expression model based on ",2008
"Port-Based Ontology Modeling for Product Conceptual Design","Ontology has been known as an important means to represent design knowledge in product development, however, most ontology creation has not yet been systematically carried out. Port, as the location of intended interaction between a component and its enviornment, plays an important role in product conceptual design. It constitutes the interface of a component and defines its boundary. This paper introduces an approach, it is convenient to abstractly represent the intended exchange of signals, energy and/or material, and creat and manage port-based domain ontology, to port-based ontology modeling (PBOM) for product conceptual design. In this paper, port concept and port functional description through using natural language are first presented and their semantic synthesis is used to describe port ontology. Secondly, an ontology repository which contains the assorted primitive concepts and primitive knowledge to map the component connections and interactions is built. Meanwhile a model of port-based multi-views which contains functional view, behavior view and configuration view is articulated, and the attributes and taxonomy of ports in a hierarchy are presented. Next, a port-based ontology language (PBOL) is described to represent the process of port ontology refinement, and a port-based FBS modeling framework is constructed to describe system configuration. Furthermore, a formal knowledge framework to manage comprehensive knowledge is proposed, which could help designers create, edit, organize, represent and visualize product knowledge. Finally, a revised tape case is employed to validate the efficiency of the port ontology for product conceptual design and illustrate its application.",2008
"Customizing Products Using Functional Component Matrices","Modern design methodologies have used Function Component Matrices in a variety of different ways in order to support various facets of an engineering design process. The mapping of functions to components can be used to model and capture the dependencies and relationships that exist. This process is accomplished by breaking down complicated functions into smaller, easier to understand functions. This decomposition allows engineers to get a better understanding for how a change in each component within a product will affect the overall operation of the product. Being able to recognize the impact of the propagation of a sub-function change will give designers a better understanding of the flexibility (or lack thereof) of choices they have when designing a product for customization. In turn they can be used to inform the consumer regarding the consequences their customization choices can have on the final product. This paper discusses how a Functional Component Matrix (FCM) can be used to assist in this process of product customization and understanding change propagation.",2008
"Exploiting the Features of ISO GPS Standards to Enhance a Knowledge-Based Method for Product Redesign and Process Reconfiguration","Previous work developed a knowledge-based method named Design GuideLines Collaborative Framework (DGLs-CF), adopted during product redesign and process reconfiguration and aimed at dealing and matching with particular manufacturing and verification technologies, according to ISO GPS concepts. Aim of present work is improving the role of the ISO GPS within the DGLs-CF, not only to raise coherence in terms of concepts, but to gain some important internal and external improvements for the DGLs-CF. The main activity toward achieving this goal is the formalization of DGLs-CF product features by means of the ISO GPS features. The procedure is proposed and the consequences of its application on the elements of the DGLs-CF are evaluated. A case of study as an example of application is also presented. A significant improvement in the DGLs-CF is realized, both in terms of knowledge structure and organization, and in terms of the possibility of interfacing it with other methodologies, tools, and environments.",2008
"Lagrangian Relaxation for Decentralized Decision Making in Engineering Design","To minimize the coordination efforts among design teams and expedite the design process via parallel workflows, a cooperative and decentralized environment is often considered for team-based design. The ",2008
"Design Achievement Model for Planning Creative and Concurrent Design Process","Planning of an upstream design process that includes creative and concurrent activities has become more important for product development in a competitive market. A significant characteristic of upstream planning is that the design process is one of knowledge creation. During this process, a designer makes progress toward a more advanced knowledge level that corresponds to a more advanced design achievement. In most cases of a creative and concurrent design process, however, a designer has to compromise design achievement because of constraints such as delivery time, cost, and another designer’s intention. Therefore, in planning a design process, it is more essential to set an acceptable level of design achievement and to predict whether or not a planned design process can ensure this level, than to predict the design time needed to totally achieve the design goals. This paper proposes a new method of design process planning that focuses on quantitative prediction of knowledge level achieved in a creative and concurrent design process. A growth curve model using fuzzy numbers is introduced to predict the final achievement of each task and final achievement of consistency between tasks after running a planned design process. The reliability model of a serial system is used to calculate the total acceptability of the design achievement. An experimental system that supports design process planning based on the proposed method is developed. This paper demonstrates its application to a student design project in order to show the power of the method.",2008
"Product Realization in the Age of Mass Collaboration","There has been a recent emergence of communities working together in large numbers to develop new products, services, and systems. Collaboration at such scales, referred to as ",2008
"A Proposed Taxonomy for Physical Prototypes: Structure and Validation","This paper introduces a taxonomy of physical prototypes, benchmarking it with existing classifications to elucidate the advantages and disadvantages. The proffered taxonomy is evaluated by three approaches: (i) checking the orthogonality of the individual elements of the taxonomy, (ii) benchmarking the proposed taxonomy with existing classifications of prototypes, and (iii) demonstrating its utility by applying it to classify different prototypes. The results show the proposed taxonomy is capable of distinguishing prototypes with greater precision than what is possible with current classifications. Further, the proposed classification is generally orthogonal, allowing for more consistent delineation between prototypes. However, the findings also reveal that some of the taxa are interdependent; exploring these interdependencies to develop a deeper understanding of how prototyping influences design thinking is the focus of future research.",2008
"An Effective Random Field Characterization for Probability Analysis and Design","So far manufacturing tolerance variability over samples has been widely considered in many engineering design problems. Traditionally the tolerance variability is modeled as a spatially independent random parameter although the variability is a function of spatial variables (",2008
"Bayesian Reliability Analysis With Evolving, Insufficient, and Subjective Data Sets","A primary concern in product design is ensuring high system reliability amidst various uncertainties throughout a product life-cycle. To achieve high reliability, uncertainty data for complex product systems must be adequately collected, analyzed, and managed throughout the product life-cycle. However, despite years of research, system reliability assessment is still difficult, mainly due to the challenges of evolving, insufficient, and subjective data sets. Therefore, the objective of this research is to establish a new paradigm of reliability prediction that enables the use of evolving, insufficient, and subjective data sets (from expert knowledge, customer survey, system inspection & testing, and field data) over the entire product life-cycle. This research will integrate probability encoding methods to a Bayesian updating mechanism. It is referred to as Bayesian Information Toolkit (BIT). Likewise, Bayesian Reliability Toolkit (BRT) will be created by incorporating reliability analysis to the Bayesian updating mechanism. In this research, both BIT and BRT will be integrated to predict reliability even with evolving, insufficient, and subjective data sets. It is shown that the proposed Bayesian reliability analysis can predict the reliability of door closing performance in a vehicle body-door subsystem where the relevant data sets availability are limited, subjective, and evolving.",2008
"Planning on Mistakes: An Approach to Incorporate Error Checking Into the Design Process","Mistakes in the design process have been recognized as a major source of product quality loss. There are several methods currently used to identify and quantify these mistakes. However, these methods typically do not provide a useful context within which to quantitatively incorporate mistakes into the design process in a beneficial way. This paper presents an approach to determine when it is appropriate to perform error checking to eliminate a potential mistake. The proposed approach is intended to be used when time is a limited design resource and design goals are technically attainable. It is proposed that the cost of a mistake can be quantified as the amount of time a mistake adds or subtracts from the overall time required to achieve the design’s objectives. To determine this, an optimization problem is formulated which minimizes time spent in the design process. In this optimization problem the design variables are the binary choice whether or not to perform an error check. The approach is demonstrated in two case studies, one a simple theoretical design problem and the other the design of an I-beam. The results of these case studies demonstrate the approach’s effectiveness, and present several avenues for future work.",2008
"Development of a Probabilistic Model for Mechanistic Evaluation of Reliability of Oil Pipelines Subject to Corrosion-Fatigue Cracking","A model structure for the corrosion-fatigue degradation phenomenon has been proposed for oil pipelines. At first, a well acknowledged model for corrosion-fatigue, such as Wei’s “Super Position Model”, was pursued as a reference. By reiterating the reference model using generic data from literature and applying Monte Carlo simulation, the simplest possible structure for the model was identified. The correlation of the proposed model with the environmental effects, such as loading stress and frequency, surrounding the pipelines were estimated. The sources of random variability emerging from many sources have been reasonably embodied into only two random variables. Yet again, the scarce of field and experimental data for this particular critical degradation phenomenon has compelled the research study to rely profoundly on generic data from open literature. Hence, the best distribution estimates for the two random variables were computed. Subsequent to number of iterations, the proposed model was modified to rather simpler form. All the proposed model forms have been cross checked against the original reference model which resulted in a satisfying agreement.",2008
"Relating Two Domains via a Third: An Approach to Overcome Ambiguous Attributions Using Multiple Domain Matrices","Design Structure Matrices (DSM) and Domain Mapping Matrices (DMM) are commonly used to model and analyze the relationships within one domain (DSM) or between two domains (DMM). Being assembled into one larger square matrix, having DSMs on its diagonal and DMMs in all other fields, a so-called Multiple Domain Matrix (MDM) is formed. When relating two domains using a DMM, a problem arises when the nature of one individual relationship between the two domains is to be described. Usually, this is modeled by annotating each relationship with the additional information, much like comments in spreadsheet software. This, however, is yet impossible if the relationships should be in matrix notation to allow for algorithmic matrix analyses. Equally, this way, the annotations are not accessible as elements of another matrix, e.g. as DSM. This paper suggests a generic principle to solve the described problem in a way consistent with the matrix methodology. It proposes an approach using MDM and is thereby able to unambiguously provide the nature of each relationship between the elements of two domains. As a DSM is a mere case of a DMM having two identical domains, the approach proposed can equally be used to enrich the relationships within a DSM.",2008
"A Unified Framework for Decomposition of Design Structure Matrix and Domain Mapping Matrix","In literature, design structure matrix (DSM), which is a square matrix, has been widely used to address single-domain dependency relationships (e.g., product architecture, process workflow, and organization structure). To extend the DSM efforts, a rectangular matrix becomes a logical format to capture and analyze cross-domain dependency relationships, namely, domain mapping matrix (DMM) [1]. In this context, this paper proposes a unified framework for decomposition of DSM and DMM. The unified framework consists of four methodological phases to offer the functions of DSM clustering, DSM sequencing, and DMM decomposition. To support the development of this framework, various decomposition-related techniques from applied mathematics and engineering design are reviewed. Three matrix examples have been used to illustrate the framework’s applicability.",2008
"Towards Rules for Functional Composition","Functional decomposition is used in conceptual design to divide an overall problem with an unknown solution into smaller problems with known solutions. The procedure for functional decomposition, however, has not been formalized. In a larger effort to understand and develop rules for functional decomposition, this paper develops rules for composition of reverse-engineered functional models. First, the functional basis hierarchy is used in an attempt to compose the functional model of a hair dryer, which does not produce the desired results. Second, a set of rules for composition is presented and applied to the hair dryer functional model. This composed functional model is more similar to the desired decomposition result than the functional model developed by changing hierarchical levels. Ten additional functional models are also composed and the results shown. The findings demonstrate that composition rules can be developed empirically through analysis of functional models.",2008
"A Deterministic Lagrangian-Based Global Optimization Approach for Large Scale Decomposable Problems","We propose a deterministic approach for global optimization of large-scale nonconvex quasiseparable problems encountered frequently in engineering systems design, such as multidisciplinary design optimization and product family optimization applications. Our branch and bound-based approach applies Lagrangian decomposition to 1) generate tight lower bounds by exploiting the structure of the problem and 2) enable parallel computing of subsystems and the use of efficient dual methods for computing lower bounds. We apply the approach to the product family optimization problem and in particular to a family of universal electric motors with a fixed platform configuration taken from the literature. Results show that the Lagrangian bounds are much tighter than convex underestimating bounds used in commercial software, and the proposed lower bounding scheme shows encouraging efficiency and scalability, enabling solution of large, highly nonlinear problems that cause difficulty for existing solvers. The deterministic approach also provides lower bounds on the global optimum, eliminating uncertainty of solution quality produced by popular applications of stochastic and local solvers. For instance, our results demonstrate that prior product family optimization results reported in the literature obtained via stochastic and local approaches are suboptimal, and application of our global approach improves solution quality by about 10%. The proposed method provides a promising scalable approach for global optimization of large-scale systems-design problems.",2008
"A Structure-Based Approach to Measuring Adaptability of Product Design","Adaptable design aims to extend the utilities of designs and products. Adaptability is classified into product and design adaptabilities. Product adaptability indicates that functionality and life can be extended for both economical and environmental benefits. Design adaptability improves design reuse by using existing designs to develop new designs more efficiently. To evaluate adaptable design, it is necessary to develop a method for quantitative measurement of the adaptability of products. A new method has been developed that first analyzes the independencies of functions and functional modules and then evaluates the adaptability of interfaces with two indices, as well as the performances of adaptive requirements. The effectiveness of the proposed method is demonstrated by an illustrative example of personal computer motherboards. The results show that the method can evaluate and reveal product adaptability for improving design and providing innovative design.",2008
"Transformation Facilitators: A Quantitative Analysis of Reconfigurable Products and Their Characteristics","Products that transform into multiple states give access to greater flexibility and functionality in a single system. These “transformers” capture the imagination and can be elegant, compact, and convenient. Mechanical transformers are usually designed ",2008
"Towards Adaptable Architecture","There is a need for products that can automatically adapt to various environmental and working conditions. Since a standard theoretical framework for designing such adaptable products is not yet established, only few rules, generalized methods, software tools, and guidelines for design for adaptability can be found in literature. The goal of this paper is to address issues associated with designing adaptable product architecture and to propose a first step to develop methods and tools to deal with these issues. The paper first gives various definitions and an overview of product adaptability. Then it discusses adaptable product architectures, external conditions, and customer needs that are crucial aspects in designing adaptable products. The research proposes a scheme of an adaptable product that can constitute part of a formalized design method for adaptability. Finally, it illustrates the design choices that should be made to arrive at an adaptable product architecture.",2008
"Should Designers Worry About Market Systems?","Engineering approaches for optimizing designs within a market context generally take the perspective of a single producer, asking what design and price point will maximize producer profit predicted by consumer choice simulations. These approaches treat competitors and retailers as fixed or nonexistent, and they take business-oriented details, such as the structure of distribution channels, as separate issues that can be addressed ",2008
"A Pareto Approach to Aligning Public and Private Objectives in Vehicle Design","The quest for producing vehicles friendlier to the environment is often impeded by the fact that a producer private good objective, such as maximum profit, competes with the public good objective of minimizing impact on the environment. Contrary to commercial claims, there may be no defined decision maker in the vehicle production and consumption process who takes ownership of the public good objective, except perhaps the government. One way ecofriendly products could become more successful in the marketplace is if public and private good objectives become more aligned to each other. This paper introduces three metrics for comparing Pareto curves in bi-objective problems in terms of relative level of objective competition. The paper also presents a quantitative way of studying an individual firm’s trade-off between profit and fuel consumption for automotive products, currently undergoing an historic evolution in their design. We show how changes in technology, preferences, competition, and regulatory scenarios lead to Pareto frontier changes, possibly eliminating it altogether.",2008
"Optimal Product Design Under Price Competition","Engineering optimization methods for new product development model consumer demand as a function of product attributes and price in order to identify designs that maximize expected profit. However, prior approaches have ignored the ability of competitors to react to a new product entrant; thus these methods can overestimate expected profit and select suboptimal designs that perform poorly in a competitive market. We propose an efficient approach to new product design accounting for competitor pricing reactions by imposing Nash and Stackelberg conditions as constraints, and we test the method on three product design case studies from the marketing and engineering design literature. We find that a Stackelberg leader strategy generates higher profit than a Nash strategy. Both strategies are superior to ignoring competitor reactions: In our case studies, ignoring price competition results in overestimation of profits by 12%–79%, and accounting for price competition increases realized profits by up to 3.4%. The efficiency, convergence stability, and ease of implementation of the proposed approach enables practical implementation for new product design problems in competitive markets.",2008
"Multi-Category Design of Bundled Products for Retail Channels Considering Demand Uncertainty and Competition","Modern retailers now control in excess of 70% of many markets and thereby control the access manufacturers have to the end customer. Success of a new product design therefore depends upon acceptance of the product by the powerful retailer as well as the end customer. One prevalent approach to increasing both retailer and manufacturer revenues is to improve the attractiveness of a product offering by bundling related items together for one price. To be most effective, bundled products should be developed with an integrated design approach that seeks to achieve utility for the end customer as well as cost efficiencies through measures such as using common parts. We propose a bundled product design approach that endogenizes the profit maximizing prices set by the channel controlling (monopolist) retailer. The approach accounts for demand dependencies between the product categories and thus the impact of the bundle and cross-category competition on proposed engineering designs is known. Additionally, the approach simultaneously considers uncertainty in engineering design, competing manufacturer product attributes and customer preferences to ensure acceptable product profitability and market share under interval uncertainty. A bundled product design case study is presented for two complimentary power tools. Manufacturer profit and market share are optimized both deterministically and under uncertainty. We find that considering demand dependencies can create optimal bundle and individual product designs that increase profits for both retailer and manufacturer.",2008
"Optimal Experimental Design of Human Appraisals for Modeling Consumer Preferences in Engineering Design","Human appraisals are becoming increasingly important in the design of engineering systems to link engineering design attributes to customer preferences. Human appraisals are used to assess consumers’ opinions of a given product design, and are unique in that the experiment response is a function of both the product attributes and the respondents’ demographic attributes. The design of a human appraisal is characterized as a split-plot design, in which the respondent demographic attributes form the whole-plot factors while the product attributes form the split-plot factors. The experiments are also characterized by random block effects, in which the design configurations evaluated by a single respondent form a block. An experimental design algorithm is needed for human appraisal experiments because standard experimental designs often do not meet the needs of these experiments. In this work, an algorithmic approach to identify the optimal design for a human appraisal experiment is developed, which considers the effects of respondent fatigue and the block and split-plot structure of such a design. The developed algorithm seeks to identify the experimental design which maximizes the determinant of the Fisher Information Matrix, labeled as the ",2008
"Elicitation and Modeling of Customers’ Preferences in Industrial Design: A Comparative Study on Vehicle Front End","The understanding of customer preference, in relation to product attributes, is a key challenge in industrial design. It is based on preference measurement, which is a complex task, crucial for sound design decisions. In this paper, we study experimentally the influence of the preference elicitation method on the results of the test. For this, we designed a case study concerning the perception of vehicle’s front-end by a panel of subjects. Two different preference elicitation methods were used in the same session: direct rating on a scale, and pairwise comparison. Conjoint Analysis was used next to model the preference and to compute the part worth of the different design factors. Two conjoint methods were examined: rating based and choice based. The paper presents a comparison of the results of the different modeling with conjoint analysis, in order to assess the reliability and the validity of the models.",2008
"Choice Model Specification in Market-Based Engineering Design","The application of market demand models in engineering design is now a well-established practice. One could consider the archetypical application to be a random utility model used in conjunction with a parametric design representation to optimize the design of a single product with respect to a risk-adjusted measure of profit. Much of the work in this area over the past decade has been focused on various extensions of this archetypical framework, such as problem decomposition and product family design. A wide variety of market demand models have been applied, including models derived from classic economic methods and random utility models spanning from multinomial logit through generalized extreme value to mixed logit. While there has been some discussion of the properties of the various choices of market demand models used in prior work, the most recent work in this area suggests that the consequences of market demand model specification in engineering design problems are both more significant than once realized and not yet fully understood. In this paper, we explore the consequences of market demand model specification specifically in the context of engineering design through both a review of prior work and an illustrative example problem featuring a market demand model parameterized in terms of reservation price. These results demonstrate that choices in market demand model specification — especially those relating to representation of customer heterogeneity — can lead to substantially different conclusions in a discrete product configuration design problem.",2008
"Establishing Correspondence Between Value Functions Used in Market-Based Engineering Design","The benefits of applying market demand models in engineering design have been well established. Arguably the most common approach has been to apply a random utility model, although any representation of market demand based on customers’ responses to surveys or in market situations can be used to this end. In practice, various methods have been used to collect market data through the use of surveys, typically based either on choices from among sets of alternatives or statements of willingness-to-pay. The applicability of the data collected has been limited, however, by the complexity of the relationships between market research methods and market demand models. The prospects of reusing an enormous amount of available market research data and increasing practitioners’ freedom in applying market research data to forecast market demand compel us to investigate the correspondence between market demand models. In this paper, we build on the recent work of Cook and Wissmann to demonstrate a bilateral correspondence between the popular multinomial logit model and Cook’s S-Model. Results from two independent case studies are presented showing that the utility function in a multinomial logit model may be reparameterized as a value function in the S-Model and vice-versa. The techniques demonstrated in this paper for defining direct comparisons between previously incompatible market research methods and demand models provide the practitioner with powerful new capabilities for validating market models, fusing market models to expand customer-based trade spaces, and presenting results to decision-makers in familiar formats.",2008
"Fuzzy Controlled Hooke-Jeeves Optimization Search Algorithm","This paper presents an approach to enhance Hooke-Jeeves optimization search algorithm using fuzzy logic. Hooke-Jeeves algorithm, similar to many other search algorithms, use predetermined fixed parameters. These parameters do not depend on the objective function values at the current search location. Fuzzy logic controllers are incorporated in at various stages of the algorithm to create a new optimization search algorithm: Fuzzy-Controlled Hooke-Jeeves algorithm. The results of this paper shows that incorporating fuzzy logic in Hooke-Jeeves algorithm can enhance the ability of the algorithm to reach extremum in different typical optimization test cases and design problems.",2008
"Evolutionary Multi-Agent Systems: An Adaptive Approach to Optimization in Dynamic Environments","This paper explores the ability of a team of autonomous software agents to be effective in unknown and changing optimization environments by evolving to use the most successful algorithms at the points in the optimization process where they will be the most effective. We present the core framework and methodology which has potential applications in layout, scheduling, manufacturing, and other engineering design areas. The communal agent team organizational structure employed allows cooperation of agents through the products of their work and creates an ever changing set of individual solutions for the agents to work on. In addition, the organizational structure allows the framework to be adaptive to changes in the design space that occur during the optimization process — making our approach extremely flexible to the kinds of dynamic environments encountered in engineering design problems. An evolutionary approach is used, but evolution occurs at the strategic, rather than solution level — where the strategies of agents in the team (the decisions for picking, altering, and inserting a solution) evolve over time. As an application of this approach, individual solutions are tours in the familiar combinatorial optimization problem of the traveling salesman. With a constantly changing set of these tours, the team, each agent running a different solution strategy, must evolve to apply the solution strategies which are most useful given the set at any point in the process. As a team, the evolutionary agents produce better solutions than any individual algorithm. We discuss the extensions to our preliminary work that will make our framework highly useful to the design and optimization community.",2008
"Incorporating Twinkling in Genetic Algorithms for Global Optimization","Genetic algorithms have been extensively used as a reliable tool for global optimization. However these algorithms suffer from their slow convergence. To address this limitation, this paper proposes a two-fold approach to address these limitations. The first approach is to introduce a twinkling process within the crossover phase of a genetic algorithm. Twinkling can be incorporated within any standard algorithm by introducing a controlled random deviation from its standard progression to avoiding being trapped at a local minimum. The second approach is to introduce a crossover technique: the weighted average normally-distributed arithmetic crossover that is shown to enhance the rate of convergence. Two possible twinkling genetic algorithms are proposed. The performance of the proposed algorithms is successfully compared to simple genetic algorithms using various standard mathematical and engineering design problems. The twinkling genetic algorithms show their ability to consistently reach known global minima, rather than nearby sub-optimal points with a competitive rate of convergence.",2008
"Evaluating the Performance of Visual Steering Commands for User-Guided Pareto Frontier Sampling During Trade Space Exploration","Trade space exploration is a promising decision-making paradigm that provides a visual and more intuitive means for formulating, adjusting, and ultimately solving design optimization problems. This is achieved by combining multi-dimensional data visualization techniques with visual steering commands to allow designers to “steer” the optimization process while searching for the best, or Pareto optimal, designs. In this paper, we compare the performance of different combinations of visual steering commands implemented by two users to a multi-objective genetic algorithm that is executed “blindly” on the same problem with no human intervention. The results indicate that the visual steering commands — regardless of the combination in which they are invoked — provide a 4x–7x increase in the number of Pareto solutions that are obtained when the human is “in-the-loop” during the optimization process. As such, this study provides the first empirical evidence of the benefits of interactive visualization-based strategies to support engineering design optimization and decision-making. Future work is also discussed.",2008
"Improving the Performance of Visual Steering Commands for Multi-Dimensional Trade Space Exploration","Designers perform many tasks when developing new products and systems, and making decisions may be among the most important of these tasks. The trade space exploration process advocated in this work provides a visual and intuitive approach for formulating and solving single- and multi-objective optimization problems to support design decision-making. In this paper, we introduce an advanced sampling method to improve the performance of the visual steering commands that have been developed to explore and navigate the trade space. This method combines speciation and crowding operations used within the Differential Evolution (DE) algorithm to generate new samples near the region of interest. The accuracy and diversity of the resulting samples are compared against simple Monte Carlo sampling as well as the current implementation of the visual steering commands using a suite of test problems and an engineering application. The proposed method substantially increases the efficiency and effectiveness of the sampling process while maintaining diversity within the trade space.",2008
"Finite Element Model Updating Approach to Damage Identification in Beams Using Particle Swarm Optimization","The use of vibration-based techniques in damage identification has recently received considerable attention in many engineering disciplines. While various damage indicators have been proposed in the literature, those relying only on changes in the natural frequencies are quite appealing since these quantities can conveniently be acquired. Nevertheless, the use of natural frequencies in damage identification is faced with many obstacles, including insensitivity and non-uniqueness issues. The aim of this paper is to develop a viable damage identification scheme based only on changes in the natural frequencies and to attempt to overcome the challenges typically encountered. The proposed methodology relies on building a Finite Element Model (FEM) of the structure under investigation. A modified Particle Swarm Optimization (PSO) algorithm is proposed to facilitate updating the FEM in accordance with experimentally-determined natural frequencies in order to predict the damage location and extent. The method is tested on beam structures and was shown to be an effective tool for damage identification.",2008
"Enhanced Multi-Agent Normal Sampling Technique for Global Optimization","This paper describes an enhanced version of a new global optimization method, Multi-Agent Normal Sampling Technique (MANST) described in reference [1]. Each agent in MANST includes a number of points that sample around the mean point with a certain standard deviation. In each step the point with the minimum value in the agent is chosen as the center point for the next step normal sampling. Then the chosen points of all agents are compared to each other and agents receive a certain share of the resources for the next step according to their lowest mean function value at the current step. The performance of all agents is periodically evaluated and a specific number of agents who show no promising achievements are deleted; new agents are generated in the proximity of those promising agents. This process continues until the agents converge to the global optimum. MANST is a standalone global optimization technique and does not require equations or knowledge about the objective function. The unique feature of this method in comparison with other global optimization methods is its dynamic normal distribution search. This work presents our recent research in enhancing MANST to handle variable boundaries and constraints. Moreover, a lean group sampling approach is implemented to prevent sampling in the same region for different agents. The overall capability and efficiency of the MANST has been improved as a result in the newer version. The enhanced MANST is highly competitive with other stochastic methods such as Genetic Algorithm (GA). In most of the test cases, the performance of the MANST is significantly higher than the Matlab™ GA Toolbox.",2008
"A Fast and Efficient Compact Packing Algorithm for Free-Form Objects","In this paper, a compact packing algorithm for the placement of objects inside a container is described. The proposed packing algorithm packs three-dimensional free-form objects inside an arbitrary enclosure such that the packing efficiency is maximized. The proposed packing algorithm can handle objects with holes or cavities and its performance does not degrade significantly with the increase in complexity of the enclosure or the objects. The packing algorithm takes as input the triangulated geometry of the container and all the objects to be packed and outputs the list of objects that can be placed inside the enclosure. The packing algorithm also outputs the location and orientation of all the objects, the packing sequence, and the packed configuration. An improved layout algorithm that works with arbitrary container geometry is also proposed. Several heuristics to improve the performance of the packing algorithm as well as certain aspects that facilitate fast and efficient handling of CAD data are also discussed. A comprehensive benchmarking of the proposed packing algorithm on synthetic and hypothetical problems reflects its superior performance as compared to other similar approaches.",2008
"Analytical Robust Design of Mechanical Systems","Based on general principles of robust design and axiomatic design, relationship among robustness, structural parameters, design parameters and uncontrollable factors has been established. Various factors that affect system robustness were analyzed mathematically to determine the relationship between robustness and structural characteristics of the linear system. The relations among functional requirements were also explored. Accordingly, an optimization model was established to determine design parameters. This new robust design approach can be used for linear mechanical system analysis.",2008
"Modeling Variability in Torso Shape for Chair and Seat Design","Anthropometric data are widely used in the design of chairs, seats, and other furniture intended for seated use. These data are valuable for determining the overall height, width, and depth of a chair, but contain little information about body shape that can be used to choose appropriate contours for backrests. A new method is presented for statistical modeling of three-dimensional torso shape for use in designing chairs and seats. Laser-scan data from a large-scale civilian anthropometric survey were extracted and analyzed using principal component analysis. Multivariate regression was applied to predict the average body shape as a function of overall anthropometric variables. For optimization applications, the statistical model can be exercised to randomly sample the space of torso shapes for automated virtual fitting trials. This approach also facilitates trade-off analyses and other the application of other design decision-making methods. Although seating is the specific example here, the method is generally applicable to other designing for human variability situations in which applicable body contour data are available.",2008
"A General Approach to Robustness Assessment for Multidisciplinary Systems","In many engineering applications, both random and interval variables may exist. The mixture of both types of variables has been dealt with in robust design for single disciplinary systems. This work focuses on robustness assessment for general multidisciplinary systems with the presence of both random and interval variables. To alleviate the intensive computational demand, we propose a semi-second-order Taylor expansion method to estimate robustness. A performance function is approximated linearly in terms of random variables, but the interaction terms between random and interval variables are kept. Robustness is measured by the standard deviation of a system performance. The maximum and minimum standard deviations over the interval variables are identified. With proposed method, the impact of both random and interval variables on the system performance can be found efficiently.",2008
"Predicting 5th and 95th Percentile Anthropometric Segment Lengths From Population Stature","Designing for human variability frequently necessitates an estimation of the spatial requirements of the intended user population. These measures are often obtained from “proportionality constants” which predict the lengths of relevant anthropometry using stature. This approach is attractive because it is readily adapted to new populations—only knowledge of a single input, stature, is necessary to obtain the estimates. The most commonly used ratios are those presented in Drillis and Contini’s report from 1966 [1]. Despite the prevalence of their use, these particular values are limited because the size and diversity of the population from which these ratios were derived is not in the literature, and the actual body dimensions that each ratio represents are not clear. Furthermore, they are often misinterpreted and used inappropriately. This paper introduces a new approach, the “boundary ratio” which mitigates many of these issues. Boundary ratios improve on the traditional application of proportionality constants by: 1) explicitly defining the body dimensions, 2) defining constants for the 5th , 50th , and 95th  percentile measures, and 3) providing distinct constants for males and females when necessary. This approach is shown to better model the range of variability exhibited in population body dimensions.",2008
"An Integrated Process Planning System Architecture for Machining and On-Machine Inspection","Inspection is an essential part of the entire manufacturing chain providing measurement feedback to the process planning system. Fully automated machining requires automatic inspection process planning and real-time inspection results feedback. As inspection process planning is still based on G&M codes containing low-level information or vendor-specific bespoke routines, inspection process planning is mostly isolated from machining process planning. With the development of new data model standards STEP and STEP-NC providing high-level product information for the entire manufacturing chain, it is achievable to combine machining and inspection process planning to generate optimal machining and inspection sequences with real-time measurement results feedback. This paper introduces an integrated process planning system architecture for combined machining and inspection. In order to provide real-time inspection feedback, On-Machine Inspection (OMI) is chosen to carry out inspection operations. Implementation of the proposed architecture has been partially carried out with a newly developed data model and interpreter software. A case study was carried out to test the feasibility of the proposed architecture.",2008
"Direct 3D Layer Metal Deposition and Toolpath Generation","Multi-axis slicing for solid freeform fabrication (SFF) manufacturing process can yield non-uniform thickness layers, or 3-D layers. Using the traditional parallel layer construction approach to build such a layer leads to a staircase which requires machining or other post processing to form the desired shape. This paper presents a direct 3-D layer deposition approach. This approach uses an empirical model to predict the layer thickness based on experimental data. The toolpath between layers is not parallel; instead, it follows the final shape of the designed geometry and the distance between the toolpath in the adjacent layers varies at different locations. Directly depositing a 3-D layer not only eliminates the staircase effect, but also improves the manufacturing efficiency by shortening the deposition and machining times. A single track deposition experiment has demonstrated these advantages. Thus, it is a beneficial addition to the traditional parallel deposition method.",2008
"A Method and Software Prototype to Support On-Line Planning on CNC Machines","Process planning for parts made on CNC machines is usually performed in sequence and carried out without feeding back information about the fabrication process to the planning stage. Shifting planning capabilities to the machine level enables consideration of direct machine feedback. In this paper a method for feature decomposition and tool path planning for pocket milling on a 3 axis milling machine is presented. Based on standard computerized numerical control (CNC) technology, G-code is transferred to the fabrication machine whereat adaptation of processing parameters can take place within a feature. In order to enable a feedback loop from the processing to the planning stage, subfeature elements are defined. Interrelations between subsequent elements are identified and standardized with regard to the manufacturing needs to realize fully machined features. For each element a tool path is generated and translated into valid G-code. After fabrication of each subfeature element processing parameters can be improved. The presented method is implemented in a prototype software tool based on an open source CAD/CAM/CAE kernel and extends an existing open source CAM software framework.",2008
"Locating Precision of Fit Datums","One of the methods design engineers use to achieve alignment between precision features of two mating parts is through locating or alignment features. The most common locating features are a pair of small holes and pins that would define the in-plane position and orientation of two parts with respect to each other. These locating features establish a datum frame for all other critical features of a part. Design engineers would be interested to know the worst-case and probabilistic value of misalignments resulting from their choices of tolerances that assure fit of the datum features as well as their choices of tolerances that locate the precision features with respect to such a datum. This paper uses two-dimensional tolerance analysis methods to formulate this alignment problem. Worst-case estimates as well as Monte Carlo simulation have been used to analyze the misalignments resulting from play in the datum fit. It is shown that a number of dimensionless parameters can characterize the misalignment and these are used to create graphs adequate for design work.",2008
"Identifying Feature Handles of Freeform Shapes","Trends, ergonomics and engineering analysis post more challenges than ever to product shape designs, especially in the freeform area. In this paper, freeform feature handles are proposed for easing of difficulties in modifying an existing freeform shape. Considering the variations of curvature as the footprint of a freeform feature(s), curvature analysis is applied to find manipulators, e.g. handles, of a freeform feature(s) in the shape. For these, a Laplacian based pre-processing tool is proposed first to eliminate background noise of the shape. Then least square conformal mapping is applied to map the 3D geometry to a 2D polygon mesh with the minimum distortions of angle deformation and non-uniform scaling. By mapping the curvature of each vertex in the 3D shape to the 2D polygon mesh, a curvature raster image is created. With image processing tools, different levels of curvature changing are identified and marked as feature point(s) / line(s) / area(s) in the freeform shape. Following the definitions, the handles for those intrinsic freeform features are established by the user based on those feature items. Experiments were conducted on different types of shapes to verify the rightness of the proposed method. Different effects caused by different parameters are discussed as well.",2008
"Contact Analysis Between a Moving Solid and the Boundary of Its Swept Volume","The modeling of many practical problems in design and manufacturing involving moving objects relies on sweeps and their boundaries, which are mathematically described by the envelopes to the family of shapes generated by the moving object. In many problems, such as the design and analysis of higher pairs or tool path planning, contact changes between the moving object and the boundary of its swept volume become critical because they often translate into functional changes of the system under consideration. However, the difficulty of this task quickly escalates beyond the reach of existing approaches as the complexity of the shape and motion increases. We recently proposed a sweep boundary evaluator for general sweeps in conjunction with efficient point sampling and surface reconstruction algorithms that relies on our novel point membership classification (PMC) test for general solid sweeps. In this paper we describe a new approach that automates the prediction of changes in the state of contact between a shape of arbitrary complexity moving according to an affine motion, and the boundary of its swept set. We show that we can predict when and where such contact changes occur with only minimal additional computational cost by exploiting the data output by our sweep boundary evaluator. We discuss the problem and the associated computational issues in a 2D framework, and we conclude by discussing the extension of our approach to 3D moving objects.",2008
"A Hybrid Models Deformation Tool for Free-Form Shapes Manipulation","This paper addresses the way models mixing various types of geometric representations (e.g. NURBS curves and patches, polylines, meshes), potentially immersed in spaces of different dimensions (e.g. NURBS patch and its 2D trimming lines), can be deformed simultaneously. The application domains range from the simple deformation of a set of NURBS curves in a 2D sketcher to the simultaneous deformation of meshes, patches as well as trimming lines lying in parametric spaces. The deformation itself results from the solution of an optimization problem defined by a set of geometric constraints and deformation behaviors. This new breakthrough on how geometric models can be manipulated has been made possible thanks to our linear mechanical model of deformation that can be coupled to manifolds of dimension zero (e.g. points, vertices) and one (e.g. edges, segments) whatever the spaces dimension. An extended constraints toolbox is also proposed that enables the specification of both characteristic points/curves and continuity conditions between the various geometric models. The link between the semantics of the deformation behaviors and the geometric models is ensured through the use of multiple minimizations. The approach is illustrated with several examples coming from our prototype software.",2008
"Interval Extensions of Signed Distance Functions: Uniform Samplings and the Range of Associated Implicit Objects","This paper considers the problem of inferring the geometry of an object from values of the signed distance sampled on a uniform grid. The problem is motivated by the desire to effectively and efficiently model objects obtained by 3D imaging technology that is now ubiquitous in medical diagnostics. Recently developed techniques for automated segmentation convert intensity to signed distance, and the voxel structure imposes the uniform sampling grid. While specification of the signed distance function throughout the ambient space would provide an implicit model that uniquely specifies the object, a set of uniformly sampled signed distance values may uniquely determine neither the distance function nor the shape of the object. Here we employ essential properties of the signed distance to construct upper and lower bounds on the allowed variation in signed distance in 1, 2, and 3 dimensions. The bounds are combined to produce interval-valued extensions of the signed distance function including a tight global extension and more computationally efficient local bounds that provide useful criteria for root exclusion/isolation.",2008
"Shape Optimization of a Camoid Follower Surface","Camoids are three dimensional cams that can produce more complex follower output than plain disc cams. A camoid follower motion is described by a surface rather than a curve. The camoid profile can be directly synthesized once the follower surface is fully described. To define a camoid follower motion surface it is required that the surface pass by all predefined constraints. Constraints can be follower position, velocity and acceleration. These design constraints are scattered all along the camoid follower surface. Hence a fitting technique is needed to satisfy these constraints which include position and its derivatives (velocity and acceleration). Furthermore if the fitting function can be of a parametric nature, then it would be possible to optimize the follower surface to obtain better performance according to a specific objective. Previous research has established a method to fit camoid follower surface positions, but did not tackle the satisfaction of derivative constraints. This paper presents a method for defining a camoid follower characteristic surface B-Splines on two steps first synthesizing the sectional cam curves then using a surface interpolation technique to generate the follower characteristic surface. The fitting technique is parametric in nature which allows for its optimization. Real coded Genetic algorithms are used to optimize the parameters of the surface to meet a specified objective function. A demonstration problem to illustrate the suggested methodology is presented.",2008
"Euclidean Symmetry Detection From Scanned Meshes Based on a Combination of ICP and Region Growing Algorithms","Recently, meshes of engineering objects have been easily acquired by 3D laser or high-energy industrial X-ray CT scanning systems and they are widely used in product developments. For the effective use of scanned meshes in inspection, re-design, and simulation of the objects, it is important to reconstruct CAD models from the meshes. Engineering objects often exhibit Euclidean symmetries for their functionalities. Therefore, it is essential to detect such symmetries when reconstructing CAD models with compact data representations which are similar to the ones already defined in CAD systems. However, existing methods for reconstructing CAD models have not focused on detecting such symmetries. In this paper, we propose a new method that detects partial or global Euclidean symmetries, including translation, rotation, and reflection, from scanned meshes of engineering objects based on the combination of the ICP and the region growing algorithms. Our method can robustly and efficiently extract pairs of symmetric regions and their transformations under which the pair can be closely matched to each other. We demonstrate the effectiveness of the proposed method from experiments on various scanned meshes.",2008
"A Measure of the Information Loss for Inspection Point Reduction","Since the vehicle program in automotive industry gets more and more extensive, the costs related to inspection increase. Therefore, there are needs for more effective inspection preparation. In many situations, a large number of inspection points are measured, despite the fact that only a small subset of points is needed. A method, based on cluster analysis, for identifying redundant inspection points has earlier been successfully tested on industrial cases. Cluster analysis is used for grouping the variables into clusters, where the points in each cluster are highly correlated. From every cluster only one representing point is selected for inspection. In this paper the method is further developed and multiple linear regression is used for evaluating how much of the information that is lost when discarding an inspection point. The information loss can be quantified using an efficiency measure based on linear multiple regression, where the part of the variation in the discarded variables that can be explained by the remaining variables is calculated. This measure can be illustrated graphically and that helps to decide how many clusters that should be formed, i.e. how many inspection points that can be discarded.",2008
"A Comparison of Virtual Condition Cylinder Evaluation Methods in Coordinate Metrology","When a cylindrical datum feature is specified at maximum material condition (MMC) or least material condition (LMC) a unique circumstance arises: a virtual condition (VC) cylindrical boundary must be defined [1]. The geometric relationship between a cylindrical point cloud obtained from inspection equipment and a VC cylinder has not been specifically addressed in previous research. In this research, novel approaches to this geometric analysis are presented, analyzed, and validated. Two of the proposed methods are new interpretations of established methods applied to this unique geometric circumstance: least squares and the maximum inscribing cylinder (MIC) or minimum circumscribing cylinder (MCC). The third method, the Hull Normal method, is a novel approach specifically developed to address the VC cylinder problem. Each of the proposed methods utilizes a different amount of sampled data, leading to various levels of sensitivity to sample size and error. The three methods were applied to different cylindrical forms, utilizing various sampling techniques and sample sizes. Trends across sample size were analyzed to assess the variation in axial orientation when compared to the true geometric form, and a relevant case study explores the applicability of these methods in real world applications.",2008
"A Subdivision Scheme for Discrete Motion Generation and Swept Volume Analysis","In this paper, the four-point interpolatory subdivision scheme for curve generation is adapted to the interpolation of a set of positions of a cylindrical tool represented by dual quaternions. The resulting discrete model of the tool path lends itself naturally to an algorithm for computing the characteristic curve belonging to the boundary surface of the swept volume of a cylinder at each of the discrete positions. This approach to compute the discrete model of the swept surface of a motion is numerically robust and computationally efficient since it is based only on linear combinations. The results have applications in NC simulation and verification, robot path planning, and computer graphics.",2008
"Novel Design of a 3-DOF Parallel Manipulator for Materials Handling","In this paper, a new design of a parallel manipulator is proposed for industrial applications, specifically for material surface finishing processes. Though most current parallel mechanisms have been based on the Stewart-Gough platform which has 6 degrees of freedom (DOF), the focus of this design is on a 3-DOF manipulator with one novel configuration. In order to benefit production, a parallel kinematic machine (PKM) capable of high speed industrial operations with high accuracy and rigidity is necessary. First, system modelling includes mobility study, inverse kinematic model, Jacobian matrix, singularity analysis and workspace calculation are conducted. Then, a CAD model is presented showing the optimum design features and detailed mechanics. Finally, finite element analysis is carried out for the device optimization.",2008
"Benefit Evaluation for Manufacturing of Marine Propellers","To increase productivity of marine propellers by raising machining efficiency, this paper presents the zigzag/spiral tool paths generation algorithm based on the arc base curve approach for three-axis machining of curved surfaces of propellers. By considering the shapes of selected cutters with different types of tool paths generated by the proposed procedure, machining efficiency can be calculated and simulated. To verify the accuracy and effectiveness of the developed approach, numerical and experimental results of machining of propeller surfaces are compared. It was proved that the machining time can be cut down up to 19% by using zigzag tool paths with a toroidal cutter. In addition, the machining knowledge revealed here can be accumulated for benefit evaluation in the manufacturing process with existing CAD/CAM systems. From the cost model, design, and process views, the overall cost savings after 5 years are investigated, and the expected benefit yield is about 45%.",2008
"An Industrial Trial of a Set-Based Approach to Collaborative Design","A set-based multiscale and multidisciplinary design method has been proposed in which distributed designers manage interdependencies by exchanging targets and Pareto sets of solutions. Prior research has shown that the set-based method (SBM) has the potential to reduce the number of costly iterations between design teams, relative to centralized optimization approaches, while expanding the variety of high-quality, system-wide solutions. These results have been obtained with representative examples in a laboratory setting. The goal of this research is to investigate whether similar results are obtained from an industrial trial, implemented in an industry design environment. The SBM is applied to the design of a downhole module for our industrial partners at Schlumberger, a developer of oilfield tools and services. The design was conducted on location at Schlumberger by an intern who converted the existing Schlumberger design process into a set-based design process. Results indicate that the SBM delivers the benefits predicted in the laboratory, along with a host of advantageous side effects, such as a library of back-up design options for future design projects.",2008
"A Finite Element Approach for the Implementation of Magnetostrictive Material Terfenol-D in CNG Fuel Injection Actuation","In applications broadly defined for actuation, magnetostrictive materials possess intrinsic rapid response times while providing small and accurate displacements and high-energy efficiency, which are some of the essential parameters for fast control of fuel injector valves for decreased engine emissions and lower fuel consumption. This paper investigates the application of Terfenol-D as a magnetostrictive actuator material for CNG fuel injection actuation. A prototype fuel injector assembly, which includes Terfenol-D as the core actuator material, was modeled in Finite Element Method Magnetics (FEMM) simulation software for 2D magnetics simulation. FEMM was used in order to determine the coil-circuit parameters and the required flux density or applied magnetic field to achieve the desired magnetostrictive strain, and consequently, the injector needle lift. The FEMM magnetic simulation was carried out with four different types of AWG coil wires and four different coil thicknesses of the entire injector assembly in order to evaluate the relationship between the different coil types and thicknesses against the achieved strain or injector lift.",2008
"Parametric Modeling and Optimization of Chemical Vapor Deposition Process","This paper focuses on the parametric modeling and optimization of the Chemical Vapor Deposition (CVD) process for the deposition of thin films of silicon from silane in a vertical impinging CVD reactor. The parametric modeling using Radial Basis Function (RBF) for various functions which are related to the deposition rate and uniformity of the thin films are studied. These models are compared and validated with additional sampling data. Based on the parametric models, different optimization formulations for maximizing the deposition rate and the working areas of thin film are performed.",2008
"Optimization Design of a Spatial Six-Degree-of-Freedom Parallel Manipulator Based on Genetic Algorithms and Neural Networks","Optimizing the performances of parallel manipulators by adjusting the structure parameters can be a difficult and time-consuming exercise especially when the parameters are multifarious and the objective functions are too complex. Artificial intelligence approaches can be investigated as the effective criteria to address this issue. In this paper, genetic algorithms and artificial neural network are implemented as the intelligent optimization criteria of global stiffness and dexterity for spatial six degree-of-freedom (DOF) parallel manipulator. The objective functions of global stiffness and dexterity are calculated and deduced according to the kinetostatic model. Neural networks are utilized to model the solutions of performance indices. Multi-objective optimization is developed by Pareto-optimal solution. The effectiveness of the proposed methodology is proved by simulation.",2008
"Optimal Kinematics Design of an Industrial Robot Family","Product family design is a well recognized method to address the demands of mass customization. A potential drawback of product families is that the performance of individual members are reduced due to the constraints added by the common platform, i.e. parts and components need to be shared by other family members. This paper presents a formal mathematical framework where the product family design problem is stated as an optimization problem and where optimization is used to find an optimal product family. The object of study is kinematics design of a family of industrial robots. The robot is a serial manipulator where different robots share arms from a common platform. The objective is to show the trade-off between the size of the common platform and the kinematics performance of the robot.",2008
"Topology and Dimensional Synthesis of Linkage Mechanism Based on the Constrained Superposition Method","Gradient search methods for linkage synthesis have difficulty on the design convergence due to the non-convexity and non-uniqueness characteristics on linkage design. Although evolutionary optimization approach can solve non-convex problems and produce multiple optimal solutions, it cannot be applied to linkage synthesis easily because a general and efficient method for mechanism analysis is not available. In order to automate the linkage synthesis process, a novel linkage analysis method, called the Constrained Superposition Method (CSM), is presented in this paper. The CSM is based on the concept of Finite Element Analysis. The CSM can analyze any linkage mechanism with Single-Input-Single-Output (SISO). To further improve the efficiency of evolutionary optimization process, two feasibility checks are introduced to ensure the connectivity and mobility. Finally, four linkage synthesis examples are presented and discussed to demonstrate the effectiveness of this method.",2008
"Genetic Programming of an Artificial Neural Network for Robust Control of a 2-D Path Following Robot","Genetic Programs that have phenotypes created by the application of genotypes comprising rules are robust and highly scalable. Such encodings are useful for complex applications such as controller design. This paper outlines an evolutionary algorithm capable of creating a controller for 2 DOF, path following robot. The controllers are embodied by Artificial Neural Networks capable of full functionality despite multiple failures.",2008
"Decision Support for Strategic Redesign","Researchers have paid relatively little attention to the fact that most of what is considered design is more like redesign than original design. Redesign activities are characterized by an attempt to leverage experience, knowledge, and the capital that a company has already invested into existing engineering systems. In this paper, a method for undertaking strategic redesign is proposed and explained. This method includes support for designers making decisions in redesign problems when there exist systems to be leveraged and multiple new systems to be created. In addition, strategy is introduced to the problem through the consideration that new systems may not be offered all at once, as is often assumed in product family design research. In this paper, the aim of the designer is assumed to be a creation, through redesign, of a series of new systems with desirable and distinct performance levels. In addition, a plan is required to involve as little redesign effort throughout the life of the family of systems as possible. The proposed approach is based upon the concepts of Constructal Theory and previous work to create methods for the design of mass customized families of systems. In addition, two metrics are developed to represent considerations unique to redesign as opposed to original design. These metrics for redesign effort and commonality value are utilized in the overall objective formulation for the proposed approach to redesign. Through a simple redesign scenario involving a family of universal motors, it is shown that the overall approach proposed can lead the designer towards promising redesign plans involving leveraging of existing systems, but that the constructal-inspired approach in and of itself has certain limitations when applied to redesign.",2008
"Designing Embodiment Design Processes Using a Value-of-Information-Based Approach With Applications for Integrated Product and Materials Design","Designers are continuously challenged to manage complexity in embodiment design processes (EDPs), in the context of integrated product and materials design. In order to manage complexity in design processes, a systematic strategy to embodiment design process generation and selection is presented in this paper. The strategy is based on a value-of-information-based ",2008
"Knowledge Management for Fault Tree Analysis Based on Quantity Dimension Indexing","In this paper, the authors propose computerized support for fault tree analysis (FTA) based on a new design knowledge management approach called quantity dimension indexing. FTA is a method of analyzing and visualizing the causes of fault events by expanding a fault event hierarchically to its possible cause events and constructing a tree diagram representing the entire structure of the problem. When a designer finds or encounters a problem during a product design and development process, an effective way of ensuring the security and safety of the product is to identify all the possible causes of the problem by FTA and fix them. Although FTA is an effective method, it is not easy for a designer to construct a complete fault tree without any misunderstanding or oversight. A promising approach for supporting FTA is to utilize a computerized knowledge management method. Although many knowledge management techniques for literal expression have been developed, they are not necessarily suitable for managing the engineering design knowledge of physical phenomena. To solve this problem, the authors propose a new design knowledge management approach called quantity dimension indexing and computerized support for FTA such as the verification of consistency of a fault tree and fault tree construction advice. By analyzing fault tree examples based on actual design activities in a company, the possible feasibility and future promise of the proposed approach are indicated.",2008
"Using Multiple Surrogates for Minimization of the RMS Error in Meta-Modeling","Surrogate models are commonly used to replace expensive simulations of engineering problems. Frequently, a single surrogate is chosen based on past experience. Previous work has shown that fitting multiple surrogates and picking one based on cross-validation errors (",2008
"Optimization of Engine Torque Management Under Uncertainty for Vehicle Driveline Clunk Using Time-Dependent Metamodels","Quality and performance are two important customer requirements in vehicle design. Driveline clunk negatively affects the perceived quality and must be therefore, minimized. This is usually achieved using engine torque management, which is part of engine calibration. During a tip-in event, the engine torque rate of rise is limited until all the driveline lash is taken up. However, the engine torque rise, and its rate can negatively affect the vehicle throttle response which determines performance. Therefore, the engine torque management must be balanced against throttle response. In practice, the engine torque rate of rise is calibrated manually. This paper describes an analytical methodology for calibrating the engine torque, considering uncertainty, in order to minimize the clunk disturbance, while still meeting throttle response constraints. A set of predetermined engine torque profiles which span the practical range of interest, are used and the transmission turbine speed is calculated for each profile using a bond-graph vehicle model. The turbine speed quantifies the clunk disturbance. Using the engine torque profiles and the corresponding turbine speed responses, a time-dependent metamodel is created using principal component analysis and Kriging. The metamodel predicts the turbine speed response due to any engine torque profile and is used in a deterministic and reliability-based optimization which minimizes the clunk disturbance while still meeting the throttle response target. Compared with commonly used production calibration, the clunk disturbance is reduced substantially without negatively affecting the vehicle throttle response.",2008
"A Practical Robust and Efficient RBF Metamodel Method for Typical Engineering Problems","Radial Basis Function (RBF) metamodels have recently attracted increased interest due to their significant advantages over other types of non-parametric metamodels. However, because of the interpolation nature of the RBF mathematics, the accuracy of the model may dramatically deteriorate if the training data set used contains duplicate information, noise or outliers. Also constructing the metamodel may be time consuming whenever the training data sets are large or a high dimensional model is required. In this paper, we propose a robust and efficient RBF metamodeling approach based on data pre-processing techniques that alleviate the accuracy and efficiency issues commonly encountered when RBF models are used in typical real engineering situations. These techniques include 1) the removal of duplicate training data information, 2) the generation of smaller uniformly distributed subsets of training data from large data sets and 3) the quantification and identification of outliers by principal component analysis (PCA) and Hotelling statistics. Simulation results are used to validate the generalization accuracy and efficiency of the proposed approach.",2008
"A Study of Covariance Functions for Multi-Response Metamodeling for Simulation-Based Design and Optimization","The optimal design of complex systems in engineering requires the availability of mathematical models of system’s behavior as a function of a set of design variables; such models allow the designer to find the best solution to the design problem. However, system models (e.g. CFD analysis, physical prototypes) are usually time-consuming and expensive to evaluate, and thus unsuited for systematic use during design. Approximate models, or metamodels, of system behavior based on a limited set of data allow significant savings by reducing the resources devoted to modeling during the design process. In our work in engineering design based on multiple performance criteria, we propose the use of Multi-response Bayesian Surrogate Models (MRBSM) to model several aspects of system behavior jointly, instead of modeling each individually. By doing so, it is expected that the observed correlation among the response variables can be used to achieve better models with smaller data sets. In this work, we study the approximation capabilities of several covariance functions needed for multi-response metamodeling with MRBSM, performing a simulation study in which we compare MRBSM based on different covariance functions against metamodels built individually for each response. Our preliminary results indicate that MRBSM outperforms individual metamodels in 46% to 67% of the test cases, though the relative performance of the studied covariance functions is highly dependent on the sampling scheme used and the actual correlation among the observed response values.",2008
"A Sequential Linearization Technique for Analytical Target Cascading","Large-scale design problems are high dimensional and deeply-coupled in nature. The complexity of such large-scale systems prevents designers from solving them as a whole. Analytical target cascading (ATC) provides a systematic approach in solving decomposed large-scale systems that has solvable subsystems. By coordinating between subsystems, ATC can obtain the same optima as they were undecomposed. However, a convergent coordination requires series of ATC iterations that may hinder the efficiency of ATC. In this research, a sequential linearization technique is proposed to improve the efficiency of ATC. The proposed linearization technique is applied to each ATC iteration, therefore each iteration has all linear subsystems that can be solved with high efficiency. One further motivation of the proposed strategy is its perceived potential in handling multilevel problems with random design variables. As previously studied, the sequential linear programming (SLP) algorithm in [1] provides a good balances between efficiency, accuracy and convergence for single-level design optimization under random design variables. The proposed linearization technique can integrate with the SLP algorithm for multilevel systems. The global convergence of this approach is ensured by a filter to determine the acceptance of the optima at each iteration and the corresponding trust region. A geometric programming example and a structure design example demonstrate the efficiency of the proposed method over standard ATC solution process without loss of accuracy.",2008
"Parallel Implementation of Particle Swarm Optimization (PSO) Through Digital Pheromone Sharing","In this paper, a parallelization model for PSO through sharing of digital pheromones between multiple particle swarms to search n-dimensional design spaces is presented. Digital pheromones are models simulating real pheromones produced by insects for communication to indicate a source of food or a nesting location. Particle swarms search the design space with digital pheromones aiding communication within the swarm to improve search efficiency. Digital pheromones have demonstrated the capability of searching design spaces within PSO in the previous work by authors in both single and coarse granular parallel computing environments. Multiple swarms are simultaneously deployed across various processors in the coarse granular scheme and synchronization is carried out only when all swarms achieved convergence. This was done in an effort to reduce processor-to-processor communication and network latencies. With an appropriate parallelization scheme, the benefits of digital pheromones and swarm communication can potentially outweigh the network latencies resulting in improved search efficiency and accuracy. A swarm is deployed in the design space across different processors to explore this idea. Each part of the swarm is made to communicate with each other through an additional processor. Digital pheromones aiding within a swarm, communication between swarms is facilitated through the developed parallelization model. In this paper, the development and implementation of this method together with benchmarking test cases are presented.",2008
"Consistency Constraint Allocation in Augmented Lagrangian Coordination","Many engineering systems are too complex to design as a single entity. Decomposition-based design optimization methods partition a system design problem into subproblems, and co-ordinate subproblem solutions toward an optimal system design. Recent work has addressed formal methods for determining an ideal system partition and coordination strategy, but coordination decisions have been limited to subproblem sequencing. An additional element in a coordination strategy is the linking structure of the partitioned problem, i.e., the allocation of constraints that guarantee that the linking variables among subproblems are consistent. There can be many alternative linking structures for a decomposition-based strategy which can be selected for a given partition, and this selection should be part of an optimal simultaneous partitioning and coordination scheme. This paper develops a linking structure theory for a particular class of decomposition-based optimization algorithms, Augmented Lagrangian Coordination (ALC). A new formulation and coordination technique for parallel ALC implementations is introduced along with a specific linking structure theory, yielding a partitioning and coordination selection method for ALC that includes consistency constraint allocation. This method is demonstrated using an electric water pump design problem.",2008
"Enhanced Collaborative Optimization: A Decomposition-Based Method for Multidisciplinary Design","Astute choices made early in the design process provide the best opportunity for reducing the life cycle cost of a new product. Optimal decisions require reasonably detailed disciplinary analyses, which pose coordination challenges. These types of complex multidisciplinary problems are best addressed through the use of decomposition-based methods, several of which have recently been developed. Two of these methods are collaborative optimization (CO) and analytical target cascading (ATC). CO was conceived in 1994 in response to multidisciplinary design needs in the aerospace industry. Recent progress has led to an updated version, enhanced collaborative optimization (ECO), that is introduced in this paper. ECO addresses many of the computational challenges inherent in CO, yielding significant computational savings and more robust solutions. ATC was formalized in 2000 to address needs in the automotive industry. While ATC was originally developed for object-based decomposition, it is also applicable to multidisciplinary design problems. In this paper, both methods are applied to a set of test cases. The goal is to introduce the ECO methodology by comparing and contrasting it with ATC, a method familiar within the mechanical engineering design community. Comparison of ECO and ATC is not intended to establish the computational superiority of either method. Rather, these two methods are compared as a means of highlighting several promising approaches to the coordination of distributed design problems.",2008
"Multiobjective Optimization Method for Lifecycle Design of Machine Products","Manufacturing that minimizes the exhaustion of natural resources, energy used, and deleterious environmental impact is increasingly demanded by societies that seek to protect global environments as much as possible. To achieve this, lifecycle design (LCD) is an essential component of product design scenarios, however LCD approaches have not been well integrated in optimal design methods that support quantitative decision making. This study presents a method that yields quantitative solutions through optimization analysis of a conceptual product design incorporating lifecycle considerations. We consider two types of optimization approaches that have different aims, namely, (1) to reduce the use of raw materials and energy consumption, and (2) to facilitate the reuse of the product or its parts when it reaches the end of its useful life. We also focus on how the optimization results differ according to the approach used, from the view point of the 3R concept (Reduce, Reuse and Recycling). Our method obtains optimum solutions by evaluating objectives fitted to each of these two optimization approaches with respect to the product’s lifecycle stages, which are manufacturing, use, maintenance, disposal, reuse and recycling. As an applied example, a simple linear robot model is presented, and Pareto optimum solutions are obtained for the multiobjective optimization problem whose evaluated objectives are the operating accuracy and the different lifecycle costs for the two approaches. The characteristics of the evaluated objectives and design variables, as well as the effects of using material properties as design parameters, are also examined.",2008
"Fundamental Strategies for System Optimization of Machine Product Designs","In machine product designs, a variety of characteristics such as product performances, manufacturing cost, and robustness of characteristics are evaluated, and the need for improvements is increasingly stringent over time. Such characteristics almost always have interrelationships, and systematic evaluation and optimization must be performed to obtain preferable product design solutions. To conduct effective system optimization, the complex interrelationships among characteristics that are included, and sometimes hidden, in the optimization problem, must be understood and dealt with. To construct optimization methodologies for such problems, these relationships must be clarified, and the characteristics simplified. Simplifying the characteristics makes the essence of the optimization problem clearer, and facilitates examining the interrelationships among the simplified characteristics during the optimization process. Based on the simplified characteristics, “priority relationships”, i.e., relationships among simplified characteristics that will be optimized first, and “conflicting relationships”, i.e., tradeoff relationships that will be simultaneously optimized, are obtained. Hierarchical optimization procedures develop naturally as the relationships among the simplified characteristics are clarified. This paper focuses on the priority relationships of characteristics in system optimization procedures. General and specific rules concerning priority relationships are presented, and these form the basis for the constructed optimization procedures. An applied example of a machine tool product design is presented to demonstrate the effectiveness of the proposed methodology.",2008
"Multi-Objective Optimization of Product Configuration","In the context of globalization and mass customization, selecting the appropriate product configuration requires a simultaneous consideration of multiple criteria or objectives, which are in conflict with each other. The large solution space implies that analyzing each feasible solution is a combinatorial problem. Furthermore, no single optimal solution exists; on the contrary, there is a set of valid optimal solutions, i.e., the solution set is Pareto-optimal. We present the configuration problem from the perspective of using two types of attributes: static, i.e., the attributes that have pre-defined and constant values throughout the configuration process, and dynamic, i.e., attributes whose values vary according to decisions that are being made during the configuration process. We pose the product configuration as a multiobjective optimization problem requiring that multiple objective functions cannot be combined into a single objective function. We demonstrate the applicability of using Multi-Objective Genetic Algorithms (MOGA) to solve the problem and converge to a Pareto-optimal solution set from the large number of feasible solutions.",2008
"Plug-In Hybrid Vehicle Simulation: How Battery Weight and Charging Patterns Impact Cost, Fuel Consumption, and CO2 Emissions","Plug-in hybrid electric vehicle (PHEV) technology is receiving attention as an approach to reducing U.S. dependency on foreign oil and emissions of greenhouse gases (GHG) from the transportation sector. Because plug-in vehicles require large batteries for energy storage, battery weight can have a significant impact on vehicle performance: Additional storage capacity increases the range that a PHEV can travel on electricity from the grid; however, the associated increased weight causes reduced efficiency in transforming electricity and gasoline into miles driven. We examine vehicle simulation models for PHEVs and identify trends in fuel consumption, operating costs, and GHG emissions as battery capacity is increased. We find that PHEVs with large battery capacity consume less gasoline than small capacity PHEVs when charged every 200 miles or less. When charged frequently, small capacity PHEVs are less expensive to operate and release fewer GHGs, but medium and large capacity PHEVs are more efficient for drivers that charge every 25–100 miles. While statistics on average commute length suggest that frequent charges are possible, answering the question of which PHEV designs will best help to achieve national goals will require a realistic understanding of likely consumer driving and charging behavior as well as future trends in electricity generation.",2008
"Heat Compensation in Buildings Using Thermoelectric Windows: An Energy Efficient Window Technology","In this paper, we explore the design of thermoelectric (TE) windows for applications in building structures. ",2008
"The Application of Product Platform Design to the Reuse of Electronic Components Subject to Long-Term Supply Chain Disruptions","Component reuse in multiple products has become a popular way to take advantage of the economies of scale across a family of products. Amongst electronic system developers there is a desire to use common electronic parts (chips, passive components, and other parts) in multiple products for all the economy of scale reasons generally attributed to platform design. However, the parts in electronic systems (especially those manufactured and supported over significant periods of time), are subject to an array of long-term lifecycle supply chain disruptions that can offset savings due to part commonality depending on the availability of finite resources to resolve problems on multiple products concurrently. In this paper we address the application of product platform design concepts to determine the best reuse of electronic components in products that are subject to long-term supply chain disruptions such as reliability and obsolescence issues. A detailed total cost of ownership model for electronic parts is coupled with a finite resource model to demonstrate that, from a lifecycle cost viewpoint, there is an optimum quantity of products that can use the same part beyond which costs increase. The analysis indicates that the optimum part usage is not volume dependent, but is dependent on the timing of the supply chain disruptions. This work indicates that the risk and timing of supply chain disruptions should be considered in product platform design.",2008
"Product Family Commonality Selection Through Interactive Visualization","High dimensionality and computational complexity are curses typically associated with many product family design problems. In this paper, we investigate interactive methods that combine two traditional technologies — optimization and visualization — to create new and powerful strategies to expedite high dimensional design space exploration and product family commonality selection. In particular, three different methods are compared and contrasted: (1) exhaustive search with visualization, (2) individual product optimization with visualization, and (3) product family optimization with visualization. Among these three, the individual product optimization with visualization methods appears to be the most suitable one for engineer designers, who do not have strong optimization background. This method allows designers to “shop” for the best designs iteratively, while gaining key insight into the tradeoff between commonality and individual performance. The study is conducted in the context of designing a UTC product using an in-house, system-level simulation tool. The challenges associated with (1) design space exploration involving mixed-type design variables and infeasibility, and those associated with (2) visualizing ",2008
"Assessing Functional and Shape Differentiation Within a Family of Products","Market differentiation strategies must identify competitive advantages when offering a line of products varying in features, price, quality, and/or aesthetics. Although this concept is well-known, many companies still have difficulties positioning their own products within their own product lines and against competitors. Few approaches combine two or more facets to answer the product differentiation problem. In this study, two novel indices are proposed to audit shape and functional differentiation within a family of products. The shape index appraises the shape similarity between the products upon digitization, while the functional assessment is based on functions characteristics of the product. Customers’ perception data is obtained experimentally and compared to these indices to validate the result. Pairs of products are evaluated, and the average scores are considered as the indices for a product family. A case study illustrates the usage of these two indices and performance of these tools as well. This approach can be used during detailed studies as well as early stages of the design process to help validate product family positioning.",2008
"Product Family Design: Strategic Principles to Choose Between Product-Driven and Platform-Driven Processes","The development process is a key aspect of ultimate product success. The front-end of the development activity is the foundation for building new products by first gathering customers’ needs, identifying the company’s goals, and assessing the competitive landscape. By doing so, this crucial activity directly impacts eventual development cost, which includes engineering resources, manufacturing, etc. In this paper, we study a specific design approach, namely, product family design, which allows companies to increase revenue by developing an entire family of products targeting different market segments while reducing lead-time and manufacturing costs. However, there is a significant amount of risk given the costs of developing complex shared architectures, and there are many examples from industry where product families have failed. Thus, the development stage is critical, and a well-structured development strategy can bring success while a poor one can cause significant problems during product launch, as recent case studies illustrate. In this platform-based study, we assess two drivers of this product family design: (1) a platform-driven strategy and (2) a product-driven strategy. Three facets are examined: the product, the company, and the competition. The goal is to recommend a planning framework to aid companies in selecting the right process considering their product, strategy, and environment.",2008
"Improving Cost Effectiveness in an Existing Product Line Using Component Product Platforms","Previously, we introduced a new method for improving commonality in a highly customized, low volume product line using component product platforms. The method provides a bottom-up platform approach to redesign family members originally developed one-at-a-time to meet specific customer requirements. In this paper, we extend the method with an Activity-Based Costing (ABC) model to specifically capture the manufacturing costs in the product line, including the cost associated with implementing a platform strategy. The valve yoke example is revisited in this paper, the customized ABC model is defined, two design strategy alternatives are addressed, and the new method is used to determine which alternative is better at resolving the tradeoff between commonality, total cost, and product performance. The proposed method shows promise for creating a product platform portfolio from a set of candidate component platforms that is most cost-effective within an existing product line. The proposed method allows for arbitrary leveraging as it does not rely solely on the traditional vertical, horizontal, or beachhead strategies advocated for the market segmentation grid, and this is especially beneficial when applied to an existing product line that was develop one-at-a-time time such that artifact designs are inconsistent from one to another.",2008
"Focused Product Family Improvement","Manufacturers in various industries are seeking to redesign their existing product families to better satisfy their diverse customer needs while maintaining competitive cost structures. Failure to carefully balance the commonality/variety tradeoff during product family redesign will catastrophically hamper the widely sought benefits of both appropriate commonality and variety. Existing product family redesign approaches often focus on increasing the degree of commonality or variety unilaterally and to their utmost, without considering the appropriate commonality/variety tradeoff based on both marketing and engineering resource concerns. The result is redesigned product families that are unachievable or much delayed. In this paper, the Focused Product Family Improvement Method (FPFIM) is proposed to help manufacturers utilize their limited engineering efforts to efficiently respond to market needs using their own competitive focus and commonality/variety tradeoff analysis. This method uses a graphical evaluation tool, the Product Family Evaluation Graph, to determine the necessary direction of improvement for product family redesign — either increasing appropriate commonality or increasing appropriate variety. A set of indices, the Commonality Diversity Index for commonality and variety, support the FPFIM in identifying components with undesirable commonality or undesirable variety, prime targets of redesign to satisfy the redesign intent. To illustrate the proposed method, an example application with four single-use camera families is presented.",2008
"A Strategic Module-Based Platform Design Method for Developing Customized Products in Dynamic and Uncertain Market Environments","Product family design facilitates mass customization by allowing highly differentiated products to be developed around a platform while targeting products to distinct market segments. Therefore, effective platforming of products is a cost-effective way to achieve mass customization The objective in this research is to develop a Strategic Module-based Platform Design Method (SMPDM) to determine a platform design strategy to support product family design in a dynamic and uncertain environment. Ontologies are used to represent products and enable sharing and reuse of design information. Data mining techniques are used to identify a platform and modules by utilizing design information stored in a large database or repository. To determine a platform for family design in dynamic and uncertain market environments, the SMPDM uses agent-based decision-making, involving a market-based negotiation mechanism and a game theoretic approach based on module-based platform concepts and a mathematical model. To demonstrate and validate the usefulness of the proposed method, it is applied to a family of power tools and tested in multiple scenario-based experiments. The SMPDM provides an optimal platform design strategy that can be adapted to various dynamic and uncertain market environments. Therefore, the SMPDM can help develop design strategies to manage and create a cost-effective variety of products based on a platform in support of mass customization.",2008
"Functional Part Families and Design Change for Mechanical Assemblies","We consider two questions related to functional part families: a) how to characterize function in a computational framework, and b) how does the structure-to-function model generalize when the design changes, e.g. by changing the set of design variables? For the first, we observe that function is defined on the space of behaviours of the part, whereas structure is defined in the space of design parameters. For mechanical assemblies, as the design parameters change, their effect on the motion parameters can be complex, and cannot be automated in full generality. Thus, the mapping from structure-to-function involves considerable designer knowledge. For computational purposes, we quantify this function by defining part-family-specific Configuration Space (C-space) constructions, and also a metric that operates on these C-spaces to define each function. When the design is changed, either by changing the design space (structure), or by the user expectation (function), can existing design knowledge from the earlier part family migrate to the new product family? We make a start towards exploring how this knowledge can be modified when the part family is evolved, for example by introducing additional design variables, or by changing functional roles. Using examples from several lock designs, we present a small prototype for this process of modeling function and design change, implemented on a commercial CAD engine.",2008
"Optimal Design of Product Family Throughout Commonalization, Customization and Lineup Arrangement","Product family design is a framework for effectively and efficiently meeting with spread customers’ needs by sharing components or modules across a series of products. This paper systematizes product family design toward its extension to throughout consideration of commonalization, customization and lineup arrangement under the optimal design paradigm. That is, commonalization is viewed as the operation that restricts the feasible region by fixing a set of design variables related to commonalized components or modules against later customization and final lineup offered to customers. Customization is viewed as the operation that arranges lineup by adjusting another set of design variables related to reserved freedom for customers’ needs. Their mutual and bi-directional relationships must be a matter of optimal design. This paper discusses the mathematical fundamentals of optimal product family design throughout commonalization, customization and lineup arrangement under active set strategy, and demonstrates a case study with a design problem of centrifugal compressors for showing the meaning of throughout optimal design.",2008
"An Efficient Re-Analysis Methodology for Probabilistic Vibration of Large-Scale Structures","It is challenging to perform probabilistic analysis and design of large-scale structures because it requires repeated finite-element analyses of large models and each analysis is expensive. This paper presents a methodology for probabilistic analysis and reliability-based design optimization of large-scale structures that consists of two re-analysis methods; one for estimating the deterministic vibratory response and another for estimating the probability of the response exceeding a certain level. Deterministic re-analysis can analyze efficiently large-scale finite element models consisting of tens or hundreds of thousand degrees of freedom and large numbers of design variables that vary in a wide range. Probabilistic re-analysis calculates very efficiently the system reliability for different probability distributions of the design variables by performing a single Monte Carlo simulation. The methodology is demonstrated on probabilistic vibration analysis and a reliability-based design optimization of a realistic vehicle model. It is shown that computational cost of the proposed reanalysis method for a single reliability analysis is about 1/20th  of the cost of the same analysis using NASTRAN. Moreover, the probabilistic re-analysis approach enables a designer to perform reliability-based design optimization of the vehicle at a cost almost equal to that of a single reliability analysis. Without using the probabilistic re-analysis approach, it would be impractical to perform reliability-based design optimization of the vehicle.",2008
"Uncertainty Analysis of Damage Evolution Computed Through Microstructure-Property Relations","Uncertainties in material microstructure features can lead to variability in damage predictions based on multiscale microstructure-property models. In this paper, we present an analytical approach for uncertainty analysis by combining a dimension reduction technique for evaluation of statistical moments of a random response, such as damage, with probability distribution fitting based on the extended generalized lambda distribution. This approach is used to analyze the effects of uncertainties pertaining to structure-property relations of an internal state variable plasticity-damage model that predicts failure. Using an un-notched A356 cast aluminum alloy tension specimen as an example, the predictions for damage uncertainty based on the proposed approach are compared with those found using the first order Taylor series approximation and direct Monte Carlo simulation. In particular, the spatial variabilities in microstructural properties, the constitutive model parameter sensitivities, and the effect of boundary condition uncertainties on the damage evolution and final failure are examined. The results indicate that the higher the strain the greater the scatter in damage, even when the uncertainties in the material plasticity and microstructure parameters are kept constant. For A356, the mathematical sensitivity analysis results related to damage uncertainty are consistent with the physical nature of damage progression. At the very beginning, the initial porosity and void nucleation are shown to drive the damage evolution. Then, void coalescence becomes the dominant mechanism. And finally when approaching closer to failure, fracture toughness is found to dominate the damage evolution process.",2008
"Reliability-Based Shape Optimization of a Pressure Tank Under Random and Stochastic Environments","Reliability-based design of a pressure tank under time-independent random and time-dependent stochastic uncertainties is considered. This pressure tank is an essential element in a reverse osmosis (RO) filtration system for storing filtered water and providing a useable flow rate from the faucet outlet. In this study, we consider the randomness in the welding strength between the upper and lower tanks, and the stochastic pressure applied to the inner surfaces of the tank as the main sources of uncertainty. A pressure tank with 90% reliability against fracture failure is desired. To enable the re-design of the pressure tank, the geometry is parametrized and then used as design variables in a shape optimization scheme. Kriging models are created to approximate the expensive finite element analyses in accessing the performances of each design. The uncertainty model of the welding strength between the upper and lower tanks is found to be well represented by a Gaussian distribution. The stochastic behavior of the pressure loading is modeled by a Markov-chain process. All models are integrated in a reliability-based design optimization problem formulation that has both time-independent and time-dependent reliability constraints. The first passage time and crossover rate are considered in the time-dependent reliability constraint and results of different constraint formulations are compared. The final optimal design satisfies all reliability constraints and reduces the material usage by as many as 46% comparing to the original design.",2008
"Sensitivity Analyses of FORM-Based and DRM-Based Performance Measure Approach for Reliability-Based Design Optimization","In a gradient-based design optimization, it is necessary to know sensitivities of the constraint with respect to the design variables. In a reliability-based design optimization (RBDO), the constraint is evaluated at the most probable point (MPP) and called the probabilistic constraint, thus it requires the sensitivities of the probabilistic constraints at MPP. This paper presents the rigorous analytic derivation of the sensitivities of the probabilistic constraint at MPP for both First Order Reliability Method (FORM)-based Performance Measure Approach (PMA) and Dimension Reduction Method (DRM)-based PMA. Numerical examples are used to demonstrate that the analytic sensitivities agree very well with the sensitivities obtained from the finite difference method (FDM). However, since the sensitivity calculation at the true DRM-based MPP requires the second-order derivatives and additional MPP search, the sensitivity derivation at the approximated DRM-based MPP, which does not require the second-order derivatives and additional MPP search to find the DRM-based MPP, is proposed in this paper. A convergence study illustrates that the sensitivity at the approximated DRM-based MPP converges to the sensitivity at the true DRM-based MPP as the design approaches the optimum design. Hence, the sensitivity at the approximated DRM-based MPP is proposed to be used for the DRM-based RBDO to enhance the efficiency of the optimization.",2008
"Selection of Copula to Generate Input Joint CDF for RBDO","For RBDO problems with correlated input variables, it is necessary to obtain the input joint distribution (CDF, cumulative distribution function). Then Rosenblatt transformation is used to transform the correlated input variables into the independent standard normal variables for the purpose of inverse reliability analysis. However, in practical industry RBDO problems, often only the marginal CDFs and paired samples are available from limited experimental data. In this paper, a copula, which is a link between a joint CDF and marginal CDFs, is proposed to generate an input joint CDF from these marginal CDFs and paired samples. To identify the right copula from limited data, Bayesian method is proposed to use in this paper. Using Bayesian method, the number of samples required to properly identify the right copula is investigated for different types of copulas and for different correlation coefficients. A real industry problem is used to show how a copula can be identified from the limited experimental data.",2008
"Topology Optimization for Steady-State Heat Transfer Problems Including Design-Dependent Effects","In this paper, a topology optimization method is constructed for thermal problems with generic heat transfer boundaries in a fixed design domain that includes design-dependent effects. First, the topology optimization method for thermal problems is briefly explained using a homogenization method for the relaxation of the design domain, where a continuous material distribution is assumed, to suppress numerical instabilities and checkerboards. Next, a method is developed for handling heat transfer boundaries between material and void regions that appear in the fixed design domain and move during the optimization process, using the Heaviside function as a function of node-based material density to extract the boundaries of the target structure being optimized so that the heat transfer boundary conditions can be set. Shape dependencies concerning heat transfer coefficients are also considered in the topology optimization scheme. The optimization problem is formulated using the concept of total potential energy and an optimization algorithm is constructed using the Finite Element Method and Sequential Linear Programming. Finally, several numerical examples are presented to confirm the usefulness of the proposed method.",2008
"Topology Optimization of Multiple Parts in Dynamic Controlled Systems","The importance of computer aided engineering (CAE) in product development processes and research has been increasing throughout the past years. Consequently, optimization tools gained more and more importance. In state-of-the-art processes and methods concerning structural optimization it is assumed that there exists a set of external loads or load functions acting on the part. Very often modern products represent complex mechatronic system. The fact that the system’s dynamic properties and its overall behaviour may change due to geometric modifications of a part caused by an optimization process is typically neglected. In order to take into account the interaction between the part, dynamic system, control system and the changing mechanical behaviour with all its consequences for the optimization process, a simulation of the complete mechatronic system is integrated into the optimization process within the research work presented in this paper. A hybrid multibody system (MBS) simulation, that is, a MBS containing flexible bodies, in conjunction with a cosimulation of the control system represented by tools of the Computer Aided Control Engineering (CACE) is integrated into the optimization process. The research work presented in this paper is a contribution towards the integration of existing CAE methods into a continuous process for structural optimization. The benefits will be illustrated by an example, namely a part of the humanoid robot ARMAR III of the collaborative research centre for “Humanoid Robots” [1]. Especially the optimization of two parts at a time within one optimization loop allows an efficient optimization of structures “within” their surrounding mechatronic system.",2008
"An Improved Initial Population Strategy for Compliant Mechanism Designs Using Evolutionary Optimization","In this paper, an improved initial random population strategy using a binary (0–1) representation of continuum structures is developed for evolving the topologies of path generating complaint mechanism. It helps the evolutionary optimization procedure to start with the structures which are free from impracticalities such as ‘checker-board’ pattern and disconnected ‘floating’ material. For generating an improved initial population, intermediate points are created randomly and the support, loading and output regions of a structure are connected through these intermediate points by straight lines. Thereafter, a material is assigned to those grids only where these straight lines pass. In the present study, single and two-objective optimization problems are solved using a local search based evolutionary optimization (NSGA-II) procedure. The single objective optimization problem is formulated by minimizing the weight of structure and a two-objective optimization problem deals with the simultaneous minimization of weight and input energy supplied to the structure. In both cases, an optimization problem is subjected to constraints limiting the allowed deviation at each precision point of a prescribed path so that the task of generating a user-defined path is accomplished and limiting the maximum stress to be within the allowable strength of material. Non-dominated solutions obtained after NSGA-II run are further improved by a local search procedure. Motivation behind the two-objective study is to find the trade-off optimal solutions so that diverse non-dominated topologies of complaint mechanism can be evolved in one run of optimization procedure. The obtained results of two-objective optimization study is compared with an usual study in which material in each grid is assigned at random for creating an initial population of continuum structures. Due to the use of improved initial population, the obtained non-dominated solutions outperform that of the usual study. Different shapes and nature of connectivity of the members of support, loading and output regions of the non-dominated solutions are evolved which will allow the designers to understand the topological changes which made the trade-off and will be helpful in choosing a particular solution for practice.",2008
"On Pattern Recognition in Rule-Based Topology Modification","Classical topology optimization aims at achieving a problem suited material distribution in a structure by identification of lightly loaded areas and local element-wise reduction of stiffness. The resulting topologic layout often contains small substructures which are complicated to manufacture, hence requiring an additional manual smoothing during the structural interpretation phase. One major drawback of this approach is that the results still have to be interpreted by an engineer and consequently be translated into a feasible structure. In order to gain a first conceptual yet topologically sound design proposal for composite structures, this paper presents an alternate method for an explicit, pattern based topology modification approach combined with numerical simulation of tape-laying technology. It is assumed that certain patterns exist in stress fields that are extractable by pattern recognition algorithms known from image processing. In the case that prototypical structural reinforcements for such stress patterns can be defined, an automatic topology modification algorithm with the goal of increasing the stiffness is feasible. The classification of these stress patterns is achieved by using dimensionless features matching the stress patterns with their appropriate reinforcements. When integrated into a rule-based conceptual design environment, this explicit topology modification offers the potential to generate simple and easily manufacturable topological reinforcement proposals in an automated structural design loop.",2008
"Topology Optimization of a MEMS Resonator Using Hybrid Fuzzy Techniques","This paper introduces a new methodology for the design of structures by geometry and topology optimization accounting for loading and boundary conditions as well as material properties. The Fuzzy Heuristic Gradient Projection (FHGP) method is used as a direct search technique for the geometry optimization, while the Complex Method (CM) is used as a random search technique for the topology optimization. In the proposed method, elements are designed such that they all have the same amount of stresses using the Fuzzy Heuristic Gradient Projection method. On the other hand, the complex method is used for the topology optimization step satisfying any constraint other than the stress constraint. The developed hybrid fuzzy technique is applied for different applications ranging from micro-scale to macro-scale applications. The method is applied to a micro-mechanical resonator as a microelectro-mechanical system (MEMS). The resonator is solved for minimum weight and is subjected to an equality frequency constraint and an inequality stress constraint. The proposed method is compared with the Multi-objective Genetic Algorithms (MOGAs) on solving the MEMS resonator. Results showed that the proposed hybrid fuzzy technique converges to optimum solutions faster than (MOGAs). The time consumed is improved by a 77%.",2008
"Topology Optimization of Poroelastic Structures to Minimize Mean Sound Pressure Levels","In optimization problems that aim to minimize noise, elastic structures have been designed so that fundamental eigenfrequencies depart from excitation frequencies. Moreover, for the sake of simplicity, sound pressure responses have rarely been calculated. In this paper, we propose a new topology optimization method for the design of poroelastic material layouts that minimize sound pressure levels by sound attenuation. In this method, the surrounding air is exactly modeled, and poroelastic material is located in a space filled with air to efficiently dissipate power. The Biot’s theory is incorporated into the optimization scheme to deal with poroelastic material, and we utilize a new bi-material continuum that consists of poroelastic material combined with an equivalent representation of air in the Biot’s theory. Several design problems are presented to demonstrate that the proposed method can provide optimal layouts of poroelastic material that reduce sound pressure levels within specified frequency ranges.",2008
"Design of Compliant Thermal Actuators Using Structural Optimization Based on the Level Set Method","Compliant mechanisms are a new type of mechanism, designed to be flexible to achieve a specified motion as a mechanism. Such mechanisms can function as compliant thermal actuators in Micro-Electro Mechanical Systems (MEMS) by intentionally designing configurations that exploit thermal expansion effects in elastic material when appropriate portions of the mechanism structure are heated. This paper presents a new structural optimization method for the design of compliant thermal actuators based on the level set method and the Finite Element Method (FEM). First, an optimization problem is formulated that addresses the design of compliant thermal actuators considering the magnitude of the displacement at the output location. Next, the topological derivatives that are used when introducing holes during the optimization process are derived. Based on the optimization formulation and the level set method, a new structural optimization algorithm is constructed that employs the FEM when solving the equilibrium equations and updating the level set function. The re-initialization of the level set function is performed using a newly developed geometry-based re-initialization scheme. Finally, several design examples are provided to confirm the usefulness of the proposed structural optimization method.",2008
"Design Automation Tools as a Support for Knowledge Management in Topology Optimization","The problem of integrating topological optimization tools in product development process (PDP) is becoming more and more urgent since nowadays they are widely employed in several engineering fields (civil, aeronautics, aerospace, automotive). The interest for these tools is due to their capacity to better mechanical properties through a global optimization of the product in terms of weight, stiffness, resistance and cost. In particular, there is a lack of specific tools for automatic feature recognition on voxel models generated by the topological optimization tools. Our paper presents an innovative methodology that allows the integration of topological optimizers in the product development process by means of a wise and rational knowledge management and an efficient data exchange between different systems. The target has been reached through the implementation of CAD automation modules which decrease the working time and give the possibility to effectively schematize the designer’s knowledge.",2008
"Topology Synthesis of Multi-Component Structural Assemblies in Continuum Domains","Most structural products have complex geometry to meet customer’s demand of high functionality. Since manufacturing those products in one piece is either impossible or uneconomical, most structural products are assemblies of components with simpler geometries. The conventional way to design structural assemblies is to design overall geometry first, and then decompose the geometry to determine the part boundary and joint locations. This two-step process, however, can lead to sub-optimal designs since the product geometry, even if optimized as one piece, would not be optimal after decomposition. This paper presents a method for synthesizing structural assemblies directly from the design specifications, without going through the two-step process. Given an extended design domain with boundary and loading conditions, the method simultaneously optimizes the topology and geometry of an entire structure and the location and configuration of joints, considering structural performance, manufacturability, and assembleability. As a relaxation of our previous work utilizing a beam-based ground structure [1], this paper presents a new formulation in a continuum design domain, which greatly enhances the ability to represent complex structural geometry observed in real-world products. A multi-objective genetic algorithm is used to obtain Pareto optimal solutions that exhibits trade-offs among stiffness, weight, manufacturability, and assembleability.",2008
"Layer Separation for Optimization of Composite Laminates","The excellent mechanical properties of laminated composites cannot be exploited without a careful design of stacking sequence of the layers. An important variable in the search of the optimum stacking sequence is the number of layers. The larger is this number, the harder as well as longer is the search for an optimal solution. To tackle efficiently such a variable-dimensional problem, we introduce here a multi-level optimization technique. The proposed method, called Layer Separation (LS), increases or decreases the number of layers by gradually separating a layer into two, or by merging two layers into one. LS uses different levels of laminate representation ranging from a coarse level parameterization, which corresponds to a small number of thick layers, to a fine level parameterization, which corresponds to a large number of thin layers. A benefit of such differentiation is an increase of the probability of finding the global optimum. In this paper, LS is applied to the design of composite laminates under single and multiple loadings. The results show that LS convergence rate is not inferior to that of other optimization techniques available in the literature. It is faster than an evolutionary algorithm, more efficient than a layerwise method, simple to perform, and it has the advantage of possibility of termination at any point during the optimization process.",2008
"Model Validation and Error Modeling to Support Sequential Sampling","Several model validation and prediction error modeling techniques are studied and compared in this paper to help establish stopping criteria and identify critical regions in the design space in a sequential sampling framework. This study leads to the proposal of a two-phase sequential sampling and meta-modeling strategy, which is realized by the support of a multi-dimensional data visualization tool. These techniques have been successfully applied in the development and setup of a system-level parametric tool to support Heating, Ventilating, and Air Conditioning design. Maintaining the same level of accuracy, we observe a savings of 6–30 times the simulation effort needed for current practice. The benefits and drawbacks of the method are discussed, and opportunities are identified for future improvement.",2008
"Regression Modeling for Computer Model Validation With Functional Responses","Statistical analysis of functional responses based on functional data from both computer and physical experiments has gained increasing attention due to the dynamic nature of many engineering systems. However, the complexity and huge amount of functional data bring many difficulties to apply traditional or existing methodologies. The objective of the present study is twofold: (1) prediction of functional responses based on functional data and (2) prediction of bias function for validation of a computer model that predicts functional responses. In this paper, we first develop a functional regression model with linear basis functions to analyze functional data. Then combining data from both computer and physical experiments, we use the functional analysis modeling to predict the bias function which is crucial for validating a computer model. The proposed method, following the classical nonparametric regression framework, uses a single step procedure which is easily implemented and computationally efficient. Through an application example of motor engine analysis to predict acceleration performance and gear shift events, we demonstrate our approach and compare it to using the Gaussian process modeling approach.",2008
"A Comprehensive Metric for Comparing Time Histories in Validation of Simulation Models With Emphasis on Vehicle Safety Applications","Computer modeling and simulation are the cornerstones of product design and development in the automotive industry. Computer-aided engineering tools have improved to the extent that virtual testing may lead to significant reduction in prototype building and testing of vehicle designs. In order to make this a reality, we need to assess our confidence in the predictive capabilities of simulation models. As a first step in this direction, this paper deals with developing a metric to compare time histories that are outputs of simulation models to time histories from experimental tests with emphasis on vehicle safety applications. We focus on quantifying discrepancy between time histories as the latter constitute the predominant form of responses of interest in vehicle safety considerations. First we evaluate popular measures used to quantify discrepancy between time histories in fields such as statistics, computational mechanics, signal processing, and data mining. Then we propose a structured combination of some of these measures and define a comprehensive metric that encapsulates the important aspects of time history comparison. The new metric classifies error components associated with three physically meaningful characteristics (phase, magnitude and topology), and utilizes norms, cross-correlation measures and algorithms such as dynamic time warping to quantify discrepancies. Two case studies demonstrate that the proposed metric seems to be more consistent than existing metrics. It is also shown how the metric can be used in conjunction with ratings from subject matter experts to build regression-based validation models.",2008
